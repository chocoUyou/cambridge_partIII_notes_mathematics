\documentclass[10pt,a4paper]{article}


\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{calrsfs}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[mathscr]{euscript}
\usepackage{bm}

\usepackage{lmodern}

%%%%%%%%%%for writing large parallel%%%%%%
\usepackage{mathtools}
\DeclarePairedDelimiter\bignorm{\lVert}{\rVert}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%for drawing commutative diagrams.%%%%%%
\usepackage{tikz-cd}  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%for changing margin
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 

\newenvironment{proof}
{\begin{changemargin}{0.5cm}{0.5cm} 
	}%your text here
	{\end{changemargin}
}

\newenvironment{subproof}
{\begin{changemargin}{0.5cm}{0.5cm} 
	}%your text here
	{\end{changemargin}
}

\renewenvironment{i}
{\begin{itemize} 
	}%your text here
	{\end{itemize}
}

\newenvironment{p}
{\begin{proof} 
	}%your text here
	{\end{proof}
}

\newenvironment{boxing}
    {\begin{center}
    \begin{tabular}{|p{0.9\textwidth}|}
    \hline\\
    }
    { 
    \\\\\hline
    \end{tabular} 
    \end{center}
    }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%double rules%%%%%%%%%%%%%%%%%%%
\usepackage{lipsum}% Just for this example

\newcommand{\doublerule}[1][.4pt]{%
  \noindent
  \makebox[0pt][l]{\rule[.7ex]{\linewidth}{#1}}%
  \rule[.3ex]{\linewidth}{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Stochastic Calculus and Applications}
\author{Lectured by Dr. Roland Bauerschmidt, Typed by Jiwoon Park}
\date{Lent 2019}

\maketitle

\newcommand{\latinmodern}[1]{{\fontfamily{lmss}\selectfont
\textbf{#1}
}}

\newcommand{\thm}{\latinmodern{Theorem) }}
\newcommand{\thmnum}[1]{\latinmodern{Theorem #1) }}
\newcommand{\defi}{\latinmodern{Definition) }}
\newcommand{\definum}[1]{\latinmodern{Definition #1) }}
\newcommand{\lem}{\latinmodern{Lemma) }}
\newcommand{\lemnum}[1]{\latinmodern{Lemma #1) }}
\newcommand{\prop}{\latinmodern{Proposition) }}
\newcommand{\propnum}[1]{\latinmodern{Proposition #1) }}
\newcommand{\corr}{\latinmodern{Corollary) }}
\newcommand{\corrnum}[1]{\latinmodern{Corollary #1) }}
\newcommand{\pf}{\textbf{proof) }}
\newcommand{\fact}{\latinmodern{Fact : }}
\newcommand{\statement}[1]{\latinmodern{#1) }}

\newcommand{\lap}{\triangle} %%Laplacian
\newcommand{\s}{\vspace{10pt}}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\eop}{\hfill  \textsl{(End of proof)} $\square$} %end of proof
\newcommand{\eos}{\hfill  \textsl{(End of statement)} $\square$} %end of proof

\newcommand{\charac}{\bm{1}}
%\newcommand{\charac}{\mathrel{\raisebox{0pt}{\scalebox{1}[1.2]{$1$}} \mkern-5.5mu \raisebox{0.04pt}{\scalebox{1}[1.2]{$\_$}} \mkern-5.5mu\raisebox{2.5pt}{\scalebox{1}[0.8]{$\bm{|}$}} }}

\newcommand{\norms}[2]{\bignorm[\big]{#1}_{#2}}
\newcommand{\snorms}[2]{\bignorm[\small]{#1}_{#2}}
\newcommand{\tnorms}[2]{\mathrel{\raisebox{0pt}{\scalebox{1}[1.5]{$|$}}\mkern-2.0mu \raisebox{0pt}{\scalebox{1}[1.5]{$|$}}\mkern-2.0mu \raisebox{0pt}{\scalebox{1}[1.5]{$|$}} #1 \raisebox{0pt}{\scalebox{1}[1.5]{$|$}}\mkern-2.0mu \raisebox{0pt}{\scalebox{1}[1.5]{$|$}}\mkern-2.0mu \raisebox{0pt}{\scalebox{1}[1.5]{$|$}}}_{#2}} %norm with triple bars.
\newcommand{\avg}{\mathbb{E}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\borel}{\mathscr{B}}
\newcommand{\EE}{\mathscr{E}}
\newcommand{\FF}{\mathscr{F}}
\newcommand{\pa}{\partial}

\newcommand{\call}[1]{\quad \cdots\cdots\cdots\,\,(#1)}

\renewcommand{\vec}{\underline}
\renewcommand{\bar}{\overline}

\def\doubleunderline#1{\underline{\underline{#1}}}

\newcommand{\newday}{\doublerule[0.5pt]}

\setlength\parindent{0pt}

\section{Introduction}

\subsection{Motivation}

\subsection{The Wiener Integral}

$(\Omega, \FF, \prob)$ be a probability space

\defi Gaussian space $S\subset L^2(\Omega, \FF, \prob)$ 
\s

\textbf{Example :} Let $(\Omega, \FF, \prob)$ be a probability space on which a sequence of independent random variables $X_i \sim N(0,1)$ is defined. Then the $X_i$ are an orthonormal system in $L^2(\Omega, \FF, \prob)$ :
\begin{align*}
\avg(X_i X_j) =0 \quad \text{for } i\neq j \text{ and} \quad \avg(X_i^2) =1 
\end{align*}
and $S= \overline{\text{span} \{X_i\}}$ is a Gaussian space. (\textit{Exercise} : the limit in $L^2$ of Gaussian random variables is Gaussian.)
\s

\prop Let $H$ be a separable Hilbert space and $(\Omega, \FF, \prob)$ as in the example. Then there is an isomtery $I: H\rightarrow S$. In particular, for every $f\in H$, there is a random variable $I(f) \in S$ such that
\begin{align*}
I(f) \sim N(0, (f,f)_H) \quad \text{and} \quad \avg(I(f)I(g)) = (f,g)_H
\end{align*}
Moreover, $I(\alpha f + \beta g) = \alpha I(f) + \beta I(g)$ a.s.
\s

\defi A Gaussian white noise on $\reals_+$ 
\s

\prop \begin{i}
\item[(1)] For $A\subset \reals_+$, Borel, $|A|<\infty$, $WN(A) \sim N(0, |A|)$.
\item[(2)] For $A, B\subset \reals_+$ Borel, $A\cap B =\phi$ then $WN(A)$ and $WN(B)$ are independent.
\item[(3)] If $A = \cup_{i=1}^{\infty} A_i$ for disjoint sets $A_i$ with $|A_i|<\infty$, $|A| <\infty$, then 
\begin{align*}
WN(A) = \sum_{i=1}^{\infty} WN(A_i)\quad \text{in } L^2 \text{ and a.s.} \quad \cdots\cdots\cdots (\star)
\end{align*}
\end{i}
\s

For $t\geq 0$, define the Brownian motion as $B_t = WN([0,t])$, just like the integration of white noise from 0 to $t$. - Justify that this indeed (up to a modification) a BM

\subsection{The Lebesgue-Stieltjes Integral}

\defi signed measure (on $[0, T]\subset \reals_{\geq 0}$), Hahn-Jordan decomposition, total variation
\s

\prop \emph{(Hahn-Jordan)} For any positive measures $\mu_1, \mu_2$ on $[0, T]$(we do not require them to have disjoint support), there is a signed measure $\mu$ s.t. $\mu = \mu_1 - \mu_2$.
\s

\defi c\`{a}dl\`{a}g function, total variation, of bounded variation on $[0, T]$
\s

\prop \begin{i}
\item[(i)] Let $\mu$ be a signed measure on $[0, T]$. Then $a(t) = \mu ([0,t])$ is c\`adl\`ag and $|\mu|((0,t]) = v_a(0,t)$ (i.e. $|\mu|([0,T])) = |a(0)| + v_a(0,t)$ )

\quad In particular, $a\in BV([0,T])$.
\item[(ii)] Let $a: [0, T] \rightarrow \reals$ be c\`adl\`ag of bounded variation. The there is a signed measure $\mu$ such that $a(t) = \mu([0,t])$.
\end{i}
\s

\defi Let $a:[0,T]\rightarrow \reals$ be c\`adl\`ag of bounded variation, Lebesgue-Stieltjes integral respect to $a$.
\s

\fact Let $a: [0,T] \rightarrow \reals$ be c\`adl\`ag and BV (bounded variation), $h\in L^2([0,T], |da|)$. Then
\begin{align*}
\Big| \int_0^t h(s) da(s )\Big| \leq \int_0^t |h(s)| |da(s)|
\end{align*}
and the function $h\cdot a:[0,T]\rightarrow \reals$ is c\`adl\`ag and BV with signed measures $h(s)da(s)$, $|h(s)da(s)| = |h(s)||da(s)|$.
\s

\prop Let $a: [0,T] \rightarrow \reals$ be c\`adl\`ag and BV. Let $h:[0,T] \rightarrow \reals$ be \emph{left-continuous} and bounded. Then
\begin{align*}
& \int_0^t h(s) da(s) = \lim_{m\rightarrow \infty} \sum_{i=1}^{n_m} h(t_{i-1}^{(m)})(a(t_i^{(m)}) - a(t_{i-1}^{(m)})), \quad t\leq T \\
& \int_0^t h(s) |da(s)| = \lim_{m\rightarrow \infty} \sum_{i=1}^{n_m} h(t_{i-1}^{(m)})\Big|a(t_i^{(m)}) - a(t_{i-1}^{(m)})\Big|
\end{align*}
for a sequence of subdivisions $0=t_0^{(m)} < \cdots < t_{n_m}^{(m)}=t$ with $\max_i |t_i^{(m)} - t_{i-1}^{(m)}| \rightarrow 0$ as $m\rightarrow \infty$.
\s

\defi finite variation (FV) function

\section{Semimartingales}

From now on $(\Omega, \FF, (\FF_t)_{\geq 0}, \prob)$ is a filtered probability space.
\s

\defi A \textbf{c\`adl\`ag adapted process} $X$

\textit{Notation : write $X\in \FF$ to denote that a random variable $X$ is measurable with respect to the sigma algebra $\FF$}.

\subsection{Finite variation process}

\defi A finite variation process, total variation process
\s

\fact The total variation process $V$ of c\`adl\`ag adpated process $A$ is also c\`adl\`ag adapted and it is also increasing.
\s

\defi $H \cdot A$ for $A$ a finite variation process and $H$ with an integrability condition (to be stated)
\s

\defi previsible(predictable) $\sigma$-algebra, predictable process.
\s

\defi simple process
\s

\fact \emph{Simple processes and their pointwise limits are predictable.}
\s

\fact \emph{Adapted left-continuous processes are predictable}
\s

\fact \emph{Let $H$ be predictable. Then $H_t \in \FF_{t^-}$ where $\FF_{t^{-}} = \sigma(\FF_s : s< t)$. (See Example Sheet \#1)}
\s

\fact \emph{Let $X$ be adapted c\`adl\`ag. Then $X_{t^-} = \lim_{s\rightarrow t^-} X_s$ is left-continuous, predictable.}
\s

\textbf{Examples} Brownian motion is predictable. / A Poisson process $(N_t)$ is \emph{not} predictable.
\s

\prop Let $A$ be a finite variation process, and let $H$ be a predictable process such that $\int_0^t |H_s| |dA_s| < \infty$ for all $t$ and $\omega$. Then $H\cdot A$ is also a finite variation process.

\subsection{Local Martingale}

From now on, we assume that $(\Omega, \FF, (\FF_t)_{t\geq 0}, \prob)$ satisfies the \emph{usual conditions} (state)
\s

\thm \emph{(Optional Stopping Theorem, OST)} Let $X$ be a c\`adl\`ag adapted integrable process. Then the following are equivalent : state / proof in Advanced probability
\s

\defi local martingale
\s

\textbf{Example :} 
\begin{i}
\item[(\textit{i})] Every martingale is a local martingale (Take $T_n = n$ and use \emph{OST}).
\item[(\textit{ii})] Let $(B_t)$ be a standard Brownian motion on $\reals^3$. Then $(X_t)_{t\geq 1} = (1/|B_t|)_{t\geq 1}$ is a local martingale, but not a martingale. (prove)
\end{i}
\s

\prop Let $X$ be a \emph{local martingale} and $X_t \geq 0$ for all $t\geq 0$. Then $X$ is a supermartingale.
\s

\prop Let $X$ be a local martingale and suppose that there is $Z\in L^1$ such that $|X_t| \leq Z$ for all $t\geq 0$. Then $X$ is a martingale. In particular, bounded local martingales are martingales.
\s

\fact Let $X$ be a \emph{continuous adapted process} with $X_0 =0$. Then
\begin{align*}
S_n = \inf\{t\geq 0 : |X_t| =n\}
\end{align*}
are stopping times and $S_n \nearrow \infty$ as $n\rightarrow \infty.$
\s

\prop Let $X$ be a continuous local martingale with $X_0 =0$. Then the sequence $(S_n)$ defined above reduces $X$.
\s

\thm Let $X$ be a \emph{continuous local martingale} with $X_0 =0$. If $X$ is also a \emph{finite variation process}, then $X_t =0$ for all $t\geq 0$ a.s.

\subsection{$L^2$ bounded martingales}

\defi $M^2$, $M_c^2$, $\snorms{X}{M^2}$ - why is this a norm on $M^2$?

\quad In fact, $(X, Y)_{M^2} = \avg[X_{\infty}Y_{\infty}]$ is an inner product on $M^2$ that induces the inner product - prove this.
\s

\prop $M^2$ is a \emph{Hilbert space} and $M_c^2$ is a closed subspace.

\subsection{Quadratic Variation}

\defi convergent uniformly on compact intervals in probability 
\s

\thm Let $M$ be a \emph{continuous} local martingale. Then there exists a unique (up to indistinguishability) \emph{continuous adapted increasing process} $\langle M \rangle = \big( \langle M \rangle_t \big)_t$ such that (is uniquely characterized by) $\langle M \rangle_0 =0$ and $M^2 -\langle M \rangle$ is a continuous local martingale.

\quad Moreover, with $0=t_0^m < t_1^m < \cdots$ given by $t_i^m = 2^{-m}i$,
\begin{align*}
\langle M \rangle_t^{(m)} \xrightarrow{\text{ucp}} \langle M \rangle_t \quad \text{where } \langle M \rangle_t^{(m)} = \sum_{i=1}^{\lfloor 2^m t \rfloor} (M_{t_i} - M_{t_{i-1}})^2
\end{align*}
\emph{[In fact, the convergence is true for all locally finite subdivision of $[0, \infty)$ with $\max_i |t_i^m - t_{i-1}^m| \rightarrow 0$ as $m\rightarrow \infty$.]}
\s

\defi quadratic variation of $M$, $\langle M \rangle$.
\s

\textbf{Example :} $\langle B \rangle_t =t$ for $B$ a standard Brownian motion. - prove
\s

\lem \emph{(bounded case)} The theorem is true under the additional assumption $|M_t|\leq C$ for all $(\omega, t)$, $M_t = M_{t\wedge T}$ for $C,T$ deterministic constants.
\s

\lem Suppose $M$ is a continuous local martingale for which $\langle M \rangle$ exists. Let $T$ be a stopping time. Then $\langle M^T \rangle$ exists and is given by $\langle M^T\rangle_t = \langle M \rangle_{t\wedge T}$ (up to indistinguishability).
\s

\fact Let $M$ be a continuous local martingale with $M_0 =0$. Then $M\equiv 0$ \emph{iff} $\langle M \rangle =0$.
\s

\prop Let $M\in M^2_c$ with $M_0 =0$. Then $M^2 -\langle M \rangle$ is a \emph{uniformly integrable} martingale and
\begin{align*}
\norms{M}{M^2} = \big( \avg \big[\langle M \rangle_{\infty} \big] \big)^{1/2}
\end{align*}
In particular, the norm only depends on the quadratic variation.

\subsection{Covariation}

\defi For $M$ and $N$ continuous local martingales, define covariation $\langle M, N\rangle$.
\s

\prop 
\begin{i}
\item[(i)] $\langle M, N\rangle$ is the unique (up to indistinguishability) finite variation process such that $MN -\langle M, N\rangle$ is a continuous local martingale.
\item[(ii)] We have $\langle M, N \rangle_t^{(m)} \xrightarrow{ucp} \langle M, N \rangle_{t}$ where
\begin{align*}
\langle M, N \rangle_t^{(m)} = \sum_{i=1}^{\lfloor 2^m t \rfloor} (M_{i2^{-m}}-M_{(i-1)2^{-m}})(N_{i2^{-m}}-N_{(i-1)2^{-m}})
\end{align*}
\item[(iii)] The mapping $M,N \mapsto \langle M, N \rangle$ is bilinear and symmetric.
\item[(iv)] For every stopping time $T$, $\langle M^T, N^T \rangle_t = \langle M^T, N \rangle_t = \langle M, N \rangle_{T\wedge t}$.
\item[(v)] If $M, N \in M_c^2$ with $M_0 =N_0 =0$, then $M_T N_t - \langle M, N \rangle$ is a uniformly integrable martingale and
\begin{align*}
(M, N)_{M^2} = \avg \langle M, N \rangle_{\infty}
\end{align*}
\end{i}
\s

\prop \emph{(Kunita-Watanabe inequality)} Let $M$ and $N$ be continuous local martingale,s and let $H$ and $K$ be measurable processes. Then a.s.
\begin{align*}
\int_0^{\infty} |H_s| |K_s| |d\langle M, N\rangle_s| \leq \Big( \int_{0}^{\infty} |H_s|^2 d\langle M \rangle_s \Big)^{1/2} \Big( \int_{0}^{\infty} |K_s|^2 d\langle N \rangle_s \Big)^{1/2} \call{\text{KW}}
\end{align*}

\subsection{Semimartingales}

\defi (continuous) semimartingale, its quadratic variation.
\s

\emph{Exercise :} We again have limit expression
\begin{align*}
\langle X, Y \rangle_{t}^{(m)} = \sum_{i=1}^{\lfloor 2^m t\rfloor} (X_{i2^{-m}} -X_{(i-1)2^{-m}})(Y_{i2^{-m}} -Y_{(i-1)2^{-m}}) \xrightarrow{ucp} \langle X, Y \rangle_t
\end{align*}

\section{The It\^o integral}

\subsection{Simple processes}

\defi simple process, Ito integral for simple processes respect to a $M_c^2$-martingale.
\s

\prop Let $M \in M_c^2$ and $H\in \mathscr{E}$. Then $H\cdot M\in M_c^2$ and
\begin{align*}
\norms{H \cdot M}{M^2}^2 = \avg \Big( \int_0^{\infty} H_s^2 d\langle M \rangle_s \Big) \quad \text{(It\^o isometry for simple process)}
\end{align*}
What is the critical point about this proposition?
\s

\prop Let $M\in M_c^2$ and let $H\in \EE$. Then
\begin{align*}
\langle H\cdot M, N \rangle = H \cdot \langle M, N \rangle, \quad \forall N \in M_c^2
\end{align*}
\textit{i.e.} $\langle \int_0^{\cdot} H_s dM_s, N\rangle = \int_0^{\cdot} H_s d\langle M, N \rangle_s$.

\subsection{It\^o isometry}

\defi $L^2(M)$ for $M\in M_c^2$, norm and inner product in $L^2(M)$ - why is $(H,K)_{L^2(M)}$ finite for $H,K\in L^2(M)$?
\s

\fact $L^2(M) = L^2(\Omega \times [0, \infty), \mathscr{P}, d\prob d\langle M \rangle)$ is a Hilbert space. (Recall $\mathscr{P}$ is the previsible $\sigma$-algebra)
\s

\prop Let $M\in M_c^2$. Then $\EE$, the space of simple processes, is dense in $L^2(M)$.
\s

\statement{Theorem/Definition} Let $M\in M_c^2$. Then
\begin{i}
\item[(i)] The map $H\in \EE\mapsto H\cdot M \in M_c^2$ extends uniquely to an isometry $L^2(M) \rightarrow M_c^2$, the \emph{It\^o isomtery}.
\item[(ii)] $H\cdot M$ is the unique martingale in $M_c^2$ such that
\begin{align*}
\langle H\cdot M, N \rangle = H\cdot \langle M, N\rangle, \quad \forall N \in M_c^2
\end{align*}
\end{i}
$(H\cdot M)_t = \int_0^t H_s dM_s$ is then called the \textbf{It\^o integral} of $H$ with respect to $M$.
\s

\corr If $T$ is a stopping time, then
\begin{align*}
(1_{[0, T]}H) \cdot M = (H \cdot M)^T = H\cdot M^T
\end{align*}
\s

\corr $\langle H\cdot M, K\cdot N \rangle = (HK)\cdot \langle M, N \rangle$, \textit{i.e.}
\begin{align*}
\langle \int_0^{\cdot} H_s dM_s , \int_0^{\cdot} K_s dN_s \rangle_t = \int_0^t H_s K_s d\langle M, N \rangle_s 
\end{align*}
\s

\corr One has, if $t>u$,
\begin{align*}
& \avg \big( \int_0^t H_s dM_s \big) =0 \\
& \avg \big( \int_0^t H_s dM_s | \FF_u \big) = \int_0^u H_s dM_s \\
& \avg \big( \int_0^t H_s dM_s \int_0^t K_s dN_s \big) = \avg \big( \int_0^t H_s K_s d\langle M, N \rangle_s \big)
\end{align*} 
\s

\corr \emph{(Associativity of It\^o integral)} Let $H\in L^2(M)$. Then $KH \in L^2(M)$ \emph{iff} $K \in L^2(H \cdot M)$ and then
\begin{align*}
(KH)\cdot M = K\cdot (H \cdot M)
\end{align*}

\subsection{Extension to local martingales}

\defi Let $M$ be a \emph{continuous local martingale}, define $L^2_{loc}(M)$.
\s

\thm Let $M$ be a continuous local martingale.
\begin{i}
\item[(i)] For every $H \in L^2_{loc}(M)$, there is a unique (up to indistiguishability) contiuous local martingale $H\cdot M$ with $(H\cdot M)_0 =0$ such that
\begin{align*}
\langle H \cdot M, N \rangle = H \cdot \langle M, N\rangle \quad \forall N \text{ continuous local martingale}
\end{align*}
\item[(ii)] If $H\in L_{loc}^2(M)$ and $K$ is predictable then $K\in L^2_{loc}(H \cdot M)$ \emph{iff} $HK\in L^2_{loc}(M)$ and then
\begin{align*}
H \cdot (K \cdot M) = (HK) \cdot M
\end{align*}
\item[(iii)] If $T$ is a stopping time,
\begin{align*}
(1_{[0, T]} H)\cdot M = (H \cdot M)^T = H\cdot M^T
\end{align*}
\end{i}
Finally, if $M\in M^2_c$ and $H\in L^2(M)$ then this definition is consistent with the previous one. 

\subsection{Extension to Semimartingales}

\defi locally bounded process
\s

\fact If $H$ is locally bounded and predictable and if $A$ is a finite variation process,
\begin{align*}
\forall t>0, \quad \int_0^t H_s |dA_s| < \infty \quad \text{a.s.}
\end{align*}
In particular, for such $H$, and $M$ a continuous local martingale, it follows that $H\in L_{loc}^2(M)$.
\s

\defi Let $X = X_0 + M + A$ be a continuous semimartingale, and let $H$ be a predictable locally bounded process. Define $H\cdot X$.
\s

\prop \emph{(Stochastic Dominated Convergence Theorem, Stochastic DCT)} Let $X$ be a continuous semimartingale, and let $H$ be locally bounded predictable process and let $K$ be a predictable non-negative process. Let $t>0$ and assume that
\begin{i}
\item[(i)] $H_s^n \xrightarrow{n\rightarrow \infty} H_s$ for all $s\in [0,t]$.
\item[(ii)] $|H_s^n|\leq K_s$ for all $s\in [0, t]$ and $n\in \mathbb{N}$.
\item[(iii)] $\int_0^t K_s^2 d\langle M \rangle + \int_0^t K_s |dA_s| < \infty$ (where $X = X_0 + M +A)$. \emph{[This condition is always true if $K$ is locally bounded]}
\end{i}
Then $\int_0^t H_s^n dX_s \xrightarrow{ucp}\int_0^t H_s dX_s$ as $n\rightarrow \infty$.
\s

\corr Let $X$ be a continuous semimartingale, and let $H$ be a locally bounded adapted left-continuous processs. Then for any subdivision $0 =t_0^{(m)} < \cdots < t_{n_m}^{(m)}=t$ of $[0,t]$ with $\max_i |t_i^{(m)} - t_{i-1}^{(m)}| \rightarrow 0$ as $m\rightarrow \infty$, has :
\begin{align*}
\lim_{m\rightarrow \infty} \sum_{i=1}^{n_m} H_{t_{i-1}^{(m)}} \big(X_{t_i^{(m)}} -X_{t_{i-1}^{(m)}} \big) = \int_0^t H_s dX_s
\end{align*}

where the convergence is made ucp.
\emph{[Taking the left end-point $t_{i-1}^{(m)}$ is important, and is consistent with the choice of It\^o integral. Different choice corresponds to what the integral means.]}
\s

\textbf{Remark :} Suppose $H$ is continuous. Unlike the case that $X$ is of finite variation, it is essential here that $H$ is evaluated at the left end point. - state why

\subsection{It\^o formula}

\thm \emph{(Integration by parts)} Let $X$ and $Y$ be continuous semimartingales. Then a.s,
\begin{align*}
X_t Y_t - X_0 Y_0 = \int_0^t X_s dY_s + \int_0^t Y_s dX_s + \langle X, Y \rangle_t
\end{align*}
The last term $\langle X, Y \rangle$ is called the \textbf{It\^o correction.}
\s

\thm \emph{(It\^o formula)} Let $X^1, \cdots, X^p$ be continuous semimartingales, and let $f \in C^2(\reals^p ; \reals)$. Then, writing $X = (X^1, \cdots, X^p)$, a.s,
\begin{align*}
f(X_t) =  f(X_0) + \sum_{i=1}^p \int_0^t \frac{\pa f}{\pa x^i}(X_s)dX_s^i + \frac{1}{2} \sum_{i,j=1}^p \int_0^t \frac{\pa^2 f}{\pa x^i \pa x^j} (X_s) d\langle X^i, X^j \rangle_s \call{\star}
\end{align*}
Informally, we may write
\begin{align*}
df(X_t) = \sum_{i=1}^p \frac{\pa f}{\pa x^i} (X_t) dX_t^i + \frac{1}{2} \sum_{i,j=1}^p \frac{\pa^2 f}{\pa x^i \pa x^j}(X_t) d\langle X^i, X^j \rangle_t
\end{align*}

\subsubsection*{Summary of calculation rules for the It\^o integral :}

Let us adopt the notations
\begin{align*}
Z_t - Z_0 = \int_0^t H_s dX_{S_t} \quad \Leftrightarrow & \quad dZ_t = H_t dX_t \\
Z_t - Z_0 = \langle X, Y\rangle_t =\int_0^t d\langle X, Y \rangle_s \quad \Leftrightarrow & \quad dZ_t = dX_t dY_t
\end{align*}
Then, :
\begin{align*}
\begin{array}{rl}
\textbf{``Associativity" }  & H_t (K_t dX_t) = (H_t K_t) dX_t, \quad (\textit{i.e. } H\cdot (K \cdot X) = (HK) \cdot X) \\
\textbf{``Kunita-Watanabe equality" } & H_t dX_t dY_t = (H_t dX_t) dY_t, \quad (\textit{i.e. } H\cdot \langle X, Y \rangle = \langle H\cdot X, Y \rangle )\\
\textbf{``It\^o formula" } & df(X_t) = \sum_{i}\frac{\pa f}{\pa x^i} dX_t^i + \frac{1}{2} \sum_{i,j} \frac{\pa^2 f}{\pa x^i \pa x^j} (X_t) dX_t^i dX_t^j
\end{array}
\end{align*}

\section{Applications to Brownian Motion and Martingales}

\subsection{L\'evy's characterisation of Brownian motion}

\thm Let $X = (X^1, \cdots, X^p)$ be continuous local martingales. Suppose $X_0 = 0$ and that $\langle X^i, X^j \rangle_t = \delta_{ij} t$ for all $t\geq 0$. Then $X$ is a standard $p$-diemnsional Brownaim motion. That is, \emph{the covariation singles out the Brownian motion.}

\subsection{Dubins-Schwarz Theorem}

\thm Let $M$ be a continuous local martingale with $M_0 =0$ and $\langle M \rangle_{\infty} =\infty$ a.s. Let $T_s = \inf \{t\geq 0 : \langle M \rangle_t >s \}$ be the right-continuous inverse of $\langle M \rangle$.
\begin{align*}
B_s = M_{T_s}, \quad \mathscr{G}_s = \FF_{T_s}
\end{align*}
Then $T_s$ is an $(\FF_t)$ stopping times, $\langle M \rangle_{T_s}$ for all $s\geq 0$, $B$ is a $(\mathscr{G}_s)_{s\geq 0}$-Brownian motion and
\begin{align*}
M_t = B_{\langle M \rangle_t},
\end{align*}
(needs the following lemma)
\s

\lem Let $M$ be a continuous local martingale. Almost surely for all $u<v$, $M$ is constant on $[u,v]$ \emph{iff} $\langle M \rangle$ is constant on $[u,v]$.

\subsection{Girsanov's Theorem}

\defi stochastic exponential of a continuous local martingale $L$ 
\s

\fact $Z = \EE (M)$ is a continuous local martingale and it satisfies
\begin{align*}
dZ_t = Z_t dL_t
\end{align*}
-prove
\s

\thm \emph{(Girsanov)} Let $L$ be a continuous local martingale with $L_0 =0$. Suppose that $\EE (L)$ is a \emph{UI(uniformly integrable)} martingale. Define a probability measure $\mathbb{Q}$ by
\begin{align*}
\frac{d\mathbb{Q}}{d \prob} = \EE (L)_{\infty}
\end{align*}
If $M$ is a continuous local martingale with respect to $\prob$, then $\tilde{M} = M - \langle M, L\rangle$ is a continuous local martingale with respect to $\mathbb{Q}$.
\s

\emph{Remark :} The quadratic variation does not change, $\langle M \rangle = \langle \tilde{M} \rangle$. (also prove this)
\s

\prop Suppose that $\langle L \rangle$ is bounded, say $\langle L \rangle_{\infty} \leq C$. Then $\EE(L)$ is a UI martingale.
\s

The proof needs the following result.
\s

\prop Let $M$ be a continuous local martingale with $M_0 =0$. Then $M \in M_c^2$ \emph{iff} $\avg \langle M \rangle_{\infty} < \infty$ and then $M^2 - \langle M \rangle$ is a UI martingale and $\norms{M}{M^2} = (\avg \langle M \rangle_{\infty})^{1/2}$. (Proof in ES)
\s

\thm \emph{(Novikov)} Let $M$ be a continuous local martingale with $M_0 = 0$. Then $\avg (e^{\frac{1}{2} \langle M \rangle_{\infty}})< \infty$ implies that $\mathscr{E}(M)$ is a UI martingale.

(not proving)
\s

\corr \emph{(corollary of Girsanov's Theorem)} Let $B$ be a standard Brownian motion (under $\prob$) and let $L$ be a continuous local martingale with $L_0 = 0$ such that $\mathscr{E}(L)$ is a UI martingale. Then $\tilde{B} = B- \langle B, L \rangle$ is a standard Brownian motion under the measure $\mathbb{Q}$ where
\begin{align*}
\frac{d\mathbb{Q}}{d\mathbb{P}} = \mathscr{E}(L)_{\infty}
\end{align*}
\s

\textbf{Example :} too long...

\subsection{The Cameron-Martin formuala}

\defi canonical Wiener space, canonical version of Brownian motion, Cameron-Martin space
\s

\emph{Exercise :} $\mathscr{H}$ is a Hilbert space with inner product
\begin{align*}
(h, f)_{\mathscr{H}} = \int_0^{\infty} \dot{h}(s)\dot{f}(s) ds
\end{align*} 
The dual space of $\mathscr{H}$ can be identified with
\begin{align*}
\mathscr{H}^* = \{\mu \in \mathscr{M}(\reals_+) : \int_0^{\infty }(s\wedge t)\mu(ds) \mu(dt) = (\mu, \mu)_{\mathscr{H}^*}< \infty, \mu(\{0\})=0\}
\end{align*}
in the sense that for any $l: \mathscr{H} \rightarrow \reals$ bounded and linear, there is $\mu \in \mathscr{H}$ such that $l(h) = \int_0^{\infty} h(t) \mu(dt)$ and vice-versa.
\s

\emph{Remark :} We would like to think of a Brownian motion as the standard Gaussian measure on $\mathscr{H}$. This measure does not exist. But the next theorem shows it almost does.
\s

\thm \emph{(Cameron-Martin)} Let $h\in \mathscr{H}$ and define $P^h$ by ($P^h$ is going to be a canonical measure on the Wiener space)
\begin{align*}
P^h(A) = P \big( \{ w\in W : w+ h \in A \} \big)
\end{align*}
for $A\in \mathscr{W}$. Then the measure $P^h$ is absolutely continuous with respect to the Wiener measure $P$ and
\begin{align*}
\frac{dP^h}{dP} = \exp \Big( \int_0^{\infty} \dot{h}(s) dX_s - \frac{1}{2}\int_0^{\infty} \dot{h}(s)^2 ds \Big)
\end{align*}

\section{Stochastic Differential Equations}

\subsection{Notions of Solutions}


\defi stochastic diffrential equation (SDE) $E(\sigma, b)$, weak solution, strong solution, weak uniqueness(uniqueness in law), pathwise uniqueness, 

\textbf{Example :} \emph{(Tanaka)} The SDE
\begin{align*}
dX_t = \text{sign}(X_t) dB_t, \quad X_0=x \call{\text{TK}}
\end{align*}
where $\text{sign}(x) = 1$ if $x>0$, $\text{sign}(x) =-1$ if $x\leq 0$, has a weak solution that is unique in law, but it is pathwise uniqueness does not hold. 
\s

\thm \emph{(Pathwise uniqueness for SDEs with Lipschitz coefficients)} Suppose that $b$ and $\sigma$ are \emph{locally Lipschitz} (in space variable), \textit{i.e.}, for each $n >0$, there exists $K_n >0$ such that forall $|x|, |y| \leq n$, $t\geq 0$, has 
\begin{align*}
&|b(t,x) - b(t, y)| \leq K_n |x-y| \quad \text{and} \\
&|\sigma(t,x) - \sigma(t,y)| \leq K_n |x-y|.
\end{align*}
Then pathwise uniqueness holds for $E(\sigma,b)$.
\s

\statement{Gronwall's Lemma} (on \emph{Example Sheet \#3}) Let $T>0$ and let $f:[0, T] \rightarrow \reals$ be non-negative \emph{bounded} Borel function. Assume $f(t) \leq a + b \int_0^t f(s) ds$ for all $t\leq T$. Then
\begin{align*}
f(t) \leq ae^{bt} \quad \text{for all } t\leq T
\end{align*}
(see ES 3)

\subsection{Strong existence for Lipschitz coefficients}

Recall, we denote $E(\sigma, b)$ for $dX_t = b(t, X_t) dt + \sigma(t, X_t) dB_t$.
\s

\thm Assume $b$ and $\sigma$ are globally Lipschitz, \textit{i.e.} there is $K>0$ such that for all $x,y\in \reals^d$, $t\geq 0$,
\begin{align*}
|b(t, x)- b(t,y)| \leq K|x-y|, \quad |\sigma(t,x) - \sigma(t,y)| \leq K|x-y|
\end{align*}
For any $(\Omega, \FF, (\FF_t), \prob)$ (obeying usual condition), any $(\FF_t)$-Brownian motion $B$, any $x\in \reals$, there is a unique strong solution to $E_x(\sigma, b)$.
\s

\prop Under the assumptions of the theorem, let $X^{x}$ be the solution with initial condition $X_0^x =x$. Then for any $p\geq 2$,
\begin{align*}
\avg \big( \sup_{s\leq t} |X_t^x - X_s^y|^p \big) \leq C_p |x-y|^p e^{C_p (t\vee 1)^p t}
\end{align*}
\s

We need :
\s

\lem \emph{(Burkholder-Davis-Gundy (BDG) inequality)} For every real $p>0$, there exists $C_p>0$ depending only on $p$ such that, for every continuous local martingale $M$ with $M_0 =0$ and every stopping time $T$,
\begin{align*}
\avg [\sup_{0\leq s\leq T}(M^s)^p] \leq C_p \avg [ \langle M \rangle_T^{p/2}]
\end{align*}
\s

Strong solution can be considered functions of Brownian motion in the following sense. Recall the \emph{($d$-dimensional) Wiener space} $(W^d, \mathscr{W}^d, P^d)$ where
\begin{align*}
W^d = C(\reals_+, \reals^d), \quad \mathscr{W} = \sigma(X_t^i : \in \reals_+, i=1, \cdots, d), \quad \text{where }X_t(w) = w(t) \text{ for } w\in W^d
\end{align*}
and $P^d$ is the probability measure on $(W^d, \mathscr{W}^d)$ such that $(X_t)_{t\geq 0}$ is a standard Brownian motion with $X_0 =0$.

\quad The space $C(\reals_+, \reals^d)$ can be given the topology of uniform convergence on compact intervals. This topology is induced by the metric
\begin{align*}
d(w, \tilde{w}) = \sum_{k=1}^{\infty} \alpha_k (\snorms{w - \tilde{w}}{L^{\infty}([0, t];\reals^d)} \wedge 1)
\end{align*}
for any seqeunce $(\alpha_k) \subset \reals_+$ with $\sum_{k=1}^{\infty} \alpha_k =1$. 
\s

\emph{Remark :} This metric makes $C(\reals_+ \reals^d)$ a complete separable metric space (a so called \emph{Polish space}).
\s

\thm Under the assumptions of the last theorem (strong solution for Lipschitz coefficients), for $x\in\reals^d$, there exists maps
\begin{align*}
F_x : W^m = C(\reals_+ , \reals^m) \rightarrow W^d = C(\reals_+, \reals^d)
\end{align*}
measurable with respect to the completion of $\mathscr{W}^m$ on $W^m$ and w.r.t. $\mathscr{W}^d$ on $W^d$ such that
\begin{itemize}
\item[(i)] $\forall t\geq 0$, $F_x(w)_t$ is a measurable function of $\sigma (w(s) : s\leq t)$ for $P^d$-a.s. $w \in W^m$.
\item[(ii)] $\forall w\in C(\reals_+, \reals^m) : x\in \reals^d \mapsto F_x(w) \in C(\reals_+, \reals^d)$ is continuous.
\item[(iii)] $\forall x\in \reals^d$, $\forall (\Omega, \FF, (\FF_t)_{t\geq 0}, \prob)$ satisfying the usual conditions, every $(\FF_t)$-Brownian motion $\hat{B}$ with $\hat{B}_0 =0$, the unique solution to $E_x(\sigma, b)$ is $\hat{X}_t = F_x(\hat{B})_t$.
\item[(iv)] In the set-up of (iii), if $U$ is $\FF_0$-measurable, then $F_U(\hat{B})_t$ is the unique solution to $E(\sigma, b)$ with $X_0 =U$.
\end{itemize}
(Such $F$ is called the \textbf{It\^o map}.)
\s

\corr The solutions to $E_x(\sigma, b)$ can be constructed for all $x\in \reals^d$ simultaneously such that a.s. $X^x$ is continuous in the initial condition.
\begin{p}
\pf Direct from the theorem.
\end{p}

\subsection{Some examples of SDEs}

Describe the following.

\subsubsection*{Geometric Brownian motion}

\subsubsection*{The Ornstein-Uhlenbeck process}

Let $X_t$ be an Ornstien-Uhlenbeck process
\s

\fact If $X_0 =x$, then $\avg(X_t) = e^{-\lambda t} x$, $\text{Cov}(X_t, X_s) = \frac{1}{2\lambda} (e^{-\lambda |t-s|}- e^{-\lambda |t+s|})$.
\begin{p}
\pf Clearly, $\avg X_t = e^{-\lambda t} \avg X_0 + \avg \int_0^t e^{-\lambda (t-s)}dB_s = e^{-\lambda t} \avg X_0$. Also, by \emph{It\^o isomtery},
\begin{align*}
\text{Cov}(X_t, X_s) &= \avg \big( (X_t -\avg X_t)(X_s -\avg X_s) \big) \\
&= \avg \Big( \int_0^t e^{-\lambda (t-u)} dB_u \int_0^s e^{-\lambda (s-u)} dB_u \Big) \\
&= \int_0^{\infty} \charac_{u<t} e^{-\lambda (t-u)} \charac_{u<s} e^{-\lambda (s-u)} du \\
&= e^{-\lambda(t+s)} \int_0^{t+s} e^{2\lambda u} du  = \frac{1}{2\lambda} e^{-\lambda(t+s)} (e^{2\lambda(s\wedge t)}-1)
\end{align*} 
\eop
\end{p}
\s

\corr $X_t \sim N (e^{-\lambda t}x, \frac{1-e^{-2\lambda t}}{2\lambda})$
\s

\fact If $X_0 \sim N(0, \frac{1}{\lambda})$, then $X_t \sim N(0, \frac{1}{2\lambda})$ for all $t>0$, and $X_t$ is a \emph{stationary} Gaussian process with $\text{Cov}(X_s, X_t) = \frac{1}{2\lambda}e^{-\lambda |t-s|}$
\s

\subsection{Local Solutions}

\prop \emph{(Local It\^o formula)} Let $X = (X^1, \cdots, X^d)$ be semimartingales. Let $U\subset \reals^d$ be open, and let $f: U\rightarrow \reals^d$ be $C^2$. Set $T =\inf\{ t\geq 0: X_t \not\in U\}$. Then for all $t<T$,
\begin{align*}
f(X_t) = f(X_0) + \sum_{i=1}^d \frac{\pa f}{\pa x^i} (X_s) dX_s^i + \frac{1}{2}\frac{\pa^2 f}{\pa x^i \pa x^j}(X_s) d\langle X^i, X^j \rangle_s
\end{align*}
\s

\textbf{Example :} Let $B$ be a standard Brownian motion with $B_0 =1$ (in dimension 1), then
\begin{align*}
\sqrt{B_t} = 1+\frac{1}{2} \int_0^t B_s^{-1/2}dB_s - \frac{1}{8} \int_0^t B_s^{-3/2} ds
\end{align*}
for $t< T= \inf\{t\geq 0: B_t = 0 \}$.
\s

\thm Let $U\subset \reals^d$ be open and $b: \reals_+ \times U \rightarrow \reals^d$ and $\sigma : \reals_+ \times U \rightarrow \reals^{d\times m}$ be \emph{locally Lipschitz continuous}. Then for every $(\Omega, \FF, (\FF_t)_t, \prob)$, a Brownian motion $B$ adapted to this filtration, and every $x\in U$, there exists a stopping time $T$ such that, for $t< T$,
\begin{align*}
X_t = x+ \int_0^t b(s, X_s) ds + \int_0^t \sigma(s, X_s) dB_s
\end{align*}
where $T$ is such that for all $K\subset U$ compact, we have $\sup\{t< T : X_t \in K \} < T$. 

\quad Such $T$ is called the \textbf{explosion time}.
\s

\textbf{Example :} Consider the SDEs
\begin{align*}
dX_t^i = -\nabla_i H(X_t) dt + dB_t^i, \quad X_0 =x
\end{align*}
Assume that there are $a\geq 0$, $b\geq 0$ such that
\begin{align*}
x\cdot \nabla H(x) \geq - a|x|^2 -b
\end{align*}
Then, the SDE has a global solution, \textit{i.e.} $T=\infty$ a.s.
\begin{p}
\pf Let $T_n = \inf \{t\geq 0 : |X_t|^2 >n \}$. Then by \emph{It\^o's formula} to $X^{T_n}$,
\begin{align*}
\avg |X_{t\wedge T_n}|^2 & = \avg |X_0|^2 - \avg \Big( 2 \int_0^{t\wedge T_n} X_s \cdot \nabla H(X_s) ds - t\wedge T_n \Big) \\
& \leq \avg |X_0|^2 + 2a \avg \Big( \int_0^{t\wedge T_n} |X_s|^2 ds \Big) + (1+ 2b) \avg (t\wedge T_n) \\
& \leq \avg |X_0|^2 + (1+2b)t + 2a\int_0^t \avg |X_{s\wedge T_n}|^2 ds  
\end{align*}
By \emph{Gronwall's lemma},
\begin{align*}
\avg |X_{t\wedge T_n}|^2 \leq (\avg |X_0^2| + (1+2b)t) e^{2at}
\end{align*}
If $\prob(T< \infty)>0$, then for sufficiently large $t$, $|X_{t\wedge T_n}|^2 \rightarrow \infty$ as $n\rightarrow \infty$ with positive probability, so it follows that $\prob(T< \infty) =0$.

\eop
\end{p}
\s

\section{Applications to PDEs and Markov Processes}

\subsection{Prababilistic representations of solutions to PDEs}

\textbf{Exercise :} Let $b: \reals^d \rightarrow \reals^d$ and $\sigma : \reals^d \rightarrow \reals^{d\times m}$ be (locally) bounded Borel functions and let $x\in \reals^d$. Assume that $X$ is a solution to $E_x(\sigma, b)$. Then for every $f\in C^1(\reals_+) \otimes C^2(\reals^d)$,
\begin{align*}
M_t^f = f(t, X_t) - f(0, X_0) - \int_0^t \Big( \frac{\pa}{\pa s} + L \Big) f(s, X_s) ds
\end{align*}
is a continuous local martingale where
\begin{align*}
Lf (y) = \frac{1}{2} \sum_{i,j=1}^d a_{ij}(y)\frac{\pa^2 f}{\pa y^i \pa y^j} + \sum_{i=1}^d b_i(y)\frac{\pa f}{\pa y^i}
\end{align*}
where $a(y) = \sigma (y)\sigma(y)^T \in \reals^{d\times d}$.
\s

\defi The $L$ is called the \textbf{(infinitesimal) generator} of $X$.
\s

\textbf{Example :}
\begin{i}
\item $dX =dB$, a Brownian motion has $L =\frac{1}{2}\lap$.
\item $dX = -Xdt + dB$, an Orstein-Uhlenbeck process has $L = \frac{1}{2} \lap - x\cdot \nabla$.
\end{i}
\s

\subsubsection*{Drichlet-Poisson problem}
Let $U\subset \reals^d$, $U\neq \phi$ be open and bounded. Given $f\in C(\bar{U})$ and $g\in C(\pa U)$, a (DP) asks to find $u\in C^2(\bar{U})= C^2(U) \cap C(\bar{U})$ such that
\begin{align*}
\begin{cases}
-Lu(x) = f(x) \quad & \text{for }x\in U \\
u(x) = g(x) \quad & \text{for } x\in \pa U
\end{cases} \call{\text{DP}}
\end{align*}
This is called a \textbf{Poisson problem} if $f=0$ and called a \textbf{Dirichlet Problem} if $g=0$.
\s

\defi uniform ellipticity
\s

\thm Assume that $U$ has a smooth boundary, that $a$ and $b$ are H\"older continuous functions, and that $a$ is uniformly elliptic. Then for every H\"older continuous $f: \bar{U} \rightarrow \reals$ and every continuous $g: \pa U \rightarrow \reals$, (DP)  has a solution.

\emph{[See PDE textbooks. Can also use probabilistic method to prove this.]}
\s

\thm Let $U \subset \reals^d$ be open, bounded and non-empty. Let $b$ and $\sigma$ be bounded measurable, assume $a = \sigma \sigma^T$ is uniformly elliptic and let $u$ be the solution of (DP) with coefficients $\sigma$ and $b$. Let $x\in U$, let $X$ be a solution to $E_x(\sigma, b)$. Let $T_U = \inf\{t\geq 0 : X_t \not\in U\}$. Then $\avg [ T_U ] < \infty$ and
\begin{align*}
u(x) = \avg_x \Big( u(X_{T_U}) - \int_{0}^{T_U} Lu(X_s) ds \Big) = \avg_x \Big( g(X_{T_U}) + \int_0^{T_U} f(X_s) ds  \Big)
\end{align*}
\s

\subsubsection*{Cauchy Problem}

Given $f\in C_b^2(\reals^d)$, find $u\in C(\reals_+) \otimes C^2(\reals^d)$ such that
\begin{align*}
\begin{cases}
\frac{\pa u}{\pa t} = Lu \quad & \text{on } (0, \infty) \times \reals^d \\
u(0, \cdot) =f \quad & \text{on }\reals^d
\end{cases} \call{\text{CP}}
\end{align*}
where $L$ is given as above.
\s

\thm For $f\in C_b^2(\reals^d)$, there exists a solution to (CP).

\emph{[Again, refer to a standard PDE texts, such as Evans.]}
\s

\thm Let $u$ be a (bounded) solution to (CP). Let $x\in \reals^d$, let $X$ be any solution to $E_x(\sigma, b)$, $0\leq s\leq t$, then
\begin{align*}
\avg(f(X_t ) | \FF_s) = u(t-s, X_s)
\end{align*}
In particular,
\begin{align*}
\avg_x f(X_t) = u(t, x)
\end{align*}
\s

\thm \emph{(Feynman-Kac formula)} Let $L$, $b$, $\sigma$ as before. Let $f\in C_b^2(\reals^d)$, $V\in C_b(\reals^d)$ and suppose that $u: \reals_+ \times \reals^d \rightarrow \reals$ satisfies
\begin{align*}
\begin{cases}
\frac{\pa u}{\pa t} = Lu + Vu \quad &\text{on } \reals_+ \times \reals^d\\
u(0, \cdot) = f\quad &\text{on }\reals^d
\end{cases}
\end{align*}
($Vu$ here is just a pointwise multiplication). Let $X$ be a solution to $E_x(\sigma, b)$ for some $x\in \reals^d$. Then for all $t\geq 0$,
\begin{align*}
u(t,x) = \avg_x \Big[ f(X_t) \exp \Big( \int_0^t V(X_s) ds \Big) \Big]
\end{align*}

\subsection{Markov property}

Let $B(\reals^d)$ be the Banach space of \textbf{bounded Borel functions} on $\reals^d$, with $\snorms{f}{} = \sup_{x\in \reals^d} |f(x)|$ for $f\in B(\reals^d)$.
\s

\defi 
\begin{itemize}
\item[(i)] A collection of bounded linear operators $Q_t$ on $B(\reals^d)$ is a \textbf{transition semigroup} if $Q_t f \geq 0$ if $f\geq 0$ (pointwise), $Q_t \charac=\charac$, $\snorms{Q_t}{} \leq 1$, and
\begin{align*}
Q_{t+s} = Q_t Q_s \quad \forall t,s\geq 0
\end{align*}
\item[(ii)] An $(\FF_t)$-adapted process $X$ is a \textbf{Markov process} with transition semigroup $(Q_t)_t$ if
\begin{align*}
\avg(f(X_{s+t})|\FF_s) = Q_t f(X_s) \quad \forall s,t\geq 0, \,\, f\in B(\reals^d)
\end{align*}
\end{itemize}
\s

\thm Let $b: \reals^d \rightarrow \reals$, $\sigma : \reals^d \rightarrow \reals^{d\times m}$ be \emph{Lipschitz} (this can be weakened). Assume $X$ is a solution to $E(\sigma, b)$ on some $(\Omega, \FF, (\FF_t), \prob)$ and $B$. Then $X= (X_t)_{t\geq 0}$ is a Markov process with semigroup
\begin{align*}
Q_t f(x) = \avg (f(X_t^x)) = \int f(F_x(w)_t) P^m(dw)
\end{align*}
where $X_t^x$ is an arbitrary solution to $E_x(\sigma,b)$, and $F_x$ is the \emph{It\^o solution map}, $P^m$ is the Wiener measure.
\s

\defi Let $Q_t$ be the transition semigroup, invariant probability measure, reversible probability measure.
\s

\fact Reversibility of $\mu$ implies it is invariant. (Take $g=1$ and use $Q_t 1=1$.)
\s

\textbf{Example :} Consider the transition semigroup associated to the SDE, with suitable on $H$,
\begin{align*}
dX_t = -\frac{1}{2}\nabla H(X_t) dt + dB_t
\end{align*}
(Note thet, if taking $H(x)=\lambda |x|^2$, this gives an Orstein-Uhlenbeck process.) Then the measure $\mu(dx) = \frac{1}{Z} e^{-H(x)}dx$, where $Z= \int e^{-H(x)}dx$ is reversible for $(*)$.


\lem Assume that the explosion time for $(*)$ is infinite. Then for $f: C([0, T], \reals^)d\rightarrow \reals$,
\begin{align*}
\avg \Big( f(X|_{[0, T]}) \Big) = \avg^{\text{BM}} \Big[ f(X|_{[0,T]}) \exp \Big( \frac{1}{2} H(X_0) - \frac{1}{2} H(X_T) - \int_0^T (\frac{1}{8} |\nabla H|^2 - \frac{1}{4} \lap H) (X_s) ds \Big) \Big]
\end{align*}
(where $\avg^{\text{BM}}$ takes average over law under which $X$ is a Brownian motion with same initial condition.)

(not proved in the lecture)





\end{document}
