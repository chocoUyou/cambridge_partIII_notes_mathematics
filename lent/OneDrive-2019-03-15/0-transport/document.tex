\documentclass[12pt,a4paper]{article}


\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{calrsfs}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[mathscr]{euscript}
\usepackage{bm}

%%%%%%%%%%%attach pdf%%%%%%%%%%%%
%\usepackage[final]{pdfpages}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%Latin Modern Font%%%%%%%%%%%%%%%%%%%%%
\usepackage{lmodern}

\newcommand{\latinmodern}[1]{{\fontfamily{lmss}\selectfont
\textbf{#1}
}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%For writing large opertors%%%%%%%%%%%
%\usepackage{stmaryrd}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%for writing large parallel%%%%%%
\usepackage{mathtools}
\DeclarePairedDelimiter\bignorm{\lVert}{\rVert}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%for drawing commutative diagrams.%%%%%%
\usepackage{tikz-cd}  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%for changing margin
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 

\newenvironment{proof}
{\begin{changemargin}{1cm}{0.5cm} 
	}%your text here
	{\end{changemargin}
}

\newenvironment{subproof}
{\begin{changemargin}{0.5cm}{0.5cm} 
	}%your text here
	{\end{changemargin}
}

\renewenvironment{i}
{\begin{itemize} 
	}%your text here
	{\end{itemize}
}

\renewenvironment{a}
{\begin{align*}
 }
 {\end{align*}
}

\newenvironment{p}
{\begin{proof} 
	}%your text here
	{\end{proof}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%double rules%%%%%%%%%%%%%%%%%%%
\usepackage{lipsum}% Just for this example

\newcommand{\doublerule}[1][.4pt]{%
  \noindent
  \makebox[0pt][l]{\rule[.7ex]{\linewidth}{#1}}%
  \rule[.3ex]{\linewidth}{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Introduction to Optimal Transport}
\author{Lectured by Matthew Thorpe}
\date{Lent 2019}

\maketitle

\newcommand{\statement}[1]{\latinmodern{\textbf{#1)}}}

\newcommand{\thm}{\statement{Theorem}}
\newcommand{\thmnum}[1]{\statement{Theorem #1}}
\newcommand{\defi}{\statement{Definition}}
\newcommand{\definum}[1]{\statement{Definition #1}}
\newcommand{\lem}{\statement{Lemma}}
\newcommand{\lemnum}[1]{\statement{Lemma #1}}
\newcommand{\prop}{\statement{Proposition}}
\newcommand{\propnum}[1]{\statement{Proposition #1}}
\newcommand{\corr}{\statement{Corollary}}
\newcommand{\corrnum}[1]{\statement{Corollary #1}}
\newcommand{\pf}{\textbf{proof) }}

\newcommand{\norms}[2]{\bignorm[\big]{#1}_{#2}}
\newcommand{\snorms}[2]{\bignorm[\small]{#1}_{#2}}
\newcommand{\charac}{\mathrel{\raisebox{0pt}{\scalebox{1}[1.2]{$1$}} \mkern-5.5mu \raisebox{0.04pt}{\scalebox{1}[1.2]{$\_$}} \mkern-5.5mu\raisebox{2.5pt}{\scalebox{1}[0.8]{$\bm{|}$}} }}
\newcommand{\wa}[1]{ d_{ \mathrel{\scalebox{0.5}[0.5]{$W$}}^{#1}}}

\newcommand{\lap}{\triangle} %%Laplacian
\newcommand{\s}{\vspace{10pt}}
\newcommand{\bull}{$\bullet$}
\newcommand{\sta}{$\star$}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\eop}{\hfill  \textsl{(End of proof)} $\square$} %end of proof
\newcommand{\eos}{\hfill  \textsl{(End of statement)} $\square$} %end of proof

\newcommand{\call}[1]{\quad \cdots\cdots\cdots\,\,(#1)}

\newcommand{\intN}{\mathbb{Z}_N}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\abs}[1]{\big| #1 \big|}
\newcommand{\avg}{\mathbb{E}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\borel}{\mathscr{B}}
\newcommand{\EE}{\mathscr{E}}
\newcommand{\pa}{\partial}
\newcommand{\PP}{\mathscr{P}}

\renewcommand{\vec}{\underline}
\renewcommand{\bar}{\overline}

\def\doubleunderline#1{\underline{\underline{#1}}}

\newcommand{\newday}{\doublerule[0.5pt]}
\newcommand{\digression}{**********************************************************************************************}

\setlength\parindent{0pt}

\setcounter{section}{4}


\newday

(20th February, Wednesday)


\section{Semi-Discrete Optimal Transport}

Assume
\begin{i}
\item[1.] $\nu = \sum_{j=1}^n m_{j} \delta_{y_j}$, $\{m_i\}_{i=1}^n \subset [0,1]$, $\sum_{j=1}^n m_j =1$ and $\{y_j \}_{j=1}^n \subset \reals^d$.
\item[2.] $\mu \in \PP_2(\reals^d)$ has density $\rho$.  
\end{i}
\s

\definum{5.1} The \emph{Laguerre diagram (power diagram)} for a set of points $\{y_j\}_{j=1}^n$ and weight $\{w_j\}_{j=1}^n\subset \reals$ is the collection of sets
\begin{align*}
L_j = \{x\in \reals^d : |x-y|^2 - w_j < |x-y_i|^2 - w_i \,\, \forall i\neq j\}
\end{align*}
for $j=1, \cdots, n$.
\s

\emph{Comments :}
\begin{i}
\item If $w_j =0$, then the \emph{Laguerre diagram} are Voronoi cells.
\item Each $L_i$ is open.
\end{i}
\s

\textbf{Aims :}
\begin{i}
\item Show there exists optimal $T: \reals^d \rightarrow \reals^d$ that defines a Laguerre diagram.
\item and the weights $\{w_j\}_{j=1}^n$ for the optimal Laguerre diagram solve a concave varaiational problem
\begin{align*}
\max g(w) \quad \text{where } w= (w_1, \cdots w_n)
\end{align*} 
where $g$ is as defined in the next lemma.
\end{i}
\s

\lemnum{5.2} Let $\rho \in L^1(\reals^d)$ be a probability density, $\{m_j \}_{j=1}^m \subset [0,1]$ satisfy $\sum_{j=1}^m m_j =1$ and $\{y_j \}_{j=1}^m \subset \reals^d$. Then $g: \reals^n \rightarrow \reals$ defined by 
\begin{align*}
g(w) = \int_{\reals^d} \inf_j (|x-y_j|^2 - w_j) \rho(x) dx + \sum_{j=1}^n w_j m_j \call{\star}
\end{align*}
is concave.
\begin{p}
\textbf{Idea of proof :} Introduce $\gamma : \reals^n \rightarrow \{1, \cdots, n \}$. Define
\begin{align*}
G(\gamma, w) = \int_{\reals^d} \big( |x-y_{\gamma(x)}|^2 - w_{\gamma(x)}\big) \rho(x) dx + \sum_{j=1}^n w_j m_j
\end{align*}
Note (1) $w\mapsto G(\gamma, w)$ is an affine function, so concave and (2) $g(w) = \inf_{\gamma} G(\gamma, w)$. So $g$ is also concave.
\end{p}
\s

\lemnum{5.3} Define $g$ by $(\star)$ for $\rho \in L^1(\reals^d)$, $\{y_j \}_{j=1}^n \subset \reals^n$, $\{m_j \}_{j=1}^n \subset \reals$. Let $\{L_i(w)\}_{i=1}^n$ be a \emph{Laugerre diagram} with weights $w$ and points $\{y_j \}_{j=1}^n$. Then
\begin{align*}
\frac{\pa g}{\pa w_i}(w) = -\int_{L_i(w)} \rho(x) dx + m_i
\end{align*}
\begin{p}
\textbf{sketch proof)} Let $\alpha_j(x, w) = \chi_{L_j(w)}(x) (|x-y_j|^2 -w_j) \rho(x)$ so 
\begin{align*}
g(w) = \sum_{j=1}^n \Big( \int_{\reals^d} \alpha_j(x, w) dx + w_j m_j \Big)
\end{align*} 
For any $x\in L_i(w)$, we have $\chi_{L_i(w + \pm t e_i)} (x) = \chi_{L_j(w)}(x)$ for $t>0$ sufficiently small, where $e_i$ is the unit vector in $i$-direction. Moreover,
\begin{align*}
\frac{1}{t} \big( \alpha_j (x, w+ t e_i) - \alpha_j (x, w) \big) = - \chi_{L_j(w)}(x) \delta_{ij} \rho(x)
\end{align*}
Hence
\begin{align*}
\frac{\pa g}{\pa w_i}(w) = & \lim_{t\rightarrow 0^+} \frac{1}{t} \big( g(w+ t e_i) - g(w) \big) \\
=& \sum_{j=1}^n \lim_{t\rightarrow 0^+} \Big[ \int_{\reals^d} \frac{1}{t} \big( \alpha_j(x, w+ te_i) - \alpha_j(x, w) \big) dx + (w_j + \delta_{ij} t)m_j - w_j m_j \Big] \\
=& -\int_{\reals^d} \chi_{L_i(w)}(x) \rho(x) dx + m_i
\end{align*}
\eop
\end{p}
\s

Then comes the main result of the section.
\s

\thmnum{5.4} Assume $\{y_j \}_{j=1}^m \subset \reals$, $\{m_j\}_{j=1}^m \subset [0,1]$, $\sum_{j=1}^n m_j =1$ and $\nu = \sum_{j=1}^n m_j \delta_{y_j}$. Let $\mu \in \PP_2(\reals^d)$ have density $\rho$ and $g(w)$ is defined by $(\star)$, maximised by $w= (w_1, \cdots, w_n)$, and let $\{L_j\}_{j=1}^n$ be the corresponding \emph{Laguerre diagram}. Now define
\begin{align*}
&T^{\dagger}(x) = y_j \quad \text{if } x\in L_j \quad (\text{which defines }\mu\text{-a.e.}) \\
&\psi^{\dagger}(y_j) = w_j, \quad \varphi^{\dagger}(x) = \inf_{j} (|x-y_j|^2 -w_j)
\end{align*}
Then,
\begin{i}
\item[1.] $T^{\dagger}$ is a solution to the MOT problem with cost $c(x,y) = |x-y|^2$.
\item[2.] $(\varphi^{\dagger}, \psi^{\dagger})$ are an optimal pair for the Kantorovich dual problem with cost $c(x,y) = |x-y|^2$.
\end{i}
\begin{p}
\pf We first assume that $T^{\dagger}$, $\varphi^{\dagger}$ and $\psi^{\dagger}$ are admissible for the optimisation problem:
\begin{subproof}
(a) $\varphi^{\dagger} \in L^1(\mu)$, \quad (b) $T^{\dagger}_{\#}\mu = \nu$, \quad (c) $\int_{L_j} \rho(x)dx = m_j$
\end{subproof}
Assume (a), (b), (c) then
\begin{align*}
\varphi^{\dagger} (x) + \psi^{\dagger} (y_i) = \inf_j \big( |x-y_j|^2 - w_j \big) + w_i \leq |x-y_i|^2 = c(x, y_i)
\end{align*}
So $(\varphi^{\dagger}, \psi^{\dagger}) \in \Phi_c$. Now we have
\begin{align*}
\begin{array}{rl}
\mathbb{M}(T^{\dagger}) \geq & \inf_{T_{\#} \mu = \nu} \mathbb{M}(T) \quad (\text{by (b)}) \\
\geq & \min_{\pi \in \Pi(\mu, \nu)} \mathbb{K}(\pi) \\
= & \sup_{(\varphi, \psi)\in \Phi)c} \mathbb{J}(\varphi, \psi) \quad \text{by Theorem 4.1}\\
\geq & \mathbb (\varphi^{\dagger}, \psi^{\dagger}) 
\end{array}  \call{\oplus}
\end{align*}
and
\begin{align*}
\mathbb{J} (\varphi^{\dagger}, \psi^{\dagger}) =& \int_{\reals^d} \varphi^{\dagger} (x) \rho(x) dx + \sum_{j=1}^n m_j \psi^{\dagger}(y) \\
=& \sum_{j=1}^n \Big( \int_{L_j} \varphi^{\dagger} (x) \rho(x) dx + m_j \psi^{\dagger}(y_j) \Big) \\
=& \sum_{j=1}^n \int_{L_j} \Big( \big( |x-y_j|^2 - w_j \big) \rho(x) dx + m_j w_j \Big) \\
=& \sum_{j=1}^n \int_{L_j} |x- y_j|^2 \rho(x) dx \\
=& \sum_{j=1}^n \int_{L-j} |x- T^{\dagger}(x)|^2 \rho(x) dx = \mathbb{M}(T^{\dagger})
\end{align*}
Hence all inequalities in $(\oplus)$ are all equalities and in particular
\begin{align*}
\mathbb{M}(T^{\dagger}) & =\min_{T: T_{\#} \mu = \nu} \mathbb{M}(T) \\
\mathbb{J}(\varphi^{\dagger}, \psi^{\dagger}) & = \sup_{(\varphi, \psi) \in \Phi_c} \mathbb{J}(\varphi, \psi)
\end{align*}
\s

Now we are left to prove assumptions (a), (b), (c).
\begin{i}
\item[(a)] Has
\begin{align*}
-\sup_{j} w_j \leq \varphi^{\dagger}(x) \leq |x- y_i|^2 - w_i
\end{align*}
for any $i$ so $|\varphi^{\dagger}(x)| \leq 2|x|^2 + C$ for a constant $C= 2|y_1|^2 - w_1 + \sup_j w_j$ and
\begin{align*}
\norms{\varphi}{L^1(\mu)} \leq 2\int_{\reals^d} |x|^2 d\mu(x) + C < + \infty
\end{align*}
\item[(b)] Pick $i\in \{1, \cdots, n\}$. Then
\begin{align*}
\mu((T^{\dagger})^{-1} (y_i)) =& \mu(\{x: T^{\dagger}(x) = y_i \}) = \mu(L_i) = m_i \quad (\text{by (c)}) \\
=& \nu(\{y_i\})
\end{align*}
So $T^{\dagger}_{\#} \mu = \nu$ as required.
\item[(c)] Since $w$ maximises $g$, has $\frac{\pa g}{\pa w_i}(w) =0$ for each $i$, but $\frac{\pa g}{\pa w_i}(w) = - \int_{L_j} \rho(x) dx + m_j$ by \textbf{Lemma 5.3} so we have the result
\end{i} 
\eop
\end{p}
\s

\newday

(22nd February, Friday)
\s

\textbf{A proof for yesterday Example Class :} when showing $\sup_{Ax\leq b} c\cdot x = \min_{y\geq 0, A^T y =c} b\cdot y$. We first assume that $\exists y_0$ such that $y_0 \geq A^T y_0 =c$ (if not, can just define that the minimum takes value $\infty$)
\s

Correction 1 : We defined $\Xi(x) = c\cdot x$, $\Xi : E\rightarrow \reals$ by $A^T y_0 \cdot x = y_0 \cdot (Ax)$ so $\Xi$ is indeed well-defined.

Correction 2 : We should have $E^* \cong \text{Range}(A) \subset \reals^m$.
\s

\section{Existence and Characterisation of Transport Maps}

Aims in this chapter are the following.
\begin{i}
\item[(1)] To find a sufficient condition for the existence of Optimal Transport mapos for the MOT problem.
\item[(2)] To find a sufficient condition for the Monge cost to equal the Kantorovich cost.
\item[(3)] To characterise Optimal Transport maps and plans.
\end{i}
The structure would look like
\begin{i}
\item[6.1] Sate main results
\item[6.2] Background on convex analysis
\item[6.3] Prove main results
\end{i}

\subsection{Knott-Smith Optimality and Beiner's Theorem}

\defi The \textbf{subdifferential} of a convex function $\varphi$ is defined to be
\begin{align*}
\pa \varphi(z) = \{y : \varphi(z) \geq \varphi(x) + y\cdot (z-x) \quad \forall z\in \reals^d \}
\end{align*}
This is a \emph{set of slopes}.
\s

\emph{Comments :}
\begin{i}
\item[(1)] Subdifferential always exists for convex lower semi-continuous functions.
\item[(2)] if $\varphi$ is differentiable at $x$, then $\pa \varphi(x) = \{ \nabla \varphi(x) \}$.
\end{i}
\s

\thmnum{6.1} \emph{(Knott-Simith Optimality Criterion)} Let $\mu \in \mathscr{P}_2(X)$, $\nu \in \mathscr{P}_2(Y)$, $X, Y \subset \reals^d$, $c(x,y)= \frac{1}{2}|x-y|^2$. Then $\pi^{\dagger} \in \Pi(\mu, \nu)$ minimisies the KOT problem \emph{iff} $\exists \tilde{\varphi}^{\dagger} \in L^1(\mu)$ convex and lower semi-continuous such that $\text{supp}(\pi^{\dagger}) \subset \text{Gra}( \pa \tilde{\varphi}^{\dagger})$ (equivalent to having $y\in \pa \tilde{\varphi}^{\dagger}(x)$ for $\pi^{\dagger}$-a.e. $(x,y)$). Moreover the pair $(\tilde{\varphi}^{\dagger}, (\tilde{\varphi}^{\dagger})^c)$ is a minimiser of $\inf_{\tilde{\Phi}} \mathbb{J}$, where $\tilde{\Phi} = \{ (\tilde{\varphi}, \tilde{\psi}) \in L^1(\mu)\times L^1(\nu) : \tilde{\varphi}(x) + \tilde{\psi}(y) \geq x\cdot y \}$.
\s

(Previously, $\sup_{(\varphi, \psi) \in \Phi} \mathbb{J}(\varphi, \psi)$ was the dual problem. Here we are interested in the probelms $\inf_{(\tilde{\varphi}, \tilde{\psi}) \in \tilde{\Phi}} \mathbb{J}(\tilde{\varphi}, \tilde{\psi})$. The two problems can be related by : $(\tilde{\varphi}, \tilde{\psi}) \in \tilde{\Phi}$ minimises $\mathbb{J}$ over $\tilde{\Phi}$ \emph{iff} $(\varphi, \psi) \in \Phi$ maximises $\mathbb{J}$ over $\Phi$ where $\tilde{\varphi}(x) = \frac{1}{2} |x|^2 - \varphi(x)$, $\tilde{\psi}(y) = \frac{1}{2} |y|^2 - \psi(y)$.)
\s

In 1D we expect monotonicity, and the theorem is almost equivalent to monotonicity : if $x_1 \leq x_2$ then $T^{\dagger}(x_1) \leq T^{\dagger}(x_2)$. Hence $\text{supp}(\pi^{\dagger}) = \{(x,y) : x\in X, y= T(x)\}$ is \textbf{cyclically monotone} - if $(x_1, y_1), (x_2, y_2) \in \text{supp}(\pi^{\dagger})$ and $x_1\leq x_2$ then $y_1 \leq y_2$. Any cyclically monotone set can be written as a subdifferential of a convex function. This argument holds for higher dimensions and if $T$ is ``set valued".
\s

\thmnum{6.2} \emph{(Brenier's Theorem)} Let $\mu \in \mathscr{P}_2(X)$, $\nu \in \mathscr{P}_2(Y)$, $X, Y\subset \reals^d$ and $c(x,y) = \frac{1}{2} |x-y|^2$. Assume that $\mu$ does not give mass to small sets (a small set is any set with Hausdorff dimenstion at most $d-1$). Then there is a unique $\pi^{\dagger} \in \Pi(\mu, \nu)$ that minimises the KOT problem.

\quad Moreover, $\pi^{\dagger}$ satisfies $\pi^{\dagger} = (id \times \nabla \tilde{\varphi})_{\#} \mu$ where $\nabla \tilde{\varphi}$ is the unique gradient of a convex function that pushes $\mu$ forward to $\nu$ (that is, $(\nabla \tilde{\varphi})_{\#} \mu = \nu$) and $(\tilde{\varphi},\tilde{\varphi}^c)$ minimise $\mathbb{J}$ over $\tilde{\Phi}$.
\s

\emph{Comments :}
\begin{i}
\item[(1)] $\pi^{\dagger} = (id \times \nabla \tilde{\varphi})_{\#} \mu$ $\Leftrightarrow$ $d\pi^{\dagger}(x,y) = \delta_{\nabla \tilde{\varphi}(x)}(y) \times d\mu(x)$.
\item[(2)] We will show that, in \textbf{Proposition 6.5}, convex functions are differentiable Lebesgue almost everywhere. Since $\mu$ gives zero mass to ses of Lebesue measure 0, then any convex function is differentiable $\mu$ almost everywhere.
\end{i}
\s

\corrnum{6.3} Under the same assumptions as \textbf{Theorem 6.2}, $\nabla \tilde{\varphi}$ is the unique solution to the MOT problem, \textit{i.e.}
\begin{align*}
\frac{1}{2} \int_X |x- \nabla \tilde{\varphi}(x)|^2 d\mu(x) = \inf_{T: T_{\#}\mu = \nu} \frac{1}{2} \int_X |x- T(x)|^2 d\mu(x)
\end{align*}
\begin{p}
\pf Since $\min \mathbb{K} \leq \inf \mathbb{M}$, and $T^{\dagger}_{\#} \mu = \nu$ by \textbf{Theorem 6.2}, it is enough to show that $T^{\dagger} = \nabla \tilde{\varphi}$ satisfies $\mathbb{M}(T^{\dagger}) = \min \mathbb{K} = \mathbb{K}(\pi^{\dagger})$. Indeed,
\begin{align*}
\mathbb{M}(T^{\dagger}) =& \frac{1}{2} \int_X |x-T^{\dagger}(x)|^2 d\mu(x) \\
=& \frac{1}{2} \int_{X\times Y} |x- T^{\dagger}(x)|^2 d\pi^{\dagger}(x,y) \\
=& \frac{1}{2} \int_{X\times Y} |x-y|^2 d\pi^{\dagger} (x,y) \quad (\text{since } y =T^{\dagger}(x),\,\, \pi^{\dagger}\text{-a.e.}) \\
=& \mathbb{K}(\pi^{\dagger})
\end{align*}
\eop
\end{p}

\subsection{Preliminary Results for Convex Analysis}

Just in this seciton, we will write $\varphi$ rather than $\tilde{\varphi}$.
\s

Recall the \emph{Legendre-Fenchel transform}, or the convex conjugate defined by
\begin{align*}
\varphi^*(y) = \sup_{x\in \reals^d} (x\cdot y - \varphi(x))
\end{align*}
\s

\propnum{6.4} Let $\varphi : \reals^d \rightarrow \reals\cup \{\infty\}$ be proper (not identically $+ \infty$), lower semi-continuous and convex fuction. Then $\forall x, y\in \reals^d$, 
\begin{align*}
xy = \varphi(x) + \varphi^*(y) \quad \Leftrightarrow \quad y\in \pa \varphi(x)
\end{align*}
\begin{p}
\pf Note, by definition, $\varphi^*(y) \geq x\cdot y - \varphi(x)$ for all $x\in \reals^d$. So
\begin{align*}
x\cdot y= \varphi(x) + \varphi^*(y) \quad \Leftrightarrow & \quad x\cdot y \geq \varphi(x) + \varphi^*(y) \\
\Leftrightarrow & \quad x\cdot y \geq \varphi(x) + y\cdot z - \varphi(z) \quad \forall z\in \reals^d \\
\Leftrightarrow & \quad \varphi(z) \geq \varphi(x) + y\cdot(z-x) \quad \forall z\in \reals^d \\
\Leftrightarrow & \quad y\in \pa \varphi(x)
\end{align*}
\eop
\end{p}
\s

\propnum{6.5} Let $\varphi : \reals^d \rightarrow \reals \cup \{+\infty \}$ be convex, then
\begin{i}
\item[(1)] $\varphi$ is differential Lebesgue-almost everywhere on the interior of its domain and
\item[(2)] Whenever $\varphi$ is differentiable, has $\pa \varphi(x) = \{\nabla \varphi(x) \}$.
\end{i}
\s

\newday

(25th February, Monday)
\s

We use the following theorem for the proof.
\s

\statement{Rademacher's Theorem} If $U\subset \reals^d$ is open and $f:U\rightarrow \reals$ is Lipschitz continuous then $f$ is differentiable a.e.

\quad We do not prove this results.
\s

\propnum{6.5} Let $\varphi : \reals^d \rightarrow \reals \cup \{+\infty \}$ be convex, then
\begin{i}
\item[(1)] $\varphi$ is differential Lebesgue-almost everywhere on the interior of its domain and
\item[(2)] Whenever $\varphi$ is differentiable, has $\pa \varphi(x) = \{\nabla \varphi(x) \}$.
\end{i}
\begin{p}
\pf \begin{i}
\item[(1)] Let $x\in \text{int}(\text{Dom}(\varphi))$ and $\delta^*$ be such that $\overline{B(x, \delta^*)} \subset \text{int}(\text{Dom}(\varphi))$. We show that $\varphi$ is Lipschitz continuous on $B(x, \delta^*/4)$. Then by \emph{Rademacher's Theorem}, $\varphi$ is a.e. differentiable in $B(x, \delta^*/4)$. Then $\varphi$ is differentiable a.e. on $\text{int}(\text{Dom}(\varphi))$.

(a) We show $\varphi$ is uniformly bounded on $\overline{B(x, \delta^*/2)}$. Let $Q$ be a cuboid centred at $x$ with sides of legth $\sqrt{\frac{4}{d}} \delta^*$. Let $\{x_i\}_{i=1}^n$ be the corners of $Q$. Note $x\in \pa B(x, \delta^*)$ and that the set of exterme points of $Q$ is $\{x_i \}_{i=1}^d$. by the \emph{Minkowski-Careth\'{e}odory theorem} (Theorem 3.5), for each $y \in \overline{B(x, \delta^*/\sqrt{d})}$, $\exists \{\lambda_i \}_{i=1}^{2^d} \subset [0,1]$ such that $\sum_{i=1}^{2^n} \lambda_i =1$ and $y = \sum_{i=1}^{2^d} \lambda_i x_i$. So
\begin{align*}
\varphi(y) = \varphi \big( \sum_{i=1}^{2^d} \lambda_i x_i \big) \leq \sum_{i=1}^{2^d} \lambda_i \varphi(x_i) \leq \max_{i} |\varphi(x_i)| =C
\end{align*}
where the first inequality follows from convexity of $\varphi$. Now define $y' = (x-(y-x)) = 2x-y \in B(x, \delta^*/2)$. Since $x= \frac{1}{2} y' + \frac{1}{2}y$, has
\begin{align*}
& \varphi(y) \geq 2\varphi(x) - \varphi(y') \geq 2\varphi(x) - C  \\
\Rightarrow \quad & 2\varphi(x) - C \leq \varphi(y) \leq C, \quad \forall y\in \overline{B(x, \delta^*/\sqrt{d} )}
\end{align*}
so 
\begin{align*}
\norms{\varphi}{L^{\infty}(\overline{B(x, \delta^*/\sqrt{d})})} \leq \max \{C- 2\varphi(x), C\} = M < \infty
\end{align*}

(b) We show $\varphi$ is Lipschitz continuous on $B(x, \delta^*/2\sqrt{d})$. Let $x_1, x_2 \in B(x, \delta^*/2\sqrt{d})$ where $x_1 \neq x_2$. Take $x_3$ to be the intersectin of the line throgh $x_1$ and $x_2$ with $\pa B(x, \delta^*/\sqrt{d})$ and choose $x_3$ such that $x_2$ lies between $x_1$ and $x_3$. Define $\lambda = \frac{|x_2 - x_3|}{|x_1 -x_3|} \in (0,1)$. Now,
\begin{align*}
\lambda x_1 + (1- \lambda) x_3 =& \lambda x_2 + \lambda( x_1 - x_2) + (1- \lambda )x_2 + (1- \lambda)(x_3 - x_2) \\
=& x_2 + \frac{|x_2 -x_3|}{|x_1 -x_3|} (x_1 -x_2) + \frac{|x_1- x_3| - |x_2 -x_3|}{|x_2 - x_3|}(x_3 -x_2) \\
=& x_2 + \frac{1}{|x_1-x_3|} \big( |x_2 - x_3| (x_1-x_2) + |x_1 - x_2|(x_3 -x_2) \big) \\
=& x_2
\end{align*}
By convexity of $\varphi$,
\begin{align*}
\varphi(x_2) & \leq (1- \lambda) \varphi(x_3) + \lambda \varphi(x_1) \\
\Rightarrow \quad  \varphi(x_2) - \varphi(x_1) &\leq (1- \lambda)(\varphi(x_3) - \varphi(x_1)) = \frac{|x_1 -x_2|}{|x_1 -x_3|}(\varphi(x_3) - \varphi(x_1)) \\
& \leq \frac{2\sqrt{d}\times 2M}{\delta^*}|x_1 -x_2| = L|x_1 -x_2|
\end{align*}
with $L = 4\sqrt{d}M/ \delta^*$. So $\varphi$ is Lipschitz. \emph{[This proof looks very complicated but the idea is very simple!]}

\item[(2)] Let $\varphi$ be differentiable at $x$. Then
\begin{align*}
\varphi(x) + \nabla \varphi(x) \cdot (z-x) =& \varphi(x) + \lim_{h\rightarrow 0^+} \Big( \frac{\varphi(x + (z-x)h) - \varphi (x)}{h} \Big) \\
=& \varphi(x) + \lim_{h\rightarrow 0^+} \Big( \frac{\varphi((1-h)x + zh) - \varphi (x)}{h} \Big) \\
\leq & \varphi(x) + \lim_{h\rightarrow 0} \frac{(1-h)\varphi(x) + h\varphi(z) - \varphi(x)}{h} =\varphi(z) 
\end{align*}
so $\nabla \varphi(x) \in \pa \varphi(x)$.

\quad On the other hand, if $y\in \pa \varphi(x)$, then $\varphi(x) + y\cdot(z-x) \leq \varphi(x)$ for all $z\in \reals^d$ so by letting $z= x+ hw$ with $h>0$, we get
\begin{align*}
y\cdot w \leq \frac{\varphi(x+ hw) - \varphi(x)}{h}
\end{align*}
Let $h\rightarrow 0^+$, then $y\cdot w \leq \nabla \varphi(x) \cdot w$. By symmetry ($w\mapsto -w$), we also have $y\cdot w = \nabla \varphi(x) \cdot w$ for all $w\in \reals^d$. Hence $y= \nabla \varphi(x)$.
\end{i}
\eop
\end{p}
\s

\propnum{6.6} Let $\varphi : \reals \cup \{+\infty \}$ be proper. then the following are equivalent.
\begin{i}
\item[(1)] $\varphi$ is convex and lower semi-continuous.
\item[(2)] $\varphi = \psi^*$ for some proper function $\psi$.
\item[(3)] $\varphi^{**} = \varphi$.
\end{i}
\begin{p}
\pf The implications from (3) to (2) is immediate.

\quad We are left to show implication $(2)\, \Rightarrow \,(1)$. Assume (2), so $\varphi = \psi^*$ for some proper function $\psi$. Let $x_1, x_2\in \reals^d$, $t\in [0,1]$, then 
\begin{align*}
\varphi(tx_1 + (1-t)x_2) & = \sup_y \big( (tx_1 +(1-t)x_2)\cdot y - \psi(y) \big) \\
& \leq \sup_y \big( t(x_1 \cdot y) - \psi(y) \big) + \sup_y \big((1-t)(x_2 \cdot y - \psi(y) \big) \\
& = t\varphi(x_1) + (1-t) \varphi(x_2)
\end{align*}  
so $\varphi$ is convex. To show lower semi-continuity, let $x_m \rightarrow x$. Then
\begin{align*}
\liminf_{m\rightarrow \infty} \varphi(x_m) =& \liminf_{m\rightarrow \infty} \sup_y (x_m \cdot y -\psi(y)) \geq \lim_{m\rightarrow \infty} (x_m \cdot y - \psi(y)) \\
=& x\cdot y - \psi(y) \quad \text{for any } y\in \reals^d
\end{align*}
so $\liminf_{m\rightarrow \infty} \varphi(x_m) \geq \sup_y (x\cdot y - \psi(y)) = \varphi(x)$, so $\varphi$ is lower semi-continuous.
\end{p}
\s

\newday

(27th February, Wednesday)
\s

\begin{p}
\textbf{proof continued)} Next we prove implication (1) to (3). Suppose $\varphi$ is convex and lower semi-continuous. Assume $x\in \text{int}(\text{Dom}(\varphi))$ and we show $\varphi^{**}(x) = \varphi(x)$. Since $\varphi$ can be bounded below by an affine function passing through $(x, \varphi(x))$, we have $\pa \varphi(x) \neq \phi$. Let $y_0 \in \pa \varphi(x)$. By $\textbf{Proposition 6.4}$, $x\cdot y_0 = \varphi(x) + \varphi^*(y_0)$, so
\begin{align*}
\varphi(x) = x\cdot y_0 - \varphi^*(y_0) \leq \sup_{y\in \reals^n} (x\cdot y - \varphi^c(y)) =  \varphi^{**}(x)
\end{align*}
On the other hand, since $\varphi^*(y) \geq x\cdot y - \varphi(x)$ for all $y\in \reals^d$, has 
\begin{align*}
\varphi(x) \geq \sup_y (x\cdot y - \varphi^*(y)) = \varphi^{**}(x)
\end{align*}
so $\varphi(x) = \varphi^{**}(x)$ on $\text{int}(\text{Dom}(\varphi))$. See note for the case $x\not\in \text{int}(\text{Dom}(\varphi))$.

\eop
\end{p}

\subsection{Proof of the Knott-Smith Optimality Criterion}

Let $c(x,y) = \frac{1}{2} |x-y|^2$, $\mu, \nu \in \mathscr{P}_2 (\reals^d)$. We first make few observations. 
\begin{i}
\item[(A)] Let $(\varphi, \psi) \in \Phi_c$. Define $\tilde{\varphi}(x) = \frac{1}{2} |x|^2 - \varphi(x)$ and $\tilde{\psi}(y) = \frac{1}{2} |y|^2 - \psi(y)$. One can see that $\tilde{\varphi} \in L^1(\mu)$, $\tilde{\psi} \in L^1(\nu)$. and 
\begin{align*}
\tilde{\varphi}(x) + \tilde{\psi}(y) =& \frac{1}{2} |x|^2 + \frac{1}{2} |y|^2 - \varphi(x) - \varphi(y) \\
\geq & \frac{1}{2} |x|^2 + \frac{1}{2} |y|^2 - \frac{1}{2} |x-y|^2 = x\cdot y
\end{align*}
So $(\tilde{\varphi}, \tilde{\psi}) \in \tilde{\Phi}$ where $\tilde{\Phi} = \{(\tilde{\varphi}, \tilde{\psi}) \in L^1(\mu) \times L^1(\nu) : \tilde{\varphi}(x) + \tilde{\psi}(y) \geq x\cdot y\}$.

\quad Similarly if $(\tilde{\varphi}, \tilde{\psi}) \in \tilde{\Phi}$, then $(\varphi, \psi) \in \Phi_c$.

\item[(B)] If we let $M = \frac{1}{2}\int_X |x|^2 d\mu(x) + \frac{1}{2} \int_Y |y|^2 d\nu(y)$, then $\mathbb{J}(\tilde{\varphi}, \tilde{\psi}) = M - \mathbb{J}(\varphi, \psi)$ and for $\pi \in \Pi(\mu, \nu)$, has $\mathbb{K}(\pi) = M - \int_{X\times Y} x\cdot y d\pi(x,y)$. Kantorovich duality (\textbf{Theorem 4.1}) implies that
\begin{align*}
\min_{(\tilde{\varphi}, \tilde{\psi}) \in \tilde{\Phi}} \mathbb{J}(\tilde{\varphi}, \tilde{\psi})  = \max_{\pi \in \Pi(\mu, \nu)} \int_{X\times Y} x\cdot y d\pi(x,y)
\end{align*}
Also 
\begin{align*}
\pi^{\dagger} \in \Pi(\mu, \nu)\text{ minimises }\mathbb{K}\text{ over }\Pi(\mu, \nu) \,\, &\Leftrightarrow \,\, \pi^{\dagger}\text{ maximises }\int_{X\times Y} x\cdot y d\pi(x,y) \\
(\varphi, \psi) \in \Phi_c\text{ maximises }\mathbb{J}\text{ over }\Phi_c \,\, & \Leftrightarrow \,\,(\tilde{\varphi}, \tilde{\psi}) \in \tilde{\Phi}\text{ minimise }\mathbb{J}\text{ over }\tilde{\Phi}.
\end{align*}

\item[(C)] Recall that there exists maximiser $(\varphi^{cc}, \varphi^c)\in \Phi_c$ of $\mathbb{J}$. So $(\tilde{\varphi}, \tilde{\psi}) := (\frac{1}{2}|\cdot|^2 - \varphi^{cc}, \frac{1}{2}|\cdot|^2 - \varphi^c)$ minimise $\mathbb{J}$ over $\tilde{\Phi}$. Furthermore
\begin{align*}
\tilde{\psi}(y) =& \frac{1}{2}|y|^2 - \varphi^c(y) = \sup_{x\in X} \big( \frac{1}{2} |y|^2 - \frac{1}{2}|x-y|^2 + \varphi(y) \big) \\
=& \sup_{x\in X} \big( x\cdot y -\frac{1}{2} |x|^2 + \varphi(x) \big) = \sup_{x\in X} (x\cdot y - \tilde{\varphi}(x)) = \tilde{\varphi}^*(y)
\end{align*}
And
\begin{align*}
\tilde{\varphi}(x) =& \frac{1}{2} |x|^2 - \varphi^{cc}(x) = \sup_{y\in Y} \big( \frac{1}{2} |x|^2 - \frac{1}{2} |x-y|^2 + \varphi^c (y) \big) \\
=& \sup_{y\in Y}\big( \frac{1}{2}|x|^2 - \frac{1}{2}|x-y|^2 + \frac{1}{2}|y|^2 - \tilde{\varphi}^*(y) \big) \quad (\text{used previous computation}) \\
=& \sup_{y\in Y} (x\cdot y - \tilde{\varphi}^*(y)) \\
=& \tilde{\varphi}^{**}(x)
\end{align*}
By \textbf{Proposition 6.6}, $\tilde{\eta} := \tilde{\varphi}^{**}$ is convex and lower semi-continuous and $\tilde{\eta}^* = \tilde{\varphi}^{***} = \tilde{\varphi}^*$. 
\item[(D)] For $(\tilde{\varphi}, \tilde{\varphi}^*)$ with $\tilde{\varphi} \in L^1(\mu)$, we have
\begin{align*}
\int_{X\times Y} \tilde{\varphi}(x) + \tilde{\varphi}^* (y) d\pi^{\dagger}(x,y) & = \int_{X\times Y} x\cdot y d\pi^{\dagger}(x,y) \\
& \leq \frac{1}{2} \int_{X\times Y} |x|^2 + |y|^2 d\pi^{\dagger} (x,) =\frac{1}{2} \int_X |x|^2 d\mu(x) + \frac{1}{2} \int_Y |y|^2 d\nu(y)
\end{align*}
so $\mathbb{J}(\tilde{\varphi}, \tilde{\varphi}^*) < \infty$.
\item[(E)] From the result of (C), if we just prove that $\tilde{\varphi}^* \in L^1(\nu)$ whenever $\tilde{\varphi} \in L^1(\mu)$, then $(\eta, \eta^*) \in L^1(\mu) \times L^1(\nu)$ so there is a minimiser of $\mathbb{J}$ and $\tilde{\Phi}$ that takes the form $(\tilde{\eta}, \tilde{\eta}^*)$ where $\tilde{\eta}$ is convex, lower semi-continuous and is proper.
\s

To see this, assume $\tilde{\varphi}\in L^1(\mu)$. First note that $\exists x_0 \in X$ and $b_0 = \tilde{\varphi}(x_0) +1 \in \reals$ such that
\begin{align*}
\tilde{\varphi}^*(y) \geq x_0 \cdot y - \tilde{\varphi}(x_0) -1 =: x_0 \cdot y - b_0 =: f(y)
\end{align*} 
Then we have $\tilde{\varphi}^* - f(y) \geq 0$, so $\norms{\tilde{\varphi}^* - f}{L^1(\mu)} = \int_Y \big( \tilde{\varphi}^*(y) - f(y) \big) d\nu(y)$. Hence
\begin{align*}
\norms{\tilde{\varphi}^* - f}{L^1(\mu)} =& \int_Y \big( \tilde{\varphi}^*(y) - f(y) \big) d\nu(y) \\
\leq & \, \mathbb{J} (\tilde{\varphi}, \tilde{\varphi}^*) + \norms{\tilde{\varphi}}{L^1(\mu)} + \frac{1}{2} |x_0| + \frac{1}{2} \int_Y |y|^2 d\nu(y) + b_0 \\
<&  +\infty
\end{align*}
where the second line is implied by Cauchy-Schwarz ienquality. So $\tilde{\varphi}^* - f\in L^1(\nu)$ and since $f\in L^1(\nu)$, we conclude $\tilde{\varphi}^* \in L^1(\nu)$ as required.

\end{i}
\s

We now come back to the one of the two main theorems of the chapter.
\s

\thmnum{6.1} \emph{(Knott-Smith(KS) optimality criterion)} Let $\mu \in \mathscr{P}_2(X)$, $\nu \in \mathscr{P}_2(Y)$, $X, Y\in \reals^d$, $c(x,y) = \frac{1}{2} |x-y|^2$. Then $\pi^{\dagger} \in \Pi(\mu, \nu)$ minimises $\mathbb{K}$ over $\Pi(\mu, \nu)$ \emph{iff} there exists $\tilde{\varphi} \in L^1(\mu)$ convex, lower-semicontinuous such that $y\in \pa \tilde{\varphi}(x)$ for $\pi^{\dagger}$-a.e. $(x,y)$.

\quad Moreover $(\tilde{\varphi}, \tilde{\varphi}^*)$ maximises $\mathbb{J}$ over $\tilde{\Phi}$.
\begin{p}
\pf Let $\pi^{\dagger} \in \Pi(\mu, \nu)$ minimise $\mathbb{K}$ over $\Pi(\mu, \nu)$ and $(\tilde{\varphi}, \tilde{\varphi}^*) \in \tilde{\Phi}$ minimise $\mathbb{J}$ over $\tilde{\Phi}$. By \emph{Kantorovich duality},
\begin{align*}
&\int_X \tilde{\varphi}(x) d\mu(x) + \int_Y \tilde{\varphi}^*(y) d\nu(y) =\int_{X\times Y} x\cdot y d\pi^{\dagger}(x,y) \\
\Rightarrow \quad  &\int_{X\times Y} \big( \tilde{\varphi}(x) + \tilde{\varphi}^*(y) - x\cdot y\big) d\pi^{\dagger}(x,y) =0 \\
\Rightarrow \quad & \tilde{\varphi}(x) + \tilde{\varphi}^*(y) = x\cdot y \quad \text{for } \pi^{\dagger}\text{-a.e. }(x,y) \quad \text{(by property of } \tilde{Phi}) \\
\Rightarrow \quad &y\in \pa \tilde{\varphi}(x) \quad \text{for } \pi^{\dagger} \text{-a.e. }(x,y) 
\end{align*}

Conversely suppose $\pi^{\dagger} \in \Pi(\mu, \nu)$ and $\tilde{\varphi} \in L^1(\mu)$ and satisfy $y\in \pa \tilde{\varphi}(x)$ for $\pi^{\dagger}$-a.e. $(x,y)$ where $\tilde{\varphi}$ is lower semi-continuos and convex. We want to show that $\pi^{\dagger}$ is opimal for KOT problem and $(\tilde{\varphi}, \tilde{\varphi}^*)$ optimal for ``KD" problem.

\quad By \textbf{Proposition 6.4},
\begin{align*}
\int_{X\times Y} \big( \tilde{\varphi}(x) + \tilde{\varphi}^*(y) - x\cdot y \big) d\pi^{\dagger}(x,y) =0
\end{align*}
By point (D) above, we have $\tilde{\varphi}^* \in L^1(\nu)$, so $(\tilde{\varphi}, \tilde{\varphi}^*) \in \tilde{\Phi}$. Then
\begin{align*}
\min_{\tilde{\Phi}} \mathbb{J} \leq \mathbb{J}(\tilde{\varphi}, \tilde{\varphi}^*) = \int_{X\times Y} x\cdot y d\pi^{\dagger}(x,y) \leq \max_{\pi \in \Pi(\mu, \nu)} \int_{X\times Y} x\cdot y d\pi(x,y)
\end{align*}
By duality, LHS=RHS, so all inequalities above are in fact eqaulities. So $(\tilde{\varphi}, \tilde{\varphi}^*)$ minimise $\mathbb{J}$ over $\tilde{\Phi}$ and $\pi^{\dagger}$ maximises $\int_{X\times Y} x\cdot y d\pi(x,y)$ over $\Pi(\mu, \nu)$.

\eop
\end{p}

\subsection{Proof of Brenier's Theorem}

In this section, we prove the second one of the two main theorems of the chapter.
\s

\thmnum{6.2} \emph{(Brenier's Theorem)} Let $\mu \in \mathscr{P}_2(X)$, $\nu \in \mathscr{P}_2(Y)$, $X, Y\subset \reals^d$ and $c(x,y) = \frac{1}{2} |x-y|^2$. Assume that $\mu$ does not give mass to small sets (a small set is any set with Hausdorff dimenstion at most $d-1$). Then there is a unique $\pi^{\dagger} \in \Pi(\mu, \nu)$ that minimises the KOT problem.

\quad Moreover, $\pi^{\dagger}$ satisfies $\pi^{\dagger} = (id \times \nabla \tilde{\varphi})_{\#} \mu$ where $\nabla \tilde{\varphi}$ is the unique gradient of a convex function that pushes $\mu$ forward to $\nu$ (that is, $(\nabla \tilde{\varphi})_{\#} \mu = \nu$) and $(\tilde{\varphi},\tilde{\varphi}^c)$ minimise $\mathbb{J}$ over $\tilde{\Phi}$.
\s

\newday

(1st March, Friday)
\s

(We have a lecture on the 15th March)

(Exercies Class 3 is at Tuesday, 5th March, 4pm, MR13)
\s

\begin{p}
\textbf{proof of Brenier's theorem)} Let $\pi^{\dagger} \in \Pi(\mu, \nu)$ minimise KOT problem and $\{\pi^{\dagger}(\cdot |x) \}_{x\in X}$ be the disinteration of measure,
\begin{align*}
\pi^{\dagger}(A\times B) = \int_A \pi^{\dagger}(B|x) d\mu(x)
\end{align*}
By \textbf{Theorem 6.1}, for $\mu$-a.e. $x\in X$ and $\pi^{\dagger}(\cdot |x) $-a.e. $y\in Y$ we have $y\in \pa \tilde{\varphi}(x)$ where $\tilde{\varphi}$ minimises $\mathbb{J}$ over $\tilde{\Phi}$. Since $\pa \tilde{\varphi}(x) = \{\nabla \tilde{\varphi}(x)\}$ for $\mu$-a.e. $x\in X$, has $y = \nabla \tilde{\varphi}(x)$ for $\mu$-a.e. $x\in X$ and $\pi^{\dagger}(\cdot|x)$-a.e. $y\in Y$. Hence $\pi^{\dagger}(\cdot | x) = \delta_{\nabla \tilde{\varphi}(x)}$. So
\begin{align*}
\pi^{\dagger}(A\times B) = \int_A  \charac_{\nabla \tilde{\varphi}(x) \in B} d\mu(x) =& \mu \big( \{ x: x\in A \text{ and } \nabla \tilde{\varphi}(x) \in B \}\big) \\
= & ( \text{Id} \times \nabla \tilde{\varphi})_{\#} \mu(A \times B)
\end{align*}
so $\pi^{\dagger} = (Id \times \nabla \tilde{\varphi})_{\#} \mu$. Also
\begin{align*}
\nu(B) = \pi^{\dagger}(X\times B) &= ( \text{Id} \times \nabla \tilde{\varphi})_{\#} \mu(X\times B) \\
&= \mu \big( \{x: (x, \nabla \tilde{\varphi}(x)) \in X\times B  \}\big) \\
&= \mu \big(\{ x: \nabla \tilde{\varphi}(x) \in B \}\big) = (\nabla \tilde{\varphi})_{\#} \mu(B)
\end{align*}

For uniqueness, suppose $\bar{\varphi}$ is convex and satisfies $(\nabla \bar{\varphi})_{\#}\mu =\nu$. We show $\nabla \tilde{\varphi} = \nabla \bar{\varphi}$ a.e. By \textbf{Thoerem 6.1},
\begin{align*}
\bar{\pi} = (Id \times \nabla \bar{\varphi})_{\#} \mu
\end{align*}
is an optimal transport plan and $(\bar{\varphi}, \bar{\varphi}^c)$ minimise $\mathbb{J}$ over $\tilde{\Phi}$. So
\begin{align*}
& \int_X \bar{\varphi} d\mu + \int_{Y}\bar{\varphi}^c d\nu = \int_X \tilde{\varphi} d\mu + \int_Y \tilde{\varphi}^c d\nu \\
\Rightarrow \quad & \int_{X\times Y} (\bar{\varphi}(x) + \bar{\varphi}^c (y)) d\pi^{\dagger}(x,y) = \int_{X\times Y} (\tilde{\varphi}(x) + \tilde{\varphi}^c (y)) d\pi^{\dagger}(x,y) \\
&=\int_{X\times Y} x\cdot y \, d\pi^{\dagger} (x,y)  \quad \text{by } \textbf{Proposition 6.4}\\
&=\int_{X\times Y} x\cdot y \, d(\text{Id} \times \nabla \tilde{\varphi})_{\#} \mu(x,y)\\
&=\int_X x\cdot \nabla \tilde{\varphi}(x) d\mu (x)
\end{align*}
Also,
\begin{align*}
\int_{X\times Y} \big( \bar{\varphi}(x) +\bar{\varphi}^*(y) \big) d\pi^{\dagger}(x,y) = \int_X  \big( \bar{\varphi}(x) + \bar{\varphi}^*(\nabla \tilde{\varphi}(x)) \big) d\mu(x)
\end{align*}
so
\begin{align*}
\int_X \big( \bar{\varphi}(x) + \bar{\varphi}^*(\nabla \tilde{\varphi}(x)) - x\cdot \nabla \tilde{\varphi}(x)\big)d\mu(x)=0
\end{align*}
but $\bar{\varphi}(x) + \bar{\varphi}^*(y) \geq x\cdot y$, so
\begin{align*}
\bar{\varphi}(x) + \bar{\varphi}^*(\nabla \tilde{\varphi(x)}) - x\cdot \nabla \tilde{\varphi}(x) =0 \quad \mu\text{-a.e. } x\in X
\end{align*}
By \textbf{Proposition 6.4}, $\nabla \tilde{\varphi}(x) \in \pa \bar{\varphi}(x) = \{ \nabla \bar{\varphi}(x)\}$, so by \textbf{Propositioin 6.5}, $\nabla \tilde{\varphi}(x) = \nabla \bar{\varphi}(x)$ for $\mu$-a.e. $x\in X$.

\eop 
\end{p}

\section{Wasserstein Distances}

In this chapter, we assum $c(x,y)  = |x-y|^p$ with $p\in [1, +\infty)$ with $X=Y \subset \reals^d$.
\s

The objectives in this chapter are
\begin{i}
\item[(1)] Define the \emph{Wasserstein distnace} $\wa{p}$ and show it is a metric in $\mathscr{P}_p(X)$.
\item[(2)] Show equivalence of $\wa{p}$ and $d_{W^q}$ when $X$ is bounded.
\item[(3)] Relaionship with weak-* topology.
\item[(4)] Show that $(\mathscr{P}_p(X), \wa{p})$ is a \emph{geodesic space} (to be defined later).
\end{i}
\s

Another interesting cost function that does not fit into the Wasserstein framework is $c(x,y) = 1_{x\neq y}$.
\s

\propnum{7.1} Let $\mu, \nu \in \mathscr{P}(X)$, $X\subset \reals^d$, $c(x,y) =\charac_{x\neq y}$. Then
\begin{align*}
\inf_{\pi \in \Pi(\mu, \nu)} \mathbb{K}(\pi) = \frac{1}{2} \snorms{\mu - \nu}{TV}
\end{align*}
where $\snorms{\mu}{TV} = 2\sup_A |\mu(A)|$.
\begin{p}
\pf By the \emph{KR theorem} (\textbf{Theorem 4.13}),
\begin{align*}
\min_{\pi \in \Pi(\mu, \nu)} \mathbb{K}(\pi ) &= \sup \big\{ \int_X fd(\mu - \nu) : f\in L^1(|\mu-\nu|), \norms{f}{Lip} \leq 1 \big\} \\
&= \sup \big\{ \int_X fd(\mu - \nu) : 0\leq f(x)\leq 1 \,\, \forall x\in X \big\}
\end{align*}
where $\snorms{f}{Lip}$ is given in terms of $c$ rather than a usual metric. Write $\mu - \nu = (\mu - \nu)_+ - (\mu - \nu )_-$ where $(\mu - \nu )_{\pm} \in \mathscr{M}_+(X)$ and are singular (\textit{i.e.} has disjoint support). We may achieve the supremum when we choose $f(x) = 1$ if $x\in \text{supp}(\mu - \nu )_+$ and $f(x)=0$ otherwise. Then 
\begin{align*}
\min_{\pi \in \Pi(\mu, \nu)}\mathbb{K}(\pi) = (\mu - \nu )_+ (X)
\end{align*}
For $TV$, the optimal choice is $A = \text{supp}((\mu - \nu )_+)$ and $\snorms{\mu- \nu}{TV} =2(\mu - \nu )_+ (X)$. So $\min \mathbb{K} = \frac{1}{2} \snorms{\mu - \nu }{TV}$.

\eop
\end{p}

\subsection{Wasserstein Distances}

We work on the space of measures with bounded $p^{th}$ moment,
\begin{align*}
\mathscr{P}_p(X) = \{ \mu \in \mathscr{P}(X) : \int_X |x|^p d\mu (x) < +\infty \}
\end{align*}
If $X$ is bounded, then $\PP_p(X) = \PP(X)$.
\s

\definum{7.2} Let $\mu, \nu \in \PP_p(X)$, then the \textbf{$p$ -Wasserstien distance} is defined to be
\begin{align*}
\wa{p}(\mu, \nu)= \min_{\pi \in \Pi(\mu, \nu)} \Big( \int_{X\times Y} |x-y|^p d\pi(x,y) \Big)^{1/p}
\end{align*}
If $\mu \in \PP_p(X)$, $\nu \in  \PP_p(X)$, then
\begin{align*}
\wa{p}(\mu, \nu)^p = \min_{\pi \in \Pi(\mu, \nu)} p\int_{X\times Y} |x|^p + |y|^p d\pi(x,y) = p\int |x|^p d\mu(x) + p \int |y|^p d\nu(y) < + \infty
\end{align*}
\s

\newday

(4th March, Monday)
\s

Recall, we defined

\definum{7.2} For $\mu, \nu\in \PP_p(X)$, the \textbf{Wasserstein distance} is defined by
\begin{align*}
\wa{p}(\mu, \nu)= \min_{\pi \in \Pi(\mu, \nu)} \Big( \int_{X\times Y} |x-y|^p d\pi(x,y) \Big)^{1/p}
\end{align*}
\s

\propnum{7.3} Let $X\subset \reals^d$. Then the distnace $\wa{p} : \PP_p(X) \times \PP_p(X) \rightarrow [0, \infty)$ is a metric.
\begin{p}
\pf \begin{i}
\item[(1)] It is easy to see that $\wa{p}(\mu, \nu) \geq 0$ for any $\mu$, $\nu$.

\item[(2)] If $\mu = \nu$, then $\pi(x,y) = \delta_y(x) \mu(x)$ satisfies $\pi \in \Pi(\mu, \nu)$ and
\begin{align*}
\wa{p}(\mu, \nu) \leq \int_{X\times Y} |x-y|^p d\pi(x,y) = 0
\end{align*}
If $\wa{p}(\mu, \nu) =0$, then $\exists \pi \in \Pi(\mu, \nu)$ such that $\int_{X\times Y} |x-y|^p d\pi(x,y) =0$ so $x=y$ for $\pi$-a.e. $x,y$. So for any function $f:X\rightarrow \reals$,
\begin{align*}
\int_X f(x) d\mu(x) = \int_{X\times X} f(x) d\pi(x,y) = \int_{X\times X} f(y) d\pi(x,y)  = \int_X f(y)d\nu(y) \call{*}
\end{align*}
Since $(*)$ holds for all $f:X\rightarrow \reals$ integrable w.r.t $\mu$ and $\nu$ (or just for $f$ continuous and bounded is sufficient), we have $\mu = \nu$.
\item[(3)] Clearly $\wa{p}(\mu, \nu)= \wa{p}(\nu,\mu)$. (To write out formally, define $s:(x,y) \mapsto (y,x)$ and observe that whenever $\pi \in \Pi(\mu, \nu)$, has $s_{\#} \Pi(\nu, \mu)$)
\item[(4)] To see the triangular inequality, we make use of \emph{glueing lemma} (\textbf{Lemma 7.4}). Let $\mu, \nu, \omega \in \PP_p(X)$ and $\pi_{XY} \in \pi(\mu, \nu)$ and $\pi_{YZ} \in \pi(\nu, \omega)$ be the optimal plans, \textit{i.e.}
\begin{align*}
\wa{p}(\mu, \nu) = \Big( \int_{X\times X} |x-y|^p d\pi_{XY}(x,y) \Big)^{1/p} \\
\wa{p}(\nu, \omega) = \Big( \int_{X\times X} |y-z|^p d\pi_{YZ}(y,z) \Big)^{1/p}
\end{align*}
Take $\gamma \in \PP(X\times Y\times Z)$ such that $P^{X,Y}_{\#} \gamma =\pi_{X\times Y}$ and $P^Y_{\#} \gamma =\pi_{YZ}$ using the \emph{glueing lemma}. Define $\pi_{XZ} = P^{X, Z}_{\#} \gamma$. Since $\pi_{XZ}(A\times X) = \gamma(A\times X \times X) = \pi_{XY}(A\times X) = \mu(A)$ and similarly $\pi_{XZ} (X\times C) = \omega(C)$, we see that $\pi_{XZ} \in \Pi(\mu, \omega)$. Now, by \emph{Minkowski's inequality},
\begin{align*}
\wa{p}(\mu, \omega) &\leq \Big( \int_{X\times X} |x-z|^p d\pi_{XZ}(x,z) \Big)^{1/p} = \Big( \int_{X\times X\times X} |x-z|^p d\gamma (x,y,z) \Big)^{1/p} \\
&\leq \Big( \int_{X\times X\times X} |x-y|^p d\gamma (x,y,z) \Big)^{1/p} + \Big( \int_{X\times X\times X} |y-z|^p d\gamma (x,y,z) \Big)^{1/p} \\
&= \Big( \int_{ X\times X} |x-y|^p d\pi_{XY} (x,y) \Big)^{1/p} + \Big( \int_{X\times X} |y-z|^p d\pi_{YZ} (y,z) \Big)^{1/p} \\
&=\wa{p}(\mu, \nu) + \wa{p}(\nu, \omega)
\end{align*}
\end{i}
\eop
\end{p}
\s

For the triangular inequality, we needed the following \emph{glueing lemma}.
\s

\lemnum{7.4} Let $X, Y, Z \subset \reals^d$, $\mu\in \PP(X)$, $\nu \in \PP(Y)$, $\omega \in \PP(Z)$, $\pi_1\in \Pi(\mu, \nu)$, $\pi_2 \in \Pi(\nu, \omega)$. Then there is a measure $\gamma \in \PP (X\times Y \times Z)$ such that $P_{\#}^{X\times Y} \gamma = \pi_1$ and $P_{\#}^{Y,Z} = \pi_{12}$ where $P^{X,Y}(x,y,z) = (x,y)$, $P^{Y,Z}(x,y,z) =(y,z)$.
\begin{p}
\pf By disintegration of measures, we can write
\begin{align*}
\pi_1 (A\times B) = \int_B \pi_1(A|y )d\nu(y), \quad \pi_2 (B\times C) = \int_B \pi_2(C|y )d\nu(y)
\end{align*}
for families $\{\pi_1(\cdot |y) \}_{y\in Y} \subset \PP(X)$ and $\{\pi_2(\cdot |y) \}_{y\in Y} \subset \PP(Z)$. Define $\gamma \in \mathscr{M}(X\times Y\times Z)$ by
\begin{align*}
\gamma(A \times B \times C) = \int_B \pi_1(A|y)\pi_2(C|y) d\nu(y)
\end{align*}
We can check 
\begin{align*}
\gamma(A\times B \times Z) &= \int_B \pi_1(A|y) \pi_2(Z|y) d\nu(y) = \pi_1(A\times B) \\
\gamma(X\times B \times C) &= \pi_2(B\times C)
\end{align*}
So $P_{\#}^{X,Y}(\gamma) = \pi_1$ and $P^{Y, Z}_{\#} \gamma =\pi_2$. It also follows that $\gamma \in \PP(X\times Y\times Z)$.

\eop
\end{p}
\s

\propnum{7.5} Let $X\subset \reals^d$. For every $p,q \in [1, +\infty)$, $q\leq p$ and any $\mu, \nu\in \PP_p(X)$, we have $\wa{p}(\mu, \nu) \geq d_{W^q} (\mu, \nu)$.

\quad Furthermore, if $X$ is bounded then $\wa{p}^p (\mu, \nu) \leq \text{diam}(X)^{p-1} \wa{1}(\mu, \nu)$ (where $\text{diam}(X) = \sup_{w, z\in X} |w-z|$)
\begin{p}
\pf The first part uses Jensen's inequality. The second part makes use of H\"older inequality.

\eop
\end{p}

\subsection{The Wasserstein Topology}

\textbf{Aim} : show $\mu_n \xrightarrow{w-^*} \mu$ \emph{iff} $\wa{p}(\mu_n,\mu) \rightarrow 0$. We start with the case when $X\subset\reals^d$ is compact.
\s

\thmnum{7.6} Let $X\subset \reals^d$ be \emph{compact}, $\{\mu_n\}_{n\in \mathbb{N}} \subset \PP(X)$ and $\mu\in \PP(X)$ and $p\in [1, \infty)$. Then
\begin{align*}
\mu_m \xrightarrow{w-^*} \mu \quad \emph{iff} \quad \wa{p}(\mu_m, \mu) \rightarrow 0
\end{align*}
\begin{p}
\pf By \textbf{Proposition 7.5}, it is enough to prove the result for $p=1$.

\quad Assume $\wa{1}(\mu_m, \mu)\rightarrow 0$. by the \emph{Kantorovich-Rubinstein Theorem} (\textbf{Theorem 4.13}),
\begin{align*}
\wa{1}(\mu, \nu) = \sup \Big\{ \int_X \varphi d(\mu -\nu) : \varphi\in L^1(|\mu-\nu|),  |\varphi(x) - \varphi(y)| \leq |x-y| \Big\}
\end{align*}
Let $\varphi$ be Lipschitz with $\snorms{\varphi}{Lip} >0$. Then define $\tilde{\varphi} = \frac{1}{\snorms{\varphi}{Lip}}\varphi$ so that $\tilde{\varphi}$ is 1-Lipschitz. So
\begin{align*}
\frac{1}{\snorms{\varphi}{Lip}} \int \varphi d(\mu_m -\mu) = \int \tilde{\varphi}(\mu_m - \mu) \leq \wa{1}(\mu_m, \mu) \rightarrow 0
\end{align*}
Then $\limsup_{m\rightarrow \infty} \int\varphi d\mu_m \leq \int\varphi d\mu$. Substituiting $\varphi \mapsto -\varphi$ gives the inverse inequality, so together we have
\begin{align*}
\lim_{m\rightarrow \infty} \int_X \varphi d\mu_n = \int \varphi d\mu
\end{align*}
By \emph{Portmanteau Theorem} (\textbf{Theorem 1.2}), has $\mu_m \xrightarrow{w-^*} \mu$.
\s

Conversely, assume $\mu_n \xrightarrow{w-^*} \mu$ and let $(m_k)_k \subset \mathbb{N}$ be the subsequence such that
\begin{align*}
\lim_{k\rightarrow \infty} \wa{1}(\mu_{m_k}, \mu) = \limsup_{m\rightarrow \infty} \wa{1}(\mu_m, \mu)
\end{align*}
Let $\tilde{\varphi}_{m_k}$ be 1-Lipschitz and such that $\wa{1}(\mu_{m_k}, \mu) \leq \int_X \tilde{\varphi}_{m_k} d(\mu_{m_k} - \mu) + \frac{1}{k}$. Pick $x_0 \in \text{supp}(\mu)$ and define $\varphi_{m_k}(x) = \tilde{\varphi}_{m_k}(x) - \tilde{\varphi}_{m_k}(x_0)$. So, 
\begin{align*}
\wa{1}(\mu_{m_k}, \mu) &\leq \int_X \varphi_{m_k} d(\mu_{m_k} - \mu) - \tilde{\varphi}_{m_k}(x_0) \int_X d(\mu_{m_k} - \mu) + \frac{1}{k} \\
&= \int_X \varphi_{m_k} d(\mu_{m_k} - \mu) + \frac{1}{k}
\end{align*}
Since $\varphi_{m_k}$ are 1-Lipschitz and $\varphi_{m_k}(x_0) =0$, we have $\{\varphi_{m_k}\}_{k\in \mathbb{N}}$ uniformly bounded and equi-continuous. By the \emph{Arzel\`a-Ascoli Theorem}, there is a subsequence (relabelled) such that $\varphi_{m_k} \rightarrow \varphi$ uniformly and $\varphi$ is 1-Lipschitz. Hence,
\begin{align*}
\limsup_{m\rightarrow \infty} \wa{1}(\mu_m, \mu) & \leq \limsup_{k\rightarrow \infty} \Big( \int_X \varphi_{m_k}d(\mu_{m_k} - \mu) + \frac{1}{k} \Big) \\
& =\limsup_{k\rightarrow \infty} ( \int_X (\varphi_{m_k} - \varphi) d(\mu_{m_k} - \mu) + \int_X \varphi d(\mu_{m_k} - \mu)) \\
& \leq \limsup_{k\rightarrow \infty} \norms{\varphi_{m_k} - \varphi}{\infty} + \limsup_{k\rightarrow \infty} \int_X \varphi d(\mu_{m_k} - \mu) = 0
\end{align*}
\eop
\end{p}
\s

\newday

(6th March, Wednesday)
\s

\thmnum{7.7} $\mu_n, \mu \in \PP_p(\reals^d)$. Then 
\begin{align*}
\wa{p}(\mu_n, \mu) \rightarrow 0 \quad \emph{iff} \quad \mu_n \xrightarrow{w^*} \,\, \mu\text{ and }\int_{\reals^d} |x|^p d\mu_n \rightarrow \int_{\reals^d} |x|^p d\mu
\end{align*}
\begin{p}
\pf Let $\wa{p}(\mu_n, \mu)\rightarrow 0$. By \textbf{Proposition 7.5}, $\wa{1}(\mu_n, \mu)\rightarrow 0$. Analogous to the proof of \textbf{Theorem 7.6}, we have $\int \varphi d(\mu_n - \mu) \rightarrow 0$ for all Lipschitz functions $\varphi$. So $\mu_n \xrightarrow{w^*} \mu$.

\quad To show $\int |x|^p d\mu_n(x) \rightarrow \int |x|^p d\mu(x)$, we write
\begin{align*}
\int_{\reals^d} |x|^p d\mu_n(x) = \wa{p}^p (\mu_n, \delta_0), \quad \int_{\reals^d} |x|^0 d\mu(x) = \wa{p}^p (\mu, \delta_0) 
\end{align*}
Also by triangular inequality,
\begin{align*}
\wa{p}(\mu, \delta_0) - \wa{p}(\mu, \mu_n) \leq \wa{p}(\mu_n, \delta_0) = \Big(\int |x|^p d\mu_n(x) \Big)^{1/p} \leq \wa{p}(\mu_n, \mu) + \wa{p}(\mu, \delta_0) \rightarrow \wa{p}(\mu, \delta_0)
\end{align*}
but both very LHS and the RHS converges to $\wa{p}(\mu, \delta_0)$, so $\wa{p}(\mu_n, \delta_0) \rightarrow \wa{p}(\mu, \delta_0)$ as desired.
\s

To show the converse, let $\mu_n \xrightarrow{w^*} \mu$ and $\int |x|^p d\mu \rightarrow \int |x|^p d\mu$. For any $R>0$, define $\phi_R(x) = (\min \{|x|,R\})^p$ which is continuous and bounded. So
\begin{align*}
\int_{\reals^d} |x|^p - \phi_R(x) d\mu_n \rightarrow \int_{\reals^d}|x|^p - \phi_R(x) d\mu \call{*}
\end{align*}
Now
\begin{align*}
\int_{\reals^d} (|x|^p - \phi_R(x)) d\mu(x) = \int_{|x|>R} (|x|^p -R^p) d\mu \leq \int_{|x|>R} |x|^p d\mu
\end{align*}
For any $\epsilon >0$, we can find $R>0$ such that $\int_{\reals^d} (|x|^p -\phi_R(x)) d\mu(x) < \epsilon /2$. By $(*)$, for $m$ sufficiently large, $\int_{\reals^d} (|x|^p - \phi_R(x)) d\mu_m(x) < \epsilon$. Since $(a+b)^p \geq (a^p + b^p)$ for any $a,b\geq 0$, for $|x|>R$, we have $(|x| - R)^p \leq |x|^p - R^p = |x|^p - \phi_R(x)$. So
\begin{align*}
\int_{|x|>R} (|x| - R)^p d\mu_m < \epsilon, \quad \int_{|x|>R} (|x| - R)^p d\mu < \epsilon 
\end{align*}
Let $P_R : \reals^d \rightarrow \overline{B(0, R)}$ be the projection onto $\overline{B(0, R)}$, \textit{i.e.} $P_{R} =x$ if $x\in \overline{B(0, R)}$ and $P_R(x) = xR / |x|$ is otherwise.Then $P_R$ is continuous, and $P_R =id$ on $\overline{B(0, R)}$, and for $x\not\in \overline{B(0, R)}$, we have $|x- P_R(x)| = |x| - R$. Hence
\begin{align*}
\wa{p}(\mu, (P_R)_{\#}\mu) &\leq \Big( \int_{\reals^d} |x- P_R(x)|^p d\mu(x) \Big)^{1/p} \quad \text{since } \wa{p}(\cdot, \cdot) \text{ is optimal for MOT} \\
&= \Big( \int_{|x|>R} |x- P_R(x)|^p d\mu(x) \Big)^{1/p} \\
&= \Big( \int_{|x|>R} (|x|- R)^p d\mu(x) \Big)^{1/p} \leq \epsilon^{1/p} 
\end{align*}
and by same principles, $\wa{p}(\mu, (P_R)_{\#}\mu) \leq \epsilon^{1/p}$.

\quad Meanwhile, for any $\varphi \in C_b^0 (\reals^d)$, we have
\begin{align*}
\int \varphi d( (P_R)_{\#} \mu_m) & = \int \varphi (P_R(x)) d\mu_m(x) \\
& \rightarrow \int \varphi(P_R(x)) d\mu(x) \quad \text{by weak convergence of } (\mu_m) \\
& =\int \varphi d( (P_R)_{\#} \mu_m)
\end{align*} 
So $(P_R)_{\#} \mu_m \xrightarrow{w^*} (P_R)_{\#} \mu$. Then by \textbf{Theorem 7.6}, and $\overline{B(0, R)}$ is compact, has $\wa{p}((P_R)_{\#}\mu_m, (P_R)_{\#}\mu) \rightarrow 0$.

\quad Putting these toghther,
\begin{align*}
\limsup_{m\rightarrow \infty} \wa{p}(\mu_m, \mu) & \leq \limsup_{m\rightarrow \infty} \big( \wa{p}(\mu_m, (P_R)_{\#}\mu_m) + \wa{p}((P_R)_{\#}\mu_m, (P_R)_{\#}\mu) \big) + \wa{p}((P_R)_{\#}\mu, \mu) \\
& \leq 2\epsilon^{1/p}
\end{align*}
Let $\epsilon \rightarrow 0$ then $\wa{p}(\mu_m, \mu) \rightarrow 0$ as requried.

\eop
\end{p}

\subsection{Geodesics in the Wasserstien Space}

\definum{7.8} Let $p\in [1, +\infty]$ and $(Z, d)$ be a metric space and $\omega: (a,b)(\subset \reals) \rightarrow Z$ a curve in $Z$. We say $\omega \in \text{AC}^p((a,b),Z)$ if $\exists g\in L^p((a,b))$ such that 
\begin{align*}
d(\omega(t_0), \omega(t_1)) \leq \int_{t_0}^{t_1} g(s) ds, \quad \forall a< t_0 < t_1 <b
\end{align*}
If $p=1$, we say $\omega$ is a \textbf{absolutely continuous} curve.

\quad If we only have $g\in L^p_{loc}((a,b))$, then we say $w\in \text{AC}^p_{loc}((a,b),Z)$ and curves $\omega \in \text{AC}_{loc}^1 ((a,b),Z)$ are called \textbf{locally absolutely continuous}.
\s

\definum{7.9}
\begin{itemize}
\item[(1)] Let $(Z,d)$ be a metric space and $\omega :[0,1] \rightarrow Z$ a curve. We define the \textbf{length} of $\omega$ by
\begin{align*}
\text{Len}(\omega) := \sup \big\{ \sum_{k=0}^{n-1} d(\omega(t_k),\omega(t_{k+1}) ) : n\in \mathbb{N}, 0=t_0 <t_1 < \cdots < t_n =1 \big\}
\end{align*}
\item[(2)] A curve $\omega : [0,1] \rightarrow Z$ is said to be a \textbf{geodesic} between $z_0 \in Z$ and $z_1 \in Z$ if 
\begin{align*}
\omega \in \text{argmin} \big\{ \text{Len}(\bar{\omega}) \, \big|\, \bar{\omega} : [0, 1] \rightarrow Z, \bar{\omega} =z_0, \bar{\omega}(1)= z_1 \big\}.
\end{align*}
\item[(3)] A curve $\omega : [0, 1] \rightarrow z$ is said to be a \textbf{constant speed geodesic} between $z_0 \in z$ and $z_1 \in z$ if
\begin{align*}
d(\omega(t), \omega(s)) = |t-s| d (\omega(0), \omega(1))
\end{align*}
\end{itemize}
\s

\textbf{Notes :}
\begin{i}
\item[(1)] If $\omega$ is a constant speed geodesic, then it is a geodesic.
\item[(2)] If $d(\omega(t), \omega(s)) = |t-s| d(z_0, z_1)$ for all $s, t\in (0,1)$ then $\omega \in \text{AC}^1((0, 1),Z)$ with $g(s) = d(z_0, z_1)$.
\end{i}
\s

\definum{7.10} Let $(Z, d)$ be a metric space.
\begin{itemize}
\item[(1)] We say $(Z, d)$ is a \textbf{length space} if
\begin{align*}
\forall x, y\in z, \quad d(x,y) = \inf \{\text{Len}(\omega) : \omega \in \text{AC}^1((0, 1),Z), \omega(0)=x, \omega(1) =y \}
\end{align*}
\item[(2)] We say $(Z,d)$ is a \textbf{geodesic space} if
\begin{align*}
\forall x, y\in z, \quad d(x,y) = \min \{\text{Len}(\omega) : \omega \in \text{AC}^1((0, 1),Z), \omega(0)=x, \omega(1) =y \}
\end{align*}
In particular, the minimum is achieved.
\end{itemize}
\s

\thmnum{7.11} Let $p\in [1, +\infty)$, $X\subset \reals^d$ convex. Define $P_t : X\times X \rightarrow X$ by $P_t(x,y) = (1-t) x+ty$. Let $\mu, \nu \in \PP_p(X)$ and assume that $\pi \in \Pi(\mu, \nu)$ minimize $\mathbb{K}$ with cost $c(x,y) = |x-y|^p$. Then, the curve $\mu_t = (P_t)_{\#} \pi$ is a constant speed geodesic in $(\PP_p(X), \wa{p})$ connecting $\mu$ and $\nu$.

\quad Furthermore, if $\pi = (id \times T)_{\#} \mu$ where $T:X\rightarrow X$ (so in particular $T$ is an optimal transport map), then $\mu_t = ((1-t)id + tT)_{\#} \mu$.
\begin{p}
\pf Note $P_0 =P^X$ and $P_1 = P^Y$, so $\mu_0 = (P_0)_{\#} \pi = (P^X)_{\#} \pi = \mu$ and similarly $\mu_1 = \nu$, so $\mu_+$ connects $\mu_0$ and $\mu_1$. It is enough to show
\begin{align*}
\wa{p} (\mu_s, \mu_t) = |t-s| \wa{p}(\mu, \nu) \quad \forall s, t\in [0,1]
\end{align*}
Suppose $\wa{p} (\mu_s, \mu_t) \leq |t-s| \wa{p} (\mu, \nu)$ for all $s, t\in [0,1]$. If we can find $s,t$ such that $0\leq s <t \leq 1$ such that $\wa{p} (\mu_s, \mu_t) < (t-s) \wa{p}(\mu, \nu)$ then
\begin{align*}
\wa{p}(\mu, \nu) &\leq \wa{p} (\mu, \nu_s) + \wa{p}(\mu_s, \mu_t) + \wa{p}(\mu_t, \nu) \\
&< \wa{p}(\mu, \nu), 
\end{align*}
a contradiction. So there are no such $s,t$.

\quad So we are just left to show that $\wa{p} (\mu_s, \mu_t) \leq |t-s| \wa{p} (\mu, \nu)$ for all $s, t\in [0,1]$. To see this, let $\pi_{s,t} = (P_s \times P_t)_{\#} \pi$. Then for $A\subset X$,
\begin{align*}
\pi_{s,t}(A\times X) & = \pi \Big( \Big\{ (x,y) : (P_s \times P_t)(x,y) \in A\times X \Big\} \Big) \\
& = \pi \Big( \Big\{ (x,y) : P_s(x,y) \in A \Big\} \Big) \\
& = (P_s)_{\#}\pi(A)
\end{align*} 
Similarly, has $\pi_{s,t}(X\times B) = \mu_t(B)$. Hence $\pi_{s,t}  \in \Pi(\mu_s, \mu_t)$. Now
\begin{align*}
\wa{p} (\mu_s, \mu_t) & \leq \Big( \int_{X\times X} |x-y|^p d\pi_{s,t}(x,y) \Big)^{1/p} \\
&= \Big( \int_{X\times X} |P_s(x,y) - P_t(x,y)|^p d\pi(x,y) \Big)^{1/p} \\
&= |t-s| \Big( \int_{X\times X} |x-y|^p d\pi(x,y) \Big)^{1/p} \\
&= |t-s| \wa{p} (\mu, \nu)
\end{align*}

\quad The furthermore part of theorem follows from
\begin{align*}
\mu_t &= (P_t)_{\#} \pi = (P_t )_{\#} (id \times T)_{\#} \mu \\
&= (P_t \circ(id \times T))_{\mu} \mu
\end{align*}
but $P_t \circ(id \times T)(x) = (1-t)x + tT(x)$ so
\begin{align*}
\mu_t = ((1-t)id + tT)_{\#} \mu
\end{align*}
\eop
\end{p}
\s

\newday

(8th March, Friday)
\s

\section{Gradient Flows in Wasserstein Spaces}

We first study the gradient Flow of function defined on an Euclidean space. The theory developed in this lecture can be generalized to the setting in a measure space with a metric.
\s

In Euclidean space, the gradient flow is given by
\begin{align*}
\frac{d}{dt} u(t) = - \nabla \phi(u(t))
\end{align*}
for $u: t\mapsto \reals^d$.
\s

\subsection{Gradient Flows for Convex Functions in $\reals^d$}

Recall, we had
\begin{itemize}
\item $\phi : \reals^d \rightarrow \reals$ is \textbf{convex} if for all $u_0, u_1 \in \reals^d$, $\theta \in [0,1]$
\begin{align*}
\phi(\theta u_0 + (1-\theta) u_1) \leq \theta \phi(u_0) + (1-\theta)\phi(u_1)
\end{align*}
\item If $\phi$ is differentiable, then it is convex \emph{iff} it has monotonicity of $\nabla \phi$, i.e.
\begin{align*}
\langle \nabla \phi(u_0) - \nabla \phi(u_1), u_0 -u_1 \rangle \geq 0
\end{align*}
\item If $\nabla \phi$ is \textbf{$L$-Lipschitz} then
\begin{align*}
\phi(u_0) \leq \phi(u_1) + \langle \nabla \phi(u_1), u_0 - u_1 \rangle + \frac{L}{2} \norms{u_0 -u_1}{}^2
\end{align*}
\end{itemize}

\defi $\phi$ is \textbf{$\lambda$-convex} if it satisfies, for $u_{\theta} = (1-\theta)u_0 + \theta u_1$, $\theta \in [0,1]$,
\begin{align*}
\phi(u_{\theta}) \leq (1- \theta)\phi(u_0) + \theta \phi(u_1) - \frac{\lambda}{2} \theta(1-\theta)\norms{u_0 -u_1}{}^2
\end{align*}
If $\phi \in C^2(\reals^d)$, this is equivalent to having
\begin{itemize}
\item[(1)] $\nabla^2 \phi (u) \geq \lambda \text{Id}$. \textbf{(Hessian inequality / curvature condition)}
\item[(2)] $\langle \nabla \phi(u_0) - \nabla  \phi(u_1), u_0 - u_1 \rangle \geq \lambda \snorms{u_0 - u_1}{}^2$. \textbf{($\lambda$-monotonicity of $\nabla \phi$)}
\item[(3)] $\phi(u_1) \geq \phi(u_0) + \langle \nabla \phi(u_0), u_1 - u_0 \rangle + \frac{\lambda}{2} \snorms{u_1 -u_0}{}^2$. \textbf{(subgradient inequality)}
\end{itemize}

\emph{Remark :}
\begin{itemize}
\item $\phi$ is $\lambda$-convex \emph{iff} $f(u) := \phi(u) - \frac{\lambda}{2} \snorms{u}{}^2$ is convex. Hence whenever $\phi$ is $\lambda$-convex, there are constants $a,b$ such that
\begin{align*}
\phi(x) \geq a+ b\cdot x + \frac{\lambda}{2} \snorms{x}{}^2
\end{align*}
\item It is direct from the definition of being $\lambda$-convexity that whenever $\phi$ is $\lambda$-convex and has a minimizer, then the minimizer is unique.
\end{itemize} 
\s

Consider in the \emph{discrete case}, the problem
\begin{align*}
\min_{u\in \reals^d} \phi(u)
\end{align*}
(I could not copy down) then $u$ should solve $u_{k+1} = u_k - \tau \nabla \phi(u_k)$.
\s

Now let us transfer this to the continuous case. So
\begin{align*}
u_{k+1} = u_k - \tau \nabla \phi(u_k) \quad \Leftrightarrow \quad \frac{u_{k+1} - u_k}{\tau} = - \nabla \phi(u_k)
\end{align*}
so if we put $u_{k+1} = u_{(k+1)\tau}$, then the analogous equation in the continuous setting will be
\begin{align*}
\frac{d}{dt} u(t) = - \nabla \phi(u(t))
\end{align*}
\s

\defi A \textbf{Gradient Flow} of $\phi : \reals^d \rightarrow \reals$ starting from an initial pint $u_0 \in \reals^d$ is a curve $u: (0, +\infty) \rightarrow \reals^d$, $t \mapsto u(t) \in \reals^d$ that solves (uniquely) the Cauchy problem
\begin{align*}
\begin{cases}
\frac{d}{dt} u(t) = - \nabla \phi(u(t)) \\
\lim_{t\rightarrow 0^+} u(t) = u_0 
\end{cases} \call{\text{P}_{\text{GI}}}
\end{align*}
\s

\newcommand{\pgi}{\text{P}_{\text{GI}}}

\propnum{1} Suupose $\phi$ is a convex function. Let $u_1, u_2$ be two solutions of $(\pgi)$, then we have
\begin{align*}
\frac{d}{dt} \phi(u(t)) = -\norms{\nabla \phi(u(t)))}{}^2
\end{align*}
and
\begin{align*}
\snorms{u_1(t) - u_2(t)}{} \leq \snorms{u_1(0)- u_2(0)}{}
\end{align*}

\textit{i.e.} gradient flow is a contraction.

\quad In particular, $(\pgi)$ has a unique solution with initial condition given.
\begin{p}
\pf The first equality is a direct consequence of $(\pgi)$.

\quad To see the second inequality, let $g(t) = \frac{1}{2} \snorms{u_1(t) - u_2(t)}{}^2$. Then since $u_j(t) = -\nabla \phi(u_j(t))$ ($j=1,2$),
\begin{align*}
g'(t) &= \langle u_1(t) - u_2(t), u_1'(t) - u_2'(t) \rangle \\
&= - \langle u_1(t)- u_2(t) , \nabla \phi(u_1(t)) - \nabla \phi(u_2(t)) \rangle \leq 0
\end{align*}
by convexity of $\phi$. So $g(t) : [0, + \infty) \rightarrow \reals_+$ has $g(t) \leq g(0)$.

\eop
\end{p}
\s

We can draw a stonger result if $\phi$ is $\lambda$-convex. Suppose $\phi$ is $\lambda$-convex. Then 
\begin{align*}
-\langle u_1(t) - u_2(t) , \nabla \phi(u_1(t)) - \nabla \phi(u_2(t)) \rangle \leq - \lambda \snorms{u_1(t) - u_2(t)}{}^2
\end{align*}
so
\begin{align*}
g'(t) \leq - \lambda \snorms{u_1(t) - u_2(t)}{}^2 = -2\lambda g(t)
\end{align*}
The \emph{Gronwall's lemma} now implies
\begin{align*}
g(t) \leq e^{-2\lambda t}g(0)
\end{align*}
and in other words,
\begin{align*}
\snorms{u_1(t) - u_2(t)}{}^2 \leq e^{-2\lambda t} \snorms{u_1(0)- u_2(0)}{}^2,
\end{align*}
the \emph{exponential convergence} (also called linear convergence).
\s

Moreover, we have that if $\phi$ is $\lambda$-convex, then $\text{argmin}_u \phi$ is a singleton. So if $u_1(t)$ is a curve and $u_2(t) \equiv \bar{u} =\text{argmin}(\phi)$, then
\begin{align*}
\norms{u_1(t) - \bar{u}}{}^2 \leq e^{-2\lambda t}\norms{u_1(0)- \bar{u}}{}^2\rightarrow 0
\end{align*}
\s

\defi An \textbf{explicit Euler scheme} solves for
\begin{align*}
\frac{u^{\tau}_{k+1} - u^{\tau}_k}{\tau} = - \nabla \phi(u_k^{\tau})
\end{align*}
This is very easy to implement. However, if we choose $\tau(1- \frac{\tau L}{2})>0$, we get a stability issue.

\quad An \textbf{implicit Euler scheme} solves for
\begin{align*}
\frac{u_{k+1}^{\tau} - u_k^{\tau}}{\tau} = -\nabla \phi (u_{k+1}^{\tau})
\end{align*}
This corresponds to
\begin{align*}
u_{k+1}^{\tau} = \text{argmin} \big\{ \phi(u) + \frac{1}{2\tau} \snorms{u- u_k^{\tau}}{}^2 \big\} \quad \Leftrightarrow \quad 0 =\nabla \phi(u^{\tau}_{k+1}) + \frac{1}{\tau}(u^{\tau}_{k+1} - u^{\tau}_k)
\end{align*}
With such choice of $u^{\tau}_{k}$ strating from $u^{\tau}_0 =u_0$,
\begin{align*}
\phi(u_{k+1}^{\tau}) + \frac{1}{2\tau} \norms{u^{\tau}_{k+1} - u^{\tau}_{k}}{}^2 \leq \phi(u_k^{\tau})
\end{align*}
so if we let $\Delta_k = \frac{1}{2\tau} \norms{u^{\tau}_{k+1}- u_k^{\tau}}{}^2$, then $\Delta_k \leq \phi(u_k^{\tau}) - \phi(u^{\tau}_{k+1})$, and so
\begin{align*}
\sum_{k+1}^K \Delta_k \leq \phi(u_0^{\tau}) - \phi(u^{\tau}_{K+1}) =: C < \infty
\end{align*}
\s

If we have defined
\begin{align*}
u^{\tau}(t) &= u^{\tau}_k \quad \text{for } t= k\tau \\
\tilde{u}^{\tau}(t) &= u_k^{\tau} + (t- k \tau) \nu^{\tau}_{k+1} \quad \text{for } t \in [ k\tau, (k+1) \tau ) \text{ and } v_{k+1}^{\tau} &= \frac{u_{k+1}^{\tau}- u_k^{\tau}}{\tau}
\end{align*}
then $(\tilde{u}^{\tau})'(t) =v^{\tau}(t)$ and
\begin{align*}
\frac{ \snorms{ u^{\tau}_{k+1} - u^{\tau}_k }{}^2 }{2\tau} = \frac{\tau}{2} \Big( \frac{ \snorms{u^{\tau}_{k+1} - u^{\tau}_k}{}^2}{\tau^2} \Big)^2 = \frac{\tau}{2} \norms{v^{\tau}_{k+1}}{}^2 = \frac{\tau}{2} \int_{k\tau}^{(k+1)\tau} \frac{1}{2} \norms{ (\tilde{u}^{\tau})'(t) }{}^2 dt
\end{align*}
and because of summability ($\sum_{k+1}^K \Delta_k = C < \infty$), 
\begin{align*}
\frac{\tau}{2} \int_0^T \frac{1}{2}\norms{(\tilde{u}^{\tau})'(t)}{}^2 <C
\end{align*}
Using this, one can show that $u^{\tau}(t)$ converges uniformly to $u$ on every compact set $[0,T]$.
\s

\newday

(11th March, Monday)
\s

In Euclidean spaces, we defined the solution $u$ of
\begin{align*}
\frac{du}{dt}(t) = -\nabla \phi(u(t)) \call{*}
\end{align*}
to be the gradient flow of $\phi$. Note,
\begin{itemize}
\item[(a)]
\begin{align*}
\frac{d}{dt} \phi(u(t)) = \langle \nabla \phi(u(t)), \frac{du}{dt}(t) \rangle = -\norms{\nabla \phi(u(t))}{}^2
\end{align*}

If $\phi$ is $\lambda$-convex, then
\item[(b)] $(*)$ is equivalent in $\reals^d$ to \textbf{Propsition 8.8}
\begin{align*}
\frac{d}{dt} \phi(u(t)) = -\frac{1}{2} \Big|\frac{du}{dt}(t)\Big|^2 - \frac{1}{2} |\nabla \phi(u(t))|^2 \quad \forall t\in (0, \infty) \call{\text{EDE}}
\end{align*}
\item[(c)] $(*)$ is equivalent in $\reals^d$ to
\begin{align*}
\frac{d}{dt} \Big( \frac{1}{2} |u(t) - v|^2 \Big) + \frac{\lambda}{2} |u(t) - v|^2 \leq \phi(v) - \phi(u(t)) \quad \forall t\in (0, \infty), \,\, \forall v\in \reals^d \call{\text{EVI}}
\end{align*}
\end{itemize}
\s

\subsection{Gradient Flows in Metric Spaces}

\textbf{Recall :} A curve $w: (a,b) \rightarrow Z$ is \textbf{absolutely continuous} in a metric space $(Z,d)$ if $\exists g \in L^1((a,b))$ such that
\begin{align*}
d(w(t_0), w(t_1)) \leq \int_{t_0}^{t_1} g(s) ds \quad \forall t_0, t_1 \in (a,b) \text{ with }t_0<t_1 \call{**}
\end{align*}
And if $g\in L^p((a,b))$ we write $w\in \text{AC}^p((a,b), Z)$.
\s

\defi We define the \textbf{metric derivative} of $w$ by
\begin{align*}
|w'|(t) = \lim_{s\rightarrow t} \frac{d(w(s), w(t))}{|t-s|} \call{\oplus}
\end{align*}
if the limit exists.
\s

\thmnum{8.10} Let $(Z,d)$ be a complete and separable metric space. If $w: (a,b) \rightarrow Z$ is absolutely continuous then the limit $(\oplus)$ exists for Lebesgue-a.e. $t\in (a,b)$. Moreover, the function
\begin{align*}
(a,b) \rightarrow \reals, \quad t\mapsto |w'|(t)
\end{align*}
is an $L^1(a,b)$-function and one can choose $g=|w'|$ in ($**$).

\quad If $\tilde{g}$ is any other function satisfying ($**$), then $|w'|(t) \leq \tilde{g}(t)$ for Lebesgue-a.e. $t$.
\s

\definum{8.11} Let $(Z,d)$ be a metric space then the \textbf{metric slope} of $\phi : Z\rightarrow \reals$ at $v\in \reals$ is defined by
\begin{align*}
|\pa \phi|(v) = \limsup_{w\rightarrow v} \frac{(\phi(v)- \phi(w))_+}{d(v,w)}
\end{align*}
\s

\subsubsection{GVI Gradient Flows}

\definum{8.12} Given a metric space $(Z,d)$ and a function $\phi : Z\rightarrow \reals$ an \textbf{evolution variational inequality $(\text{EVI}_{\lambda})$ gradient flow} is a locally absolutely continuous curve
\begin{align*}
(a,b) \rightarrow \reals, \quad t\mapsto u(t) \in \text{Dom}(\phi) \subset Z
\end{align*}
satisfying
\begin{align*}
\frac{1}{2}\frac{d}{dt}\Big( d^2(u(t), v) \Big) + \frac{\lambda}{2} d^2(u(t), v) \leq \phi(v)- \phi(u(t)) \quad \text{for a.e. }t \in (0, \infty)
\end{align*}
\s

\propnum{8.13} Let $(Z,d)$ be a complete and separable metric space and $\phi : Z\rightarrow \reals \cup \{+ \infty\}$ a proper function. If $u$ and $v$ are $\text{EVI}_{\lambda}$ gradient flows with initial condition $u(0) = u_0$ and $v(0) = v_0$. Then
\begin{align*}
d(u(t), v(t)) \leq e^{-\lambda t} d(u_0, v_0)
\end{align*}
\s

\emph{Remark :} If $\lambda >0$ then $\exists$ at most one $\text{EVI}_{\lambda}$ gradient flow and $u(t)$ converges exponentially to $u^*$, the minimiser of $\phi$, \textit{i.e.}
\begin{align*}
d(u(t), u^*) \leq e^{-\lambda t} d(u_0, u^*)
\end{align*}
We have not mentioned that $\phi$ is $\lambda$-convex, but if the $\text{EVI}_{\lambda}$ gradient flow exists for any initial condition, then $\phi$ is $\lambda$-convex, and this implies that the minimiser is unique.
\s

\newday

(13th March, Wednesday)
\s

\propnum{8.13} Let $(Z,d)$ be a complete and separable metric space and $\phi : Z\rightarrow \reals \cup \{+ \infty \}$ a proper function. If $u$ and $v$ are $\text{EVI}_{\lambda}$ gradient flows with initial condition $u(0) = u_0$ and $v(0) = v_0$. Then
\begin{align*}
d(u(t), v(t)) \leq e^{-\lambda t} d(u_0, v_0)
\end{align*}
\begin{p}
\pf We have, for any $w\in \reals$,
\begin{align*}
& \frac{1}{2} \frac{dt}{d} d^2(u(t), w) + \frac{\lambda}{2} d^2(u(t),w) \leq \phi(w) - \phi(u(t)) \call{1}\\
& \frac{1}{2} \frac{dt}{d} d^2(v(t), w) + \frac{\lambda}{2} d^2(v(t),w) \leq \phi(w) - \phi(v(t)) \call{2}
\end{align*}
Choose $w = v(s)$ in (1)  and $w= u(s)$ in (2), then (1), (2) and $\lambda$-convexity of $u$ together gives
\begin{align*}
\frac{1}{2} \frac{d}{dt} d^2(u(t), v(s))\Big|_{s=t} + \frac{1}{2}\frac{d}{dt} d^2 (v(t), u(s))\Big|_{s=t} \leq - \lambda d^2(u(t), v(t))
\end{align*}
Since
\begin{align*}
\frac{d}{dt} d^2(u(t), v(t)) = \frac{d}{dt} d^2(u(t), v(s)) \Big|_{s=t} + \frac{d}{dt} d^2(u(s), v(t)) \Big|_{s=t}
\end{align*}
we have
\begin{align*}
\frac{1}{2} \frac{d}{dt} d^2(u(t), v(t))\leq -\lambda d^2(u(t), v(t))
\end{align*}
Multiply by $e^{2\lambda t}$, then 
\begin{align*}
\frac{d}{dt} \big( e^{2\lambda t} d^2(u(t), v(t)) \big) = 2\lambda e^{2\lambda t} d^2(u(t), v(t)) + e^{2\lambda t} \frac{d}{dt} d^2(u(t), v(t)) \leq 0
\end{align*}
Hence we have the result.

\eop
\end{p}

(skipping some materials in the section.)

\subsection{Gradient Flows in the Wasserstein Space}

We will need the continuity equation (conservation of mass). Assume we have density $\rho (x,t)$ at times $t$. So each set $A\subset \reals^d$ has mass $\int_A \rho(x,t) dx$. We assume that mass is only lost on the boundary of $A$. So
\begin{align*}
\int_A \frac{\pa \rho}{\pa t} (x,t) dt = \frac{d}{dt} \int \rho(x,t) dx = \int_{\pa A} v(x,t)\cdot n(x,t) dS 
\end{align*}
where the mass is moving with velocity $v$ and $n$ is the unit normal to $\pa A$. By the divergence theorem,
\begin{align*}
\int_A \frac{\pa \rho}{\pa t}(x, t) dx = -\int_A \nabla \cdot (v(x,t) , \rho(x,t)) dx
\end{align*}
This is true for any suitable $A$, so we have
\begin{align*}
\frac{\pa \rho}{\pa t}(x,t) = - \nabla \cdot (v(x,t) \rho(x,t)), 
\end{align*}
called the \textbf{Continuity equation}.
\s

We now make change of notation, $u(t) \rightarrow \mu_t$.
\s

\subsubsection{Wasserstein Tangent Space}

(see [4] Ambrosio, Gigli, Savar\'{e}, 2008, ``Gradient Flows in Metric Spaces" for more information)
\s

\thmnum{8.23} \emph{(Abolutely continous curves / the continuity equation)}
\begin{itemize}
\item[(1)] Let $(0, + \infty) \rightarrow \PP_2(\reals^d)$, $t\mapsto \mu_t$ be absolutely continuous and $|\mu'| \in L^1((0, + \infty))$ be its metric derivative. Then $\exists$ a vector field $v_t \in L^2(\mu_t, \reals^d)$ such that 
\begin{align*}
\snorms{v_t}{L^2(\mu_t;\reals^d)} \leq |\mu'|(t) \quad \text{for a.e. }t\in (0, + \infty)
\end{align*}
and
\begin{align*}
\frac{\pa}{\pa t} \mu_t + \nabla \cdot (v_t \mu_t) =0 \quad \text{in }\reals^d \times(0, + \infty)  \call{*}
\end{align*}
holds in the sense of distributions.
\item[(2)] Let $(0, +\infty) \rightarrow \PP_2(\reals^d)$, $t\mapsto \mu_t$ be continuous with respect to weak* topology on $\PP(\reals^d)$ and satisfies $(*)$ for some vector field $v_t$ with
\begin{align*}
\int_0^{\infty} \snorms{v_t}{L^2(\mu, \reals^d)} dt < +\infty
\end{align*}
Then $\mu_t$ is absolutely continuous and $|\mu'(t)| \leq \snorms{v_t}{L^2(\mu_t ; \reals^d)} \quad \text{for a.e. }t\in (0, \infty)$
\end{itemize}
\emph{[We define $(*)$ to hold in the sense of distributions iff
\begin{align*}
\int_0^{\infty} \int_{\reals^d} \Big( \frac{\pa f}{\pa t}(x, t) + v_t(x) \cdot \nabla f(x,t) \Big) d\mu_(x) dt =0 \quad \forall f\in C_c^{\infty}(\reals^d \times (0, +\infty))
\end{align*}]}
\s

The following proposition motivates the tangent space. The tangent direction can be thought of as the direction in which a variation in the direction stays in the probability space.
\s

\propnum{8.26} Let $(0, +\infty) \rightarrow \PP_2(\reals^d)$, $t\mapsto \mu_t$ be absolutely continous. Suppose $v_t \in \overline{ \{\nabla \varphi : \varphi \in C_c^{\infty}(\reals)\}}^{L^2(\mu_t ; \reals^d)}$ satisfies $(*)$. Assume there is an optimal transport map $T^{(t,s)}$ between $\mu_t$ and $\nu_s$ for each $s,t$, \textit{i.e.} $T_{\#}^{(t,s)} \mu_t = \mu_s$. Then
\begin{align*}
\lim_{h\rightarrow 0} \frac{T^{(t, t_h)}- id}{h} = v_t
\end{align*}
where this limit is taken in $L^2(\mu_t ; \reals^d)$ (for all $t$? check) 
\s

\definum{8.27} For $\mu\in \PP_2(\reals^2)$, the \textbf{tangent space to $\PP_2(\reals^d)$ at the point $\mu$} is
\begin{align*}
\text{Tan}_{\mu} \PP_2(\reals^d) = \overline{ \{ \nabla \varphi ; \varphi \in C_c^{\infty}(\reals^d)\}}^{L^2(\mu ; \reals^d)}
\end{align*}
\s

We can use \textbf{Proposition 8.26} to differentiate $t\mapsto \frac{1}{2} \wa{2}^2(\mu_t, \sigma)$.
\s

\thmnum{8.28} Let $\sigma \in \PP_2(\reals^d)$, $(0, + \infty) \rightarrow \PP_2(\reals^d)$, $t\mapsto \mu_t$ be absolutely continuous and $v_t \in \text{Tan}_{\mu_t} \PP_2(\reals^d)$ satisfy $(*)$. Assume there is an optimal transport map $T^{t}$ between $\sigma$ and $\mu_t$ for all $t$, \textit{i.e.} $T^{(t)}_{\#} \mu_t =\sigma$. Then
\begin{align*}
\frac{1}{2} \frac{d}{dt} \wa{2}^2 (\mu_t, \sigma) = \int_{\reals^d}(x- T^{(t)}(x))\cdot v_t(x) d\mu_t(x)
\end{align*}
\begin{p}
\textbf{sketch proof)} Maps $T^{(t)}$, $T^{(t, t+h)}$, $T^{(t+h)}$ are all optimal maps. We suppose $T^{(t)} = T^{(t+h)} \circ T^{(t, t+h)}$. \emph{[Of course, this is not true in general, but there is a reasonable justification for this : when $h$ is small, then composition of optimal maps would be close to an optimal map, so we expect the equality to hold up to a $o(h)$ correction.]} Then
\begin{align*}
\frac{d}{dt} \wa{2}^2 (\mu_t, \sigma) &= \lim_{h\rightarrow 0^+} \frac{1}{h} \Big[ \int_{\reals^d} |T^{(t+h)}(x) -x|^2  d\mu_{t+h}(x) - \int_{\reals^d} |T^{(t)}(x) - x|^2 d\mu_t(x) \Big] \\
&= \lim_{h\rightarrow 0^+} \frac{1}{h}  \ \int_{\reals^d} \Big[ | T^{(t+h)}(T^{(t, t+h)}(x)) - T^{(t, t+h)}(x) |^2 - |T^{(t)} (x) - x|^2  \Big] d\mu_t(x) \\
&= \lim_{h\rightarrow 0^+} \int_{\reals^d} (x- T^{(t, t+h)}(x)) \cdot (2T^{(t)}(x) - x - T^{(t, t+h)}(x)) d\mu_t(x) \\
&= \int_{\reals^d} (-v_t(x)) \cdot (2T^{(t)}(x) - x - x) d\mu_t(x)
\end{align*}
\eop
\end{p}








\end{document}
