\documentclass[12pt,a4paper]{article}

\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{calrsfs}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[mathscr]{euscript}

%%%%%%%%%%%attach pdf%%%%%%%%%%%%
\usepackage[final]{pdfpages}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%For writing large opertors%%%%%%%%%%%
%\usepackage{stmaryrd}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%for writing large parallel%%%%%%
\usepackage{mathtools}
\DeclarePairedDelimiter\bignorm{\lVert}{\rVert}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%for drawing commutative diagrams.%%%%%%
\usepackage{tikz-cd}  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%for changing margin
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 

\newenvironment{proof}
{\begin{changemargin}{1cm}{0.5cm} 
	}%your text here
	{\end{changemargin}
}

\newenvironment{subproof}
{\begin{changemargin}{0.5cm}{0.5cm} 
	}%your text here
	{\end{changemargin}
}

\newenvironment{p}
{\begin{proof} 
	}%your text here
	{\end{proof}
}

\renewenvironment{i}
{\begin{itemize} 
	}%your text here
	{\end{itemize}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%double rules%%%%%%%%%%%%%%%%%%%
\usepackage{lipsum}% Just for this example

\newcommand{\doublerule}[1][.4pt]{%
  \noindent
  \makebox[0pt][l]{\rule[.7ex]{\linewidth}{#1}}%
  \rule[.3ex]{\linewidth}{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Mixing time of Markov chains}
\author{Jiwoon Park}
\date{Lent 2019}

\maketitle

\newcommand{\thm}{\textbf{Theorem) }}
\newcommand{\thmnum}[1]{\textbf{Theorem #1) }}
\newcommand{\defi}{\textbf{Definition) }}
\newcommand{\definum}[1]{\textbf{Definition #1) }}
\newcommand{\lem}{\textbf{Lemma) }}
\newcommand{\lemnum}[1]{\textbf{Lemma #1) }}
\newcommand{\prop}{\textbf{Proposition) }}
\newcommand{\propnum}[1]{\textbf{Proposition #1) }}
\newcommand{\corr}{\textbf{Corollary) }}
\newcommand{\corrnum}[1]{\textbf{Corollary #1) }}
\newcommand{\pf}{\textbf{proof) }}

\newcommand{\lap}{\triangle} %%Laplacian
\newcommand{\s}{\vspace{10pt}}
\newcommand{\bull}{$\bullet$}
\newcommand{\sta}{$\star$}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\eop}{\hfill  \textsl{(End of proof)} $\square$} %end of proof
\newcommand{\eos}{\hfill  \textsl{(End of statement)} $\square$} %end of proof


\newcommand{\intN}{\mathbb{Z}_N}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\norms}[2]{\bignorm[\big]{#1}_{#2}}
\newcommand{\abs}[1]{\big| #1 \big|}
\newcommand{\avg}{\mathbb{E}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\borel}{\mathscr{B}}
\newcommand{\EE}{\mathscr{E}}
\newcommand{\pa}{\partial}
\newcommand{\loc}{L^1_{\text{loc}}}
\newcommand{\tmix}{\text{tmix}}

\renewcommand{\vec}{\underline}
\renewcommand{\bar}{\overline}


\def\doubleunderline#1{\underline{\underline{#1}}}

\newcommand{\newday}{\doublerule[0.5pt]}
\newcommand{\digression}{**********************************************************************************************}

\setlength\parindent{0pt}

Will follow the lecture notes by Nathana\"{e}l Berestycki, Ch 1 - 4. (use pdf from from the lecturer's website)

Primary reference would be from Liven-Perres, ``Markov chains and mixing times"

Also useful : Montenagro-Tetali, ``mathematical aspects of mixing times". This text is very analytical.
\s

\newday

(17th January, Thursday)

\section{Preliminary}

Mixing times is at the cross road of probability theory, analyis, geometry, statistical mechanics with ties to other fields such as representations theory and theoretical computer science. We will not focus on the application side of the theory, but on the theoeretical side.
\s

\def \emph{(reversibility)} $P= (p(x,y))_{x,y}$ is \textbf{reversible} with respect to the sationary distribution $\pi$ if $\forall x, y$, $\pi(x) p(x,y) = \pi(y) p(x,y)$, i.e. satisfies the detailed balance equation. This condition is equivalent to 
\begin{itemize}
\item[(i)] $P = P^*$ with respect to $\langle f,g\rangle_{\pi} = \pi(fg)$, where $\pi(h) = \sum_{x} \pi(x) h(x)$, i.e. $\langle Pf, g\rangle_{\pi} = \langle f, Pg\rangle_{\pi}$ for all $f,g$.
\item[(ii)] $P$ is a weighted random walk of $G= (V,E)$ for some graph : assign to each $xy\in E$ a symmetric edge weight $c(x,y) = c(y,x)$. Then $p(x,y) = c(x,y)/ \sum_{z} c(x,z)$. Then for $\pi(x) = c(x) / \sum_y c(y)$, has $\pi(x) p(x,y) = \pi(y)p(y,x)$.

\quad If reversibility is already satisfied, let $c(x,y)= \pi(x)p(x,y) = c(y,x)$. Then $c(x,y)/ \sum_z c(x,z) = \pi(x) p(x,y) / \pi(x)=p(x,y)$, so we have the equivalence.
\end{itemize}
\s

\definum{1.1.} The \textbf{total-variation distance of $\mu, \nu$}, distributions on the state space $S$, is defined by
\begin{align*}
\norms{\mu - \nu}{TV} = \max_{A\subset S} \mu(A) -\nu(A)
\end{align*}
\s

\lemnum{1.1} $\norms{\mu - \nu}{TV} = \frac{1}{2} \sum_x |\mu(x) - \nu(x)| = \sum_{x: \mu(x) > \nu(x)} |\mu(x) - \nu(x)|$.
\begin{proof}
\pf Let $A\mu = \{x: \mu(x) > \nu(x) \}$ and $A\nu$ be similar. Then
\begin{align*}
\mu(A) - \nu(A) \leq \mu(A\cap A\mu) - \nu(A\cap A\mu) \leq \mu(A\mu)-\nu(A\mu)
\end{align*}
with equality if $A = A\mu$ and because $\mu(\{\mu(x)= \nu(x)\}) = \nu(\{\mu(x)= \nu(x)\})$,
\begin{align*}
\mu(A\mu) - \nu(A\mu) = \nu(A\nu)- \mu(A\nu)
\end{align*}
so
\begin{align*}
\norms{\mu- \nu}{TV} &= \frac{1}{2}(\mu(A\mu) - \nu(A\mu) + \nu(A\nu) - \mu(A\nu)) \\
& = \frac{1}{2} \sum_x |\mu(x)- \nu(x)|
\end{align*}
\eop
\end{proof}
\s

Under irreducibility and aperiodicity condition, we have
\begin{align*}
p^t(x,y) \rightarrow \pi (y) \quad t\rightarrow \infty, \,\, \forall (x,y)
\end{align*}
(Given $|S|$ is finite, such $\pi$ exists and is unique)
\s

\begin{itemize}
\item \textbf{$\epsilon$-TV mixing time} is defined by
\begin{align*}
\tmix (\epsilon) = \inf \{t: d(t) \leq \epsilon \}
\end{align*}
where $d(t) = \max_x \norms{p^t(x, \cdot)- \pi}{TV}$. Also let $\tmix = \tmix (1/4)$. Soon, we will show that
\begin{align*}
\tmix (\epsilon^k) \leq \tmix (\epsilon/2)\cdot k
\end{align*}
\item \textbf{Claim :} $d(t)$ is non-increasig in $t$.
\begin{proof}
\pf \begin{align*}
\frac{1}{2} \sum_y |p^{t+s}(x,y) - \pi(y)| &= \frac{1}{2} \sum_y |\sum_z p^t(x,z) p^s (z,y) - \pi (z) p^s (z,y)| \\
&\leq \frac{1}{2} \sum_z |p^t(x,z) - \pi(z)|
\end{align*}
\end{proof}
\end{itemize}
\s

\defi Let $(X^n)_n$ be a family of Markov Chains. Write $d_n(t)$ for $d(t)$ with respect to $X^n$. We say that \textbf{cutoff} occurs at time $t_n$ if
\begin{align*}
\forall \epsilon>0, d_n((1-\epsilon)t_n) \rightarrow 1, d_n((1+\epsilon)t_n) \rightarrow 0 \quad \text{as } x\rightarrow \infty
\end{align*}
Equivalently, $\tmix^n (\epsilon) \sim t_n$, where $a_n \sim b_n$ \emph{iff}  $a_n/b_n \rightarrow 1$.
\s

\definum{1.4} A \textbf{coupling} of probability measures $\mu$ and $\nu$ is a realization $(X,Y)$ on the same probability space such that $X\sim \mu$ and $Y\sim \nu$.
\s

\textbf{Example : } Consider $\mu = \nu$. Then may choose (i) $X=Y$ or (ii) $X,Y$ independent. So we have complete freedom to of choice whether $X$ and $Y$ are dependent or not.
\s

\thmnum{1.1} $\norms{\mu - \nu}{TV} = \inf \{\prob (X\neq Y) : (X,Y) \text{ a coupling of }\mu, \nu \}$. The coupling with this infimum is called the \textbf{optimal coupling.}
\begin{proof}
\pf For any coupling $(X,Y)$ of $\mu$ and $\nu$, we have
\begin{align*}
\mu(A) - \nu(A) = \prob(X\in A) - \prob(Y\in A)\leq \prob(X\neq Y)
\end{align*}
\s

For the converse direction, take $Z, X', Y', W$ independently with
\begin{align*}
& Z \sim \text{Ber}(\norms{\mu-\nu}{TV}) \\
& W \text{ takes value } x \text{ with probability } \mu(x) \wedge \nu(x) /p \\
& X' \text{ takes value } x \text{ with probability } (\mu(x)- \nu(x)) + \norms{\mu-\nu}{TV} \\
& Y' \text{ takes value } y \text{ with probability } (\nu(y)- \mu(y)) + \norms{\mu-\nu}{TV}
\end{align*}
where
\begin{align*}
p &= \sum_z \mu(z)\wedge \nu(z) = \sum_y \nu(y) - \sum_y (\nu(y) - \mu(y))_+ \\
&= 1 - \norms{\mu - \nu}{TV} \\
\end{align*}
Now set $X = ZX' + (1-Z)W$, $Y = ZY' + (1-Z)W$. Then $X'$ and $Y'$ have disjoint supports, so $\prob(X' = Y')=0$ and
\begin{align*}
\prob(X\neq Y) &= \prob(Z=1) = \norms{\mu - \nu}{TV}\\
\prob(X=x) &= \prob(Z=1)\prob(X'=x) + \prob(Z=0)\prob(W=x) \\
&= \prob(Z=1) \frac{(\mu(x)- \nu(x))_+}{\prob(Z=1)} + \prob(Z=0)\frac{\mu(x)\wedge \nu(x)}{\prob(Z=0)} = \mu(x) \\
\prob(Y=y) &= \nu(y)
\end{align*}
so $X$, $Y$ satisfy the desired eqaulity.

\eop
\end{proof}
\s

\propnum{1.2} Let $p(t) = \max_{x,y\in S} \norms{p^t(x,\cdot) - p^t (y, \cdot)}{TV}$. Then 
\begin{align*}
& d (t) \leq p(t) \leq 2d(t) \\
& p(t+s) \leq p(s)p(t) \quad\quad \forall t,s\geq 0
\end{align*}
\begin{proof}
Idea : let $A = \{f: \pi(f)=0 \}$, $p^*: A\rightarrow A$ the adjoint of $p$. Check $p(t) = \max_{f\in A}\norms{(p^*)^t f}{1}$, $\norms{f}{1}=2$, $\norms{h}{1} = \pi(|h|)$.
\end{proof}
\s

\newday

(22nd Jaunary, Tuesday)
\s

\propnum{1.2} Let $p(t) = \max_{x,y\in S} \norms{p^t(x,\cdot) - p^t (y, \cdot)}{TV}$. Then 
\begin{align*}
& d (t) \leq p(t) \leq 2d(t) \\
& p(t+s) \leq p(s)p(t) \quad\quad \forall t,s\geq 0
\end{align*}
\begin{proof}
\pf 
\begin{i}
\item[1.] Let $A =A(x)\subset S$ be such that $\norms{p^t(x, \cdot) - \pi}{TV}$. Then $d(t)\leq p(t)$ comes from
\begin{align*}
& \norms{p^t(x, \cdot) -\pi}{TV} = p^t(x, A) - \pi(A) = \sum_{z} \pi(z) (p^t(x,A)- p^t(z,A)) \\
\leq & \max_z p^t(x, A) - p^t(z,A) \leq p(t)
\end{align*}
The inequality $p(t) \leq 2d(t)$ is obtained using simple application of triangular inequality,
\begin{align*}
& p(t) = \max_{x,y} \norms{p^t(x,\cdot) - p^t(y, \cdot)}{TV} \\
\leq & \max_x \norms{p^t(x, \cdot) - \pi}{TV} + \max_y \norms{p^t(y, \cdot) - \pi}{TV} = 2d(t)
\end{align*}
\item[2.] Couple $(X_s, Y_s)$ such that $\prob (X_s = \hat{x}) = p^s(x, \hat{x})$, $p(Y_s = \hat{y}) = p^s(y, \hat{y})$ they are optimally coupled, as given in \textbf{Theorem 1.1}, \textit{i.e.} $\prob(X_s \neq Y_s) = \norms{p^s(x, \cdot) - p^s(y, \cdot)}{TV} \leq p(s)$ (the inequality comes from the previous part of the proposition). Conditionally on the event $X_s = z = Y_s$, take $X_{s+t} = Y_{s,t} \sim p^t(z, \cdot)$. Otherwise, if $X_s = x'$, $Y_s = y'$, with $x' \neq y'$ then take $(X_{s+t}, Y_{s+t})$ such that
\begin{align*}
&\prob[X_{s+t} = \hat{x} | X_s = x', Y_s = y'] =p^t(x', \hat{x}) \\
&\prob[Y_{s+t} = \hat{y} | X_s = x', Y_s = y'] = p^t(y', \hat{y})
\end{align*}
again optimally coupled. Then by \textbf{Theorem 1.1},
\begin{align*}
p(t+s) &\leq \prob[ X_{t+s} \neq Y_{t+s}] = \prob[ X_{t+s} \neq Y_{t+s} | X_s \neq Y_s] \prob[X_s =Y_s] \\
& = p(s)\prob[ X_{t+s} \neq Y_{t+s} | X_s \neq Y_s]
\end{align*}
Let $\nu(x,y)$ be the law of $(X_{s}, Y_{s})$ given $X_s \neq Y_s$, we have
\begin{align*}
\prob[ X_{t+s} \neq Y_{t+s} | X_s \neq Y_s] &= \sum_{z\neq z'} \nu(z,z') \prob[X_{t+s} \neq Y_{t+s} | X_s = z, Y_s =z'] \\
& p(t) \sum_{z\neq z'} \nu(z,z') = p(t)
\end{align*}
and we have the result.
\end{i}
\eop
\end{proof}
\s

\textbf{Example :} \emph{(Random to top shuffle)} We have $n$ cards labelled $1, \cdots, n$. Pick one at random and move to top.
\s

\thmnum{1.2} There cutoff around time is $n\log n$.
\begin{p}
\pf Let $X_0$ have a fixed arbitrary distribution and $Y_0$ have the equilibrium distribution. Couple these two objects as the following : given $X_t$ and $Y_t$, choose a randomly a number $i \in \{1,\cdots, n\}$ and put $i$th card of both $X_t$ and $Y_t$ on the top.

\quad Let $\tau =$(first time every card was picked). Then we may observe that $X_t = Y_t$ if $t \geq \tau$. Then $p(t) \leq \prob (\tau >t )$ - so we make bound on probabilities on $\tau$ to bound $p(t)$.
\begin{subproof}
: Let $z_i=$the time at which $i$ difference cards were picked, and $T_i = z_i - z_{i-1}$. Then they are independent, $\tau = \sum_{i=1}^n T_i$, $T_i \sim \text{Geo}(\frac{n-i+1}{n})$, $\avg[\tau] = \sum_{i=1}^n \frac{n}{n-i+1} \sim n\log n$, and $\text{Var}[\tau] = \sum_{i=1}^n \text{Var}[T_i] = \sum_{i=1}^n (n/i)^2 \leq n^2 \pi^2/6 \ll (E[\tau])^2$. By Chebyshev,
\begin{align*}
\prob[\tau > (1+\epsilon) n\log n] \xrightarrow{n\rightarrow \infty} 0
\end{align*}
\end{subproof}
Therefore we also have $p((1+\epsilon)n\log n) \leq \prob[\tau > (1+ \epsilon) n\log n] \rightarrow 0$ as $n\rightarrow \infty$.  
\s

For the other direction, let $x_i=$the card at position $i$, and $A_j$ be the event of having $A_j =\{ x : x_n> x_{n-1} > \cdots > x_{n-j}\}$, where $x_j$ is the $j$-th card of the deck. Then $\pi(A_j) = 1/(j+1)!$ ($\pi$ is the uniform distribution). If exactly $s$ cards were not shuffled then they will be the bottm $s$ cards at the original relative order. Start from $x = \begin{small} \begin{pmatrix}
1\\
\vdots\\
n
\end{pmatrix} \end{small}$. Then for $t = (1- \epsilon) n\log n$, $j = \lceil \log n \rceil$, again using similar estimate for $\sum T_i$ as above,
\begin{align*}
\prob_x( X_t \in A_j) \geq \prob[\sum_{i=1}^{n-j}T_i >t] = 1-o(t)
\end{align*}
Hence by definition of the distance function $d$,
\begin{align*}
d(t) \geq \prob_x (X_t \in A_j) - \pi(A_j) \geq 1- o(1)
\end{align*}
and we have the desired result
\eop
\end{p}
\s

\subsubsection*{$L_p$ norms}
For $f\in R^S$, let $\norms{f}{p} = (\pi |f|^p)^{1/p}$, $\pi(h) = \sum \pi(x) h(x)$ and $\norms{f}{\infty} = \max_x |f(x)|$. For a signed measure $\sigma$, let
\begin{align*}
&\norms{\sigma}{p, \pi} = \norms{\sigma/\pi}{p} = \Big( \sum_x \pi(x) \Big|\frac{\sigma(x)}{\pi(x)}\Big|^p\Big)^{1/p} \\
&\norms{\sigma}{\infty, \pi} = \max_x \Big|\frac{\sigma(x)}{\pi(x)}\Big|
\end{align*}
If $\mu, \nu$ are distributions on $X$, $\norms{\mu - \nu}{1, \pi} = \sum_{x} \pi (x) |\frac{\mu(x)}{\pi(x)} - \frac{\nu(x)}{\pi(x)}| = 2\norms{\mu -\nu}{TV}$.

\quad By Jensen's inequality, $\norms{\nu - \mu}{p, \pi}$ is non-decreasing in $p$.
\s

\lemnum{1} For reversible chains,
\begin{align*}
\norms{p^s(x, \cdot) - \pi}{2, \pi}^2 = \frac{p^{2s}(x,x)}{\pi(x)}-1
\end{align*}
\begin{p}
\pf \begin{align*}
&\norms{p^s(x, \cdot) - \pi}{2, \pi}^2 =\sum_y \pi(y) \Big(\frac{p^s(x,y)}{\pi(y)}-1 \Big)^2 = -1 + \sum_y \frac{p^s(x,y)^2}{\pi(y)} \\
=& -1 + \sum_{y} \frac{p^s(x,y)p^s(y,x)}{\pi(x)} = -1 +\frac{p^{2s}(x,x)}{\pi(x)}
\end{align*}
\eop
\end{p}
\s

\lemnum{2} Under reversibility,
\begin{align*}
\Big| \frac{p^{s+t}(x,y)}{\pi(y)} -1 \Big|^2 \leq \Big( \frac{p^{2s}(x,x)}{\pi(x)} -1\Big) \Big(\frac{p^{2t}(y,y)}{\pi(y)} -1\Big)
\end{align*}
\begin{p}
\pf \begin{align*}
(LHS) = \Big|\frac{\sum_{z}(p^s(x,z)- \pi(z))(p^t(z,y)- \pi(y))}{\pi(y)}  \Big|^2
\end{align*}
and by reversibility, $p^t(z,y)\pi(z) = p^t(y,z) \pi(y)$ so
\begin{align*}
&=\Big| \sum_{z} \pi(z) \frac{ p^s(x,z) - \pi(z)}{\pi(z)} \cdot \frac{p^t(y,z) - \pi(z)}{\pi(z)} \Big|^2 \\
&\leq \sum_{z} \pi(z) \Big( \frac{p^s(x,z) - \pi(z)}{\pi(z)}\Big)^2 \sum_z \pi(z) \Big( \frac{p^t(y,z)- \pi(z)}{\pi(z)}\Big)^2 
\end{align*}
and using \textbf{Lemma 1}, we have the desired result.
\end{p}
\s

The most interesting case of the lemma is obtained when $t=s$.
\s

\corr \begin{align*}
\max_{x,y} \Big| \frac{p^{2s}(x,y)}{\pi(y)} -1 \Big| = \max_x  \frac{p^{2s} (x,x)}{\pi(x)} - 1
\end{align*}
In other words, if we write $d_p(t) = \max_x \norms{p^t(x,\cdot) -\pi}{p, \pi}$ then $d_{\infty}(t)^2 = d_{\infty}(wt)$
\s

\textbf{Example :} The hypercube ($n$-dimensional) is $G= (\{0,1\}^n, E)$, $E= \{ \{x,y\} : x,y$ differ on exactly 1 corrdinate$\}$. Consider `lazy' random walk
\begin{align*}
p(x,y) =\begin{cases}
1/2 \quad \text{if }x=y \\
1/2n \quad \text{if } x\sim y
\end{cases}
\end{align*}

\newday

(24th January, Thursday)
\s

(no lecture on Tuesda Jan 29th)
\s

(Example continues) The transition matrix of lazy simple random walk on hypercube is $P(x,y) = \frac{1}1_{x=y} + \frac{1}{2n}1_{(x,y)\in E}$.
\s

\thm Cutoof around time is $\frac{1}{2}n\log n$.
\begin{proof}
\pf At each step, pick a random co-oordinate and ``refresh" it, \textit{i.e.} flip with probability $\frac{1}{2}$. Let
\begin{align*}
\tau = \text{first time every co-ordinate is refreshed}
\end{align*}
Then $\tau$ is a coupon-collector time(as before), so is concentrated around time $n\log n$. Then $X_{\tau} \sim \pi$, the equilibrium distribution, and $(X_{\tau}, \tau)$ are independent.
\begin{align*}
d(t) \leq \prob [\tau >t] = o(1) \quad \text{if } t= (1+ \epsilon) n\log n
\end{align*}
\begin{subproof}
: let $\text{sep}(\mu, \nu) = \max_x (1- \frac{\mu(x)}{\nu(x)})$. Then
\begin{align*}
\norms{\mu -\nu}{TV} &= \sum_{x}(\mu(x)- \nu(x))_+ = \sum_{x} \mu(x) \Big( 1- \frac{\nu(x)}{\mu(x)} \Big)_+ \\
&\leq \max_x 1- \frac{\nu(x)}{\mu(x)} = \text{sep}(\mu, \nu)
\end{align*}
Before we complete the proof, we have to prove some results.
\end{subproof}
\end{proof}

\defi $T$ is a \textbf{stationary time} if for some filtrations $\mathscr{F}_t \supset \sigma(X_s:s\leq t)$ such that $1_{\tau >t} \in \mathscr{F}_t$ and $X_{\tau}\sim \pi$.

\quad It is \textbf{strong stationary time (SST)} if alaso $X_{\tau}$ is independent form $\tau$.
\s

\lem If $\tau$ is a SST, then
\begin{align*}
\max_x \text{sep}(p^t(x,\cdot), \pi) \leq \max_x \prob_x (\tau >t)
\end{align*}
\begin{proof}
\pf \begin{align*}
p^t(x,y) \geq \sum_{s=0}^t \prob_x [X_t =y, \tau=s] &= \sum_{s=0}^t \prob[\tau =s] \prob_{\pi}(X_{t-s}= y) \\
& = \pi(y) \prob[\tau \leq t]
\end{align*}
\eop
\end{proof}
\s

This lemma justifies the inequality $d(t)\leq \prob[T>t]$ above.
\s

\begin{proof}
Now let us work in the continuous time setting - ``Refresh" each coordinate independently at rate $\frac{1}{n}$. By symmetry, $p^{2t}(x,x)$ is independent of $x$,
\begin{align*}
d_2^2(t) = \frac{p^{2t}(0,0)}{\pi(0) }-1 = 2^n Q_{2t}(0,0)-1 = (\star)
\end{align*}
where $Q = \begin{pmatrix}
-1/2n & 1/2n \\
1/2n & -1/2n
\end{pmatrix}$. Then $Q_{2t}(0,0) = \frac{1}{2} + \frac{1}{2} e^{-2t/n}$ so
\begin{align*}
(\star) = (1+ e^{-2t/n})^n -1 \leq \exp(ne^{-wt/n})) -1 = \exp(n^{-\epsilon})-1 \leq 2n^{-\epsilon} \xrightarrow{n\rightarrow \infty} 0
\end{align*}
for $t = \frac{1+\epsilon}{2} nlog n$, where we used $e^x \leq 1+x + x^2$ for $x\in[-1,1]$.
\end{proof}
\s

\section{Spectral Methods}
(we follow Levin Chapter 12)
\s

\propnum{2.7} Let $P$ be a transition matrix. Then
\begin{i}
\item[1.] If $\lambda$ is an eigenvector of $P$, then $|\lambda|\leq 1$.
\item[2.] If $P$ is irreducible and $Pf=f$, then $f = (c, \cdots, c)^T$. 
\item[3.] If $P$ is irreducible aperiodic, and $\lambda \neq 1$ is an eigenvalue, then $|\lambda|<1$.

\quad If $P$ has period, then $e^{-2\pi i/n}$ is a eigenvalue.
\end{i}
\begin{p}
\pf \begin{i}
\item[2.] Let $x\in S$ be such that $f(x)= \max_y |f(y)|$ then $Pf(x) = f(x) = \sum P(x,y)f(y)$ hence $f(y) = f(x)$ for all $y$ such that $P(x,y) >0$.

\quad Now apply this argument iterativel to find that $f$ is constant.
\item[3.] (later part) We can partition $S$ to $s$ sets $A_1, \cdots, A_s$ such that $P(x, A_{i+1}) =1$ for all $i$ and $x\in A_i$, Then $f(x) = e^{\frac{2\pi i}{s}j}$ for $x\in A_j$, so $Pf = f\cdot e^{\frac{2\pi i}{s}}$.
\end{i}
\end{p}
\s

\emph{Recall :} $P^*(x,y) = \frac{\pi(y)}{\pi(x)}P(y,x)$. Then we have
\begin{align*}
\rho(t) = \max_{f: \pi(f) =0, f\neq 0} \frac{\norms{(P^*)^T f}{1}}{\norms{f}{1}}  
\end{align*}
\begin{proof}
\emph{[Hint : try $f =\frac{1_x}{\pi(x)} - \frac{1_y}{\pi(y)}$ and express general $f\neq 0$ with $\pi f=0$ as $f=\sum_{xy} c_{xy}\Big(\frac{1_x}{\pi(x)}-\frac{1_y}{\pi(y)} \Big)$ with $\sum_{xy}|c_{xy}|=\frac{1}{2}\norms{f}{1}$.]}
\end{proof}
\s

\lem \begin{i}
\item[1.] $\sum_y P^*(x,y)=1$.
\item[2.] $P$ is irreducbie \emph{iff} $P^*$ is
\item[3.] $\pi(x_1)P(x_1, x_2) \cdots P(x_{s-1},x_s)= \pi(x_s) p^*(x_s,x_{s-1}, \cdots p^*(x_2,x_1))$ so $\pi(x) (P^*)^s(x,y) = \pi(y) P^s(y,x)$, i.e. $(P^*)^s = (P^s)^*$. In particular, $(P^*)^s (x,x) = P^s (x,x)$.
\item[4.] $P$ is aperiodic \emph{iff} $P^*$ is.
\item[5.] $\pi P^* =\pi$.
\item[6.] $\langle Pf, g\rangle_{\pi} = \langle f, P^* g\rangle_{\pi}$
\begin{proof}
\textbf{proof of 6)} \begin{i}
\item[6.]
\begin{align*}
\sum_x \pi(x) g(x) \sum_y P(x,y) f(y) &= \sum_x g(x) \sum_y \pi(y) P^*(y,x) f(y) \\
&= \sum_{y} f(y) \sum_x \pi(y) P^*(y,x)g(x) = \langle f,P^* g\rangle_{\pi}
\end{align*}
\end{i}
\end{proof}
\end{i}
\s

\lem If $Pf =\lambda f$, $\lambda \neq 1$, then $\pi f=0$.
\begin{p}
\pf \begin{align*}
\lambda \pi(f) = \langle Pf, 1\rangle_{\pi} = \langle, f, P^*\rangle_{\pi} = \pi(f)
\end{align*}
\eop
\end{p}
\s

\begin{proof}
\textbf{proof of 1,3)}Has $\rho_*(t) \geq |\lambda|^t$ if $\lambda$ is an eigenvalue of $P$ and $\rho_*$ is $rho$ w.r.t. $P^*$. But $P$ is irreducuble aperiodic then so is $P^*$. Therefore $P_*(t) \xrightarrow{t\rightarrow \infty} 0$, so $|\lambda|<1$.

\eop
\end{proof}
\s

\thmnum{2.1} Assume $P$ is irducible and reversible w.r.t. $\pi$.
\begin{i}
\item[1.] $\exists$ an orthonormal basis $f_1, \cdots, f_n$ w.r.t $\langle \cdot, \cdot \rangle_{\pi}$ such that $f_1 =1$ and $Pf_i =\lambda_i f_i$, $\lambda_1 =1 > \lambda_2 \geq \cdots \geq \lambda_n \geq -1$.
\item[2.] $\frac{P^t(x,y)}{\pi(y)} - 1 =\sum_{j=2}^n f_j(x) f_j(y)\lambda_j^t$.
\end{i}
\begin{proof}
\pf Note that $A(x,y)  =\sqrt{\frac{\pi(x)}{\pi(y)}} P(x,y)$ is symmetric, so $A = D_{\pi} P D_{\pi}^{-1}$, $D_{\pi} = \text{Diag}(\sqrt{\pi(x)})$. Then $\exists \phi_1, \cdots, \phi_n$ such that $\langle \phi_i, \phi_j \rangle = 1_{i=j}$ and $A\phi_j = \lambda \phi)j$. Let $f_j = D^{-1}_{\pi} \phi_j$, then
\begin{align*}
&Pf_j = D_{\pi}^{-1} A \phi_j = \lambda_j D_{\pi}^{-1} \phi_j = \lambda_j f_j \\
&\langle f_j, f_i\rangle_{\pi} = \langle \phi_j, \phi_i \rangle = 1_{i=j}
\end{align*}

2. 
\begin{align*}
&P^T 1_y = \sum_j P^T f_j \langle f_j, 1_y\rangle_{\pi} = \sum_j \lambda_j^T f_j \pi(y)f_j(y)
& P^T(x,y) = P^t 1_y(x) = \sum_{j=1}^n f_j(x)f_j(y) \pi_y\lambda_j^t
\end{align*}
\end{proof}
\s

\corr Let $\lambda_* = |\lambda_2| \vee |\lambda_n|$. Then
\begin{align*}
\frac{P^{2t}(x,x)}{\pi(x)} -1 \leq \lambda_*^2 (\frac{p^{2t-2}(x,x)}{\pi(x)}-1)\leq \lambda_*^{2t}(\frac{1-\pi(x)}{\pi(x)})
\end{align*}

\end{document}
