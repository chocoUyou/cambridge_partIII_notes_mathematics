\documentclass[10pt,a4paper]{report}


\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
%\usepackage{unicode-math}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{calrsfs}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[mathscr]{euscript}

\usepackage{color}

%%%%%%%%Draws Pretty Box%%%%%%%
%%%Use with \bluebox[<top pad>][<bot pad>]{<contents>}
\definecolor{myblue}{rgb}{.8, .8, 1}

\usepackage{empheq}

\newlength\mytemplen
\newsavebox\mytempbox

\makeatletter
\newcommand\bluebox{%
    \@ifnextchar[%]
       {\@bluebox}%
       {\@bluebox[0pt]}}

\def\@bluebox[#1]{%
    \@ifnextchar[%]
       {\@@bluebox[#1]}%
       {\@@bluebox[#1][0pt]}}

\def\@@bluebox[#1][#2]#3{
    \sbox\mytempbox{#3}%
    \mytemplen\ht\mytempbox
    \advance\mytemplen #1\relax
    \ht\mytempbox\mytemplen
    \mytemplen\dp\mytempbox
    \advance\mytemplen #2\relax
    \dp\mytempbox\mytemplen
    \colorbox{myblue}{\hspace{1em}\usebox{\mytempbox}\hspace{1em}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%for writing symbol above an equality
\newcommand\xeq{\stackrel{\mathclap{\normalfont\mbox{d}}}{=}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%for drawing commutative diagrams.%%%%%%
\usepackage{tikz-cd}  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%for changing margin
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 

\newenvironment{proof}
{\begin{changemargin}{1cm}{0.5cm} 
	}%your text here
	{\end{changemargin}
}

\newenvironment{subproof}
{\begin{changemargin}{0.5cm}{0.5cm} 
	}%your text here
	{\end{changemargin}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\newcommand{\thm}{\textbf{Theorem) }}
\newcommand{\thmnum}[1]{\textbf{Theorem #1) }}
\newcommand{\defi}{\textbf{Definition) }}
\newcommand{\definum}[1]{\textbf{Definition #1) }}
\newcommand{\lem}{\textbf{Lemma) }}
\newcommand{\lemnum}[1]{\textbf{Lemma #1) }}
\newcommand{\prop}{\textbf{Proposition)}}
\newcommand{\propnum}[1]{\textbf{Proposition #1) }}
\newcommand{\corr}{\textbf{Corollary) }}
\newcommand{\corrnum}[1]{\textbf{Corollary #1) }}
\newcommand{\pf}{\textbf{proof) }}


\newcommand{\lap}{\triangle} %%Laplacian
\newcommand{\s}{\vspace{10pt}}
\newcommand{\bull}{$\bullet$}
\newcommand{\sta}{$\star$}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\eop}{\hfill  \textsl{(End of proof)} $\square$} %end of proof
\newcommand{\eos}{\hfill  \textsl{(End of statement)} $\square$} %end of proof


\newcommand{\intN}{\mathbb{Z}_N}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\norms}[2]{\parallel #1 \parallel_{#2}}
\newcommand{\avg}{\mathbb{E}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\borel}{\mathscr{B}}
\newcommand{\EE}{\mathscr{E}}
\newcommand{\F}{\mathscr{F}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\var}{\text{Var}}
\newcommand{\cha}{1}


\newcommand{\newday}{===============================================================}

\setlength\parindent{0pt}

\chapter*{Advanced Probability}
\s

\newday

(2nd November, Friday)

\section*{Chapter 5. Weak Convergence}

\subsection*{5.1. Definitions}

Let $E$ be a metric space. Whenever we are talking about a metric space, the $\sigma$-algebra is given by the Borel $\sigma$-algebra. Write $C_b(E)$ for the set of bounded continuous functions on $E$.
\begin{itemize}
\item Let $(\mu_n :n\in \mathbb{N})$ be a sequence of probability measures and let $\mu$ be another probability measure on $E$. We say that $\mu_n \rightarrow \mu$ \textbf{weakly} (as $n\rightarrow \infty$) if $\mu_n (f) \rightarrow \mu(f)$ for all $f\in C_b(\reals)$.
\end{itemize}
\s

\thmnum{5.1.1} The following are equivalent.
\begin{itemize}
\item[(a)] $\mu_n \rightarrow \mu$ weakly on $E$
\item[(b)] $\liminf_{n\rightarrow \infty} \mu_n(U) \geq \mu(U)$ for all $U$ open
\item[(c)] $\limsup_{\mu(F)} \leq \mu(F)$ for all $F$ closed.
\item[(d)] $\mu_n(B) \rightarrow \mu(B)$ for all $B \in \borel$ such that $\mu(\partial B)=0$.(Boundary is the set of limit points of $B$ that are not contained in $B$.)
\end{itemize}
\begin{proof}
\pf Exercise.
\end{proof}
\s

For an example, consider a sequence $(x_n)_n \subset \reals$ such that $x_n \rightarrow 0$ as $n\rightarrow \infty$. We want to have $\delta_{x_n} \rightarrow \delta_0$. Indeed, this is true in the weak sense. However, the sequence has $\delta_{x_n}(\{0\}) =0$ for all $n$, hence we should have inequality in condition (c).
\s

We have a similar version of the theorem for the real line.
\s

\propnum{5.1.2} Consider the case $E =\reals$. TFAE
\begin{itemize}
\item[(a)] $\mu_n \rightarrow \mu$ weakly for some probability measure $\mu$.
\item[(b)] $F_n(x) \rightarrow F(x)$ for all $x\in \reals$ such that $F(x^-) = F(x)$. (Here, $F(x)  = \mu((\infty,x])$ is the \textbf{distribution function} of $\mu$.) (Sometimes called convergence of distributions)
\item[(c)] There exists a probability space $(\Omega,\F, \prob)$ and random variables $X_n, X$ on $\Omega$ such that $X_n \sim \mu_n$, $X\sim \mu$ and $X_n \rightarrow X$ almost surely.
\end{itemize}
\begin{proof}
\pf See probability and measure notes.
\end{proof}
\s

\subsection*{5.2. Prohorov's Theorem}

When does a sequence of probability measures has a converging subsequence?
\s

Let $E$ be a metric space and $(\mu_n : n\in \mathbb{N})$ be a sequence of probability measures on $E$.
\begin{itemize}
\item We say that $(\mu_n)_n$ is \textbf{tight} if for all $\epsilon >0$, there is a compact set $K \subset E$ such that
\begin{align*}
\mu_n (E \backslash K) \leq \epsilon \quad \forall n\in \mathbb{N}
\end{align*}
\s

For example, the sequence $(\delta_n)_n$ is \emph{not} tight.
\end{itemize}
\s

\thmnum{5.2.1} Let $(\mu_n : n\in \mathbb{N})$ be a sequence of probability measures on a metric space $E$ and suppose that $(\mu_n : n\in \mathbb{N})$ is tight. Then there exists a subsequence $(n_k)_k \subset \mathbb{N}$ and probability measure $\mu$ on $E$ such that $\mu_{n_k} \rightarrow \mu$ weakly as $k\rightarrow \infty$.
\s

This gives a version of weakly sequential compactness of probability measures. We are only going to prove this for $\reals$. This theorem is hard to prove in general.(e.g. there is a method using Monge-Kantorovich metric defined for Polish spaces. For this method, see "Topics in Optimal Transport", C.Villani, Ame.Soc.Math. For the general version, see the attached note)
\begin{proof}
\textbf{proof for $E=\reals$) } By a diagonal argument and by passing to a subsequence, it suffices to consider the case where $F_n(x) \rightarrow g(x)$ as $n\rightarrow \infty$ for all $x\in \mathbb{Q}$ for some $g(x) \in [0,1]$, where $F_n$ is the distribution function of $F_n$. Now $g: \mathbb{Q} \rightarrow [0,1]$ is non-decreasing so $g$ has a non-decreasing extension $G : \reals \rightarrow [0,1]$, i.e.
\begin{align*}
G(x) = \lim_{q\searrow x , q\in \mathbb{Q}} g(q)
\end{align*}
which has only countably many discontinuities.(because there should be a rational number in each discontinuity). Now we must have
\begin{align*}
F_n(x) \rightarrow G(x) \quad \forall x \text{ s.t. }G \text{is continuous at }x
\end{align*}
Set $F(x) = G(x^+)$, then $F$ and $G$ have same points of continuity, so $F_n(x) \rightarrow F(x)$ for all $x\in \reals$. 

\quad We are only left to check that $G(x) \rightarrow 1$ as $x\rightarrow \infty$ using tightness condition.

\quad Since $(\mu_n : n\in \mathbb{N})$ is tight, given $\epsilon >0$, there exists $R < \infty$ such that $\mu_n (\reals \backslash(-R,R)) \leq\epsilon$ for all $\epsilon$ so $F_n(-R) \leq \epsilon$, $F_n(R) \geq 1-\epsilon$. So
\begin{align*}
& F(x) \rightarrow 0 \quad \text{as } x\rightarrow -\infty \\
& F(x) \rightarrow 1 \quad \text{as } x\rightarrow \infty 
\end{align*}
So $F$ is distribution function. So there exists a probability measure $\mu$ such that $\mu((-\infty,x]) = F(x)$. Then $\mu_n \rightarrow \mu$ by \textbf{Prop 5.1.2.}

\eop
\end{proof}
\s

\subsection*{5.3. Weak Convergence and Characteristic Functions}

Take $E = \reals^d$. 
\begin{itemize}
\item For a probability measures $mu$ on $\reals^d$, define its \textbf{characteristic function} $\phi : \reals^d \rightarrow \mathbb{C}$ by
\begin{align*}
\phi(u) = \int_{\reals^d} e^{i \langle u, x\rangle} \mu(dx)
\end{align*}
\end{itemize}
\s

\lemnum{5.3.1} Fix $d=1$. For all $\lambda \in (0,\infty)$,
\begin{align*}
\mu(\reals \backslash (-\lambda, \lambda)) \leq C\lambda \int_{0}^{\lambda} (1- \text{Re}(\phi(u))) du
\end{align*}
where $C = (1- \sin(1))^{-1} < \infty$.
\begin{proof}
\pf Consider for $t\geq 1$. Let $A(t) = t^{-1} \int_0^t (1-\cos v) dv$. Then
\begin{align*}
A(t) \geq A(0) = 1-\sin (t) 
\end{align*}
(to see this, observe that $A(t)$ is the average of $(1-\cos(v))$ on interval $(0,t)$ and divide the cases $|t| \leq \pi /2$ and $|t| \geq \pi/2$)

So $Ct^{-1} \int_0^t (1- \cos(v)) dv \geq 1$. Substitute $v = uy$, $u=v/y$,
\begin{align*}
Ct^{-1} \int_0^{t/y} (1- \cos(uy)) y du \geq 1
\end{align*}
Put $t/y = 1/\lambda$, $\lambda = y/t$, $t= y/\lambda \geq 1$ to see
\begin{align*}
C\lambda \int_{0}^{1/\lambda} (1- \cos (uy)) du \geq 1
\end{align*}
whenever $t=y/\lambda \geq 1$(this was the assumption we started with). Now for general $y \in \reals$, has
\begin{align*}
C\lambda \int_{0}^{1/\lambda} (1- \cos (uy)) du \geq 1_{|y| \geq \lambda}
\end{align*}
Now integrate with respect to $\mu$ and use Fubini.
\begin{align*}
\mu(\reals \backslash (-\lambda, \lambda)) &\leq C\lambda \int_{\reals} \int_0^{1/\lambda} (1- \cos (uy)) du \mu(dy) \\
&= C\lambda \int_0^{1/\lambda} \int_{\reals} (1- \cos(uy)) du \mu(dy)
\end{align*}

\eop
\end{proof}
\s

\newday

(5th November, Monday)
\s

\thmnum{5.3.2} Let $\mu_n, \mu$ be probability measures on $\reals^d$ with characteristic functions $\phi_n, \phi$. Then the following are equivalent
\begin{itemize}
\item[(a)] $\mu_n \rightarrow \mu$ weakly on $\reals^d$.
\item[(b)] $\phi_n(u) \rightarrow \phi(u)$ for all $u\in \reals^d$.
\end{itemize}
We will prove only for the case $d=1$.
\begin{proof}
\pf It is clear that (a) implies (b). Suppose (b) holds. We prove via a 'compactness argument'. We aim to show that the sequence $(\mu_n)_n$ tight, and therefore has a converging subsequence, and show that the converging point is in fact $\mu$.

\quad Note that $\phi(0) = 1$ and $\phi$ is continuous. Given $\epsilon >0$, there exists $\lambda < \infty$ such that
\begin{align*}
C \lambda \int^{1/\lambda}_0 (1- \text{Re}(\phi(u))) du \leq \epsilon/2
\end{align*}
with $C = (1- \sin(1))^{-1} < \infty$. By dominated convergence,
\begin{align*}
\int_0^{1/\lambda} (1- \text{Re}(\phi_n(u))) du \xrightarrow{n\rightarrow \infty} \int^{1/\lambda}_0 (1- \text{Re}(\phi(u))) du
\end{align*}
so for sufficiently large $n$, by \textbf{Lemma 5.3.1,},
\begin{align*}
\mu_n (\reals \backslash (-\lambda , \lambda)) \leq C\lambda \int^{1/\lambda}_0 (1- \text{Re}(\phi_n(u))) du \leq \epsilon
\end{align*}
Since $\epsilon$ was arbitrary, we see that $(\mu_n : n\in \mathbb{N})$ is tight. By Prohorov's theorem, we have a converging subsequence $\mu_{n_k} \rightarrow \nu$ for some probability measure $\nu$.

\quad Suppose for a contradiction that $\nu \neq \mu$. Therefore, there exists $\epsilon >0$, and $f\in C_b(\reals^n)$ such that
\begin{align*}
|\mu_{n_k} (f) - \mu(f)| \geq \epsilon \quad \forall k
\end{align*}
By above argument, we have $\mu_{n_k} \rightarrow \nu$. But then, since $e^{inx}$ is a bounded continuous function,
\begin{align*}
\int_{\reals} e^{inx} \nu(dx) = \lim_{k\rightarrow \infty} \phi_{n_k}(n) = \phi(n)
\end{align*}
which indicates $\mu = \nu$ by uniqueness of characteristic functions (see PM notes), a contradiction.

\eop
\end{proof}
\s

In fact, the proof of the theorem implies a slightly stronger statement, which is less useful.
\s

\thmnum{5.3.3} \emph{(L\'{e}vy's continuity theorem for characteristic functions)} Let $(\mu_n : n\in \mathbb{N})$ be a sequence of probability measures on $\reals^n$ with characteristic functions $\phi_n$. Suppose $\phi_n (u) \rightarrow \phi(u)$ for all $u$ for some function $\phi$ (not necessarily a characteristic function) such that $\phi$ is continuous at $0$. Then $\phi$ is the characteristic function of some probability measure $\mu$ on $\reals^d$ and $\mu_n \rightarrow \mu$ weakly on $\reals^d$.
\s

\section*{6. Large Deviations}
	
\subsection*{6.1. Cram\'{e}rs theorem}

\thmnum{6.1.1} Let $(X_n : n\in \mathbb{N})$ be a sequence of integrable \emph{i.i.d.} random variables in $\reals$. Set $m = \avg(X_1)$, $S_n = X_1 + \cdots + X_n$. We know $S_n / n \rightarrow \delta_m$ in probability, so if $(m-\epsilon, m+ \epsilon) \cap B =\phi$ then $\prob (S_n/n \in B) \rightarrow 0$ as $n\rightarrow \infty$. Then in fact the convergence rate is given by $\sim \exp (-n \alpha(B))$ for some $\alpha$. To be precise, for all $a\geq m = \avg(X_1)$, as $n\rightarrow \infty$,
\begin{align*}
\frac{1}{n} \log \prob (S_n \geq na) \rightarrow -\psi^*(a)
\end{align*}
where $\psi^*$ is the \emph{Legendre transform} of the \emph{cumulant generating function} $\psi (\lambda) = \log (\avg(e^{\lambda X_1}))$, where Legendre transform is given by
\begin{align*}
\psi^* (x) = \sup_{\lambda \in \reals} \{ \lambda x - \psi(\lambda) \}
\end{align*}
\quad In particular, for $n$ sufficiently large and in case $\psi^{*}(a) < \infty$, we get
\begin{align*}
-\psi^{*}(a) - \epsilon \leq \frac{1}{n} \log (\prob(S_n \geq a)) \leq -\psi^{*}(a) + \epsilon 
\end{align*}
and therefore
\begin{align*}
e^{-n(\psi^*(a)+\epsilon)} \leq \prob (S_n \geq na) \leq e^{-n(\psi^*(a)-\epsilon)} .
\end{align*}
\s

\textbf{Note :} $\psi$ is always a convex function, so $\psi^*$ is also a convex function.
\s

\textbf{Examples :}
\begin{itemize}
\item[(i)] $X_1 \sim N(0,1)$, then $\avg(e^{\lambda X_1}) = e^{\lambda^2 /2}$,  $\psi(\lambda) = \lambda^2 /2$ and $\psi^*(x) = x^2/2$. Hence
\begin{align*}
\frac{1}{n} \log (\prob(S_n \geq a)) \rightarrow -\frac{a^2}{2} \quad \forall a\geq 0
\end{align*}
Can check this directly, using the fact that $S_n \sim N(0,n)$ in this case.
\item[(ii)] $X_1 \sim \text{Exp}(1)$, then
\begin{align*}
\avg(e^{\lambda X_1}) = \int_0^{\infty} e^{\lambda x} e^{-x} dx = \begin{cases}
\begin{array}{ll}
\infty & \text{if } \lambda \geq 1 \\
\frac{1}{1-\lambda} & \text{if } \lambda <1
\end{array}
\end{cases}
\end{align*} 
so $\psi(\lambda) = -\log (1-\lambda)$ if $\lambda <1$ and $\infty$ otherwise, and $\psi^*(x) = x-1-\log(x)$ for $x>0$. Cram\'{e}r's theorem implies that
\begin{align*}
\frac{1}{n} \log \prob(S_n \geq na) \rightarrow -(a-1-\log (a)) \quad \forall a\geq 1
\end{align*}
On the other hand, $\text{Var}(X_1) =1 <\infty$, so $\frac{S_n -n}{\sqrt{n}} \rightarrow N(0,1)$ by CLT. So
\begin{align*}
\prob(S_n \geq n + a\sqrt{n}) \rightarrow \int_a^{\infty} \frac{1}{\sqrt{2\pi}} e^{-x^2/2} dx
\end{align*}
so Cram\'{e}r's theorem gives a result of a different flavour from CLT for distributions with bounded variation : while CLT provides a description for distribution near the average, Cram\'{e}r gives an explanation of tail distribution of $S_n$.
\end{itemize}
\s

\begin{proof}
\textbf{preparation for proof of Cram\'{e}r's theorem) } Let $\mu(B) = \prob(X_1 \in B)$. Exclude the easy case where $\mu = \delta_m$. Define for $\lambda \geq 0$ with $\psi(\lambda) < \infty$, the \textbf{tilted distribution} $\mu_{\lambda}$ by
\begin{align*}
\mu_{\lambda} (dx) \propto e^{\lambda x} \mu(dx)
\end{align*}
For $K \geq m = \avg(X_1)$, define the conditional distribution by
\begin{align*}
\mu_K(dx | x\leq K) \propto 1_{\{x\leq K\}} \mu(dx)
\end{align*}
The CGF(cumulant generating function) of $\mu_K$ is then given by
\begin{align*}
\psi_{K}(\lambda) = \log(\avg(e^{\lambda X_1} | X_1 \leq K))
\end{align*}
\end{proof}

\s














\end{document}