\documentclass[10pt,a4paper]{report}


\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
%\usepackage{unicode-math}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{calrsfs}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[mathscr]{euscript}

\usepackage{color}

%%%%%%%%Draws Pretty Box%%%%%%%
%%%Use with \bluebox[<top pad>][<bot pad>]{<contents>}
\definecolor{myblue}{rgb}{.8, .8, 1}

\usepackage{empheq}

\newlength\mytemplen
\newsavebox\mytempbox

\makeatletter
\newcommand\bluebox{%
    \@ifnextchar[%]
       {\@bluebox}%
       {\@bluebox[0pt]}}

\def\@bluebox[#1]{%
    \@ifnextchar[%]
       {\@@bluebox[#1]}%
       {\@@bluebox[#1][0pt]}}

\def\@@bluebox[#1][#2]#3{
    \sbox\mytempbox{#3}%
    \mytemplen\ht\mytempbox
    \advance\mytemplen #1\relax
    \ht\mytempbox\mytemplen
    \mytemplen\dp\mytempbox
    \advance\mytemplen #2\relax
    \dp\mytempbox\mytemplen
    \colorbox{myblue}{\hspace{0em}\usebox{\mytempbox}\hspace{0em}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%for writing symbol above an equality
\newcommand\xeq{\stackrel{\mathclap{\normalfont\mbox{d}}}{=}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%for drawing commutative diagrams.%%%%%%
\usepackage{tikz-cd}  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%for changing margin
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 

\newenvironment{proof}
{\begin{changemargin}{1cm}{0.5cm} 
	}%your text here
	{\end{changemargin}
}

\newenvironment{subproof}
{\begin{changemargin}{0.5cm}{0.5cm} 
	}%your text here
	{\end{changemargin}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\newcommand{\thm}{\textbf{Theorem) }}
\newcommand{\thmnum}[1]{\textbf{Theorem #1) }}
\newcommand{\defi}{\textbf{Definition) }}
\newcommand{\definum}[1]{\textbf{Definition #1) }}
\newcommand{\lem}{\textbf{Lemma) }}
\newcommand{\lemnum}[1]{\textbf{Lemma #1) }}
\newcommand{\prop}{\textbf{Proposition)}}
\newcommand{\propnum}[1]{\textbf{Proposition #1) }}
\newcommand{\corr}{\textbf{Corollary) }}
\newcommand{\corrnum}[1]{\textbf{Corollary #1) }}
\newcommand{\pf}{\textbf{proof) }}


\newcommand{\lap}{\triangle} %%Laplacian
\newcommand{\s}{\vspace{10pt}}
\newcommand{\bull}{$\bullet$}
\newcommand{\sta}{$\star$}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\eop}{\hfill  \textsl{(End of proof)} $\square$} %end of proof
\newcommand{\eos}{\hfill  \textsl{(End of statement)} $\square$} %end of proof


\newcommand{\intN}{\mathbb{Z}_N}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\norms}[2]{\parallel #1 \parallel_{#2}}
\newcommand{\avg}{\mathbb{E}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\borel}{\mathscr{B}}
\newcommand{\EE}{\mathscr{E}}
\newcommand{\F}{\mathscr{F}}
\newcommand{\W}{\mathscr{W}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\var}{\text{Var}}
\newcommand{\cha}{1}


\newcommand{\newday}{===============================================================}

\setlength\parindent{0pt}

\chapter*{Advanced Probability}
\s

\newday

(2nd November, Friday)

\section*{Chapter 5. Weak Convergence}

\subsection*{5.1. Definitions}

Let $E$ be a metric space. Whenever we are talking about a metric space, the $\sigma$-algebra is given by the Borel $\sigma$-algebra. Write $C_b(E)$ for the set of bounded continuous functions on $E$.
\begin{itemize}
\item Let $(\mu_n :n\in \mathbb{N})$ be a sequence of probability measures and let $\mu$ be another probability measure on $E$. We say that $\mu_n \rightarrow \mu$ \textbf{weakly} (as $n\rightarrow \infty$) if $\mu_n (f) \rightarrow \mu(f)$ for all $f\in C_b(\reals)$.
\end{itemize}
\s

\thmnum{5.1.1} The following are equivalent.
\begin{itemize}
\item[(a)] $\mu_n \rightarrow \mu$ weakly on $E$
\item[(b)] $\liminf_{n\rightarrow \infty} \mu_n(U) \geq \mu(U)$ for all $U$ open
\item[(c)] $\limsup_{\mu(F)} \leq \mu(F)$ for all $F$ closed.
\item[(d)] $\mu_n(B) \rightarrow \mu(B)$ for all $B \in \borel$ such that $\mu(\partial B)=0$.(Boundary is the set of limit points of $B$ that are not contained in $B$.)
\end{itemize}
\begin{proof}
\pf Exercise.
\end{proof}
\s

For an example, consider a sequence $(x_n)_n \subset \reals$ such that $x_n \rightarrow 0$ as $n\rightarrow \infty$. We want to have $\delta_{x_n} \rightarrow \delta_0$. Indeed, this is true in the weak sense. However, the sequence has $\delta_{x_n}(\{0\}) =0$ for all $n$, hence we should have inequality in condition (c).
\s

We have a similar version of the theorem for the real line.
\s

\propnum{5.1.2} Consider the case $E =\reals$. TFAE
\begin{itemize}
\item[(a)] $\mu_n \rightarrow \mu$ weakly for some probability measure $\mu$.
\item[(b)] $F_n(x) \rightarrow F(x)$ for all $x\in \reals$ such that $F(x^-) = F(x)$. (Here, $F(x)  = \mu((\infty,x])$ is the \textbf{distribution function} of $\mu$.) (Sometimes called convergence of distributions)
\item[(c)] There exists a probability space $(\Omega,\F, \prob)$ and random variables $X_n, X$ on $\Omega$ such that $X_n \sim \mu_n$, $X\sim \mu$ and $X_n \rightarrow X$ almost surely.
\end{itemize}
\begin{proof}
\pf See probability and measure notes.
\end{proof}
\s

\subsection*{5.2. Prohorov's Theorem}

When does a sequence of probability measures has a converging subsequence?
\s

Let $E$ be a metric space and $(\mu_n : n\in \mathbb{N})$ be a sequence of probability measures on $E$.
\begin{itemize}
\item We say that $(\mu_n)_n$ is \textbf{tight} if for all $\epsilon >0$, there is a compact set $K \subset E$ such that
\begin{align*}
\mu_n (E \backslash K) \leq \epsilon \quad \forall n\in \mathbb{N}
\end{align*}
\s

For example, the sequence $(\delta_n)_n$ is \emph{not} tight.
\end{itemize}
\s

\thmnum{5.2.1} Let $(\mu_n : n\in \mathbb{N})$ be a sequence of probability measures on a metric space $E$ and suppose that $(\mu_n : n\in \mathbb{N})$ is tight. Then there exists a subsequence $(n_k)_k \subset \mathbb{N}$ and probability measure $\mu$ on $E$ such that $\mu_{n_k} \rightarrow \mu$ weakly as $k\rightarrow \infty$.
\s

This gives a version of weakly sequential compactness of probability measures. We are only going to prove this for $\reals$. This theorem is hard to prove in general.(e.g. there is a method using Monge-Kantorovich metric defined for Polish spaces. For this method, see "Topics in Optimal Transport", C.Villani, Ame.Soc.Math. For the general version, see the attached note)
\begin{proof}
\textbf{proof for $E=\reals$) } By a diagonal argument and by passing to a subsequence, it suffices to consider the case where $F_n(x) \rightarrow g(x)$ as $n\rightarrow \infty$ for all $x\in \mathbb{Q}$ for some $g(x) \in [0,1]$, where $F_n$ is the distribution function of $F_n$. Now $g: \mathbb{Q} \rightarrow [0,1]$ is non-decreasing so $g$ has a non-decreasing extension $G : \reals \rightarrow [0,1]$, i.e.
\begin{align*}
G(x) = \lim_{q\searrow x , q\in \mathbb{Q}} g(q)
\end{align*}
which has only countably many discontinuities.(because there should be a rational number in each discontinuity). Now we must have
\begin{align*}
F_n(x) \rightarrow G(x) \quad \forall x \text{ s.t. }G \text{is continuous at }x
\end{align*}
Set $F(x) = G(x^+)$, then $F$ and $G$ have same points of continuity, so $F_n(x) \rightarrow F(x)$ for all $x\in \reals$. 

\quad We are only left to check that $G(x) \rightarrow 1$ as $x\rightarrow \infty$ using tightness condition.

\quad Since $(\mu_n : n\in \mathbb{N})$ is tight, given $\epsilon >0$, there exists $R < \infty$ such that $\mu_n (\reals \backslash(-R,R)) \leq\epsilon$ for all $\epsilon$ so $F_n(-R) \leq \epsilon$, $F_n(R) \geq 1-\epsilon$. So
\begin{align*}
& F(x) \rightarrow 0 \quad \text{as } x\rightarrow -\infty \\
& F(x) \rightarrow 1 \quad \text{as } x\rightarrow \infty 
\end{align*}
So $F$ is distribution function. So there exists a probability measure $\mu$ such that $\mu((-\infty,x]) = F(x)$. Then $\mu_n \rightarrow \mu$ by \textbf{Prop 5.1.2.}

\eop
\end{proof}
\s

\subsection*{5.3. Weak Convergence and Characteristic Functions}

Take $E = \reals^d$. 
\begin{itemize}
\item For a probability measures $mu$ on $\reals^d$, define its \textbf{characteristic function} $\phi : \reals^d \rightarrow \mathbb{C}$ by
\begin{align*}
\phi(u) = \int_{\reals^d} e^{i \langle u, x\rangle} \mu(dx)
\end{align*}
\end{itemize}
\s

\lemnum{5.3.1} Fix $d=1$. For all $\lambda \in (0,\infty)$,
\begin{align*}
\mu(\reals \backslash (-\lambda, \lambda)) \leq C\lambda \int_{0}^{\lambda} (1- \text{Re}(\phi(u))) du
\end{align*}
where $C = (1- \sin(1))^{-1} < \infty$.
\begin{proof}
\pf Consider for $t\geq 1$. Let $A(t) = t^{-1} \int_0^t (1-\cos v) dv$. Then
\begin{align*}
A(t) \geq A(0) = 1-\sin (t) 
\end{align*}
(to see this, observe that $A(t)$ is the average of $(1-\cos(v))$ on interval $(0,t)$ and divide the cases $|t| \leq \pi /2$ and $|t| \geq \pi/2$)

So $Ct^{-1} \int_0^t (1- \cos(v)) dv \geq 1$. Substitute $v = uy$, $u=v/y$,
\begin{align*}
Ct^{-1} \int_0^{t/y} (1- \cos(uy)) y du \geq 1
\end{align*}
Put $t/y = 1/\lambda$, $\lambda = y/t$, $t= y/\lambda \geq 1$ to see
\begin{align*}
C\lambda \int_{0}^{1/\lambda} (1- \cos (uy)) du \geq 1
\end{align*}
whenever $t=y/\lambda \geq 1$(this was the assumption we started with). Now for general $y \in \reals$, has
\begin{align*}
C\lambda \int_{0}^{1/\lambda} (1- \cos (uy)) du \geq 1_{|y| \geq \lambda}
\end{align*}
Now integrate with respect to $\mu$ and use Fubini.
\begin{align*}
\mu(\reals \backslash (-\lambda, \lambda)) &\leq C\lambda \int_{\reals} \int_0^{1/\lambda} (1- \cos (uy)) du \mu(dy) \\
&= C\lambda \int_0^{1/\lambda} \int_{\reals} (1- \cos(uy)) du \mu(dy)
\end{align*}

\eop
\end{proof}
\s

\newday

(5th November, Monday)
\s

\thmnum{5.3.2} Let $\mu_n, \mu$ be probability measures on $\reals^d$ with characteristic functions $\phi_n, \phi$. Then the following are equivalent
\begin{itemize}
\item[(a)] $\mu_n \rightarrow \mu$ weakly on $\reals^d$.
\item[(b)] $\phi_n(u) \rightarrow \phi(u)$ for all $u\in \reals^d$.
\end{itemize}
We will prove only for the case $d=1$.
\begin{proof}
\pf It is clear that (a) implies (b). Suppose (b) holds. We prove via a 'compactness argument'. We aim to show that the sequence $(\mu_n)_n$ tight, and therefore has a converging subsequence, and show that the converging point is in fact $\mu$.

\quad Note that $\phi(0) = 1$ and $\phi$ is continuous. Given $\epsilon >0$, there exists $\lambda < \infty$ such that
\begin{align*}
C \lambda \int^{1/\lambda}_0 (1- \text{Re}(\phi(u))) du \leq \epsilon/2
\end{align*}
with $C = (1- \sin(1))^{-1} < \infty$. By dominated convergence,
\begin{align*}
\int_0^{1/\lambda} (1- \text{Re}(\phi_n(u))) du \xrightarrow{n\rightarrow \infty} \int^{1/\lambda}_0 (1- \text{Re}(\phi(u))) du
\end{align*}
so for sufficiently large $n$, by \textbf{Lemma 5.3.1,},
\begin{align*}
\mu_n (\reals \backslash (-\lambda , \lambda)) \leq C\lambda \int^{1/\lambda}_0 (1- \text{Re}(\phi_n(u))) du \leq \epsilon
\end{align*}
Since $\epsilon$ was arbitrary, we see that $(\mu_n : n\in \mathbb{N})$ is tight. By Prohorov's theorem, we have a converging subsequence $\mu_{n_k} \rightarrow \nu$ for some probability measure $\nu$.

\quad Suppose for a contradiction that $\nu \neq \mu$. Therefore, there exists $\epsilon >0$, and $f\in C_b(\reals^n)$ such that
\begin{align*}
|\mu_{n_k} (f) - \mu(f)| \geq \epsilon \quad \forall k
\end{align*}
By above argument, we have $\mu_{n_k} \rightarrow \nu$. But then, since $e^{inx}$ is a bounded continuous function,
\begin{align*}
\int_{\reals} e^{inx} \nu(dx) = \lim_{k\rightarrow \infty} \phi_{n_k}(n) = \phi(n)
\end{align*}
which indicates $\mu = \nu$ by uniqueness of characteristic functions (see PM notes), a contradiction.

\eop
\end{proof}
\s

In fact, the proof of the theorem implies a slightly stronger statement, which is less useful.
\s

\thmnum{5.3.3} \emph{(L\'{e}vy's continuity theorem for characteristic functions)} Let $(\mu_n : n\in \mathbb{N})$ be a sequence of probability measures on $\reals^n$ with characteristic functions $\phi_n$. Suppose $\phi_n (u) \rightarrow \phi(u)$ for all $u$ for some function $\phi$ (not necessarily a characteristic function) such that $\phi$ is continuous at $0$. Then $\phi$ is the characteristic function of some probability measure $\mu$ on $\reals^d$ and $\mu_n \rightarrow \mu$ weakly on $\reals^d$.
\s

\section*{6. Large Deviations}
	
\subsection*{6.1. Cram\'{e}rs theorem}

\thmnum{6.1.1} Let $(X_n : n\in \mathbb{N})$ be a sequence of integrable \emph{i.i.d.} random variables in $\reals$. Set $m = \avg(X_1)$, $S_n = X_1 + \cdots + X_n$. We know $S_n / n \rightarrow \delta_m$ in probability, so if $(m-\epsilon, m+ \epsilon) \cap B =\phi$ then $\prob (S_n/n \in B) \rightarrow 0$ as $n\rightarrow \infty$. Then in fact the convergence rate is given by $\sim \exp (-n \alpha(B))$ for some $\alpha$. To be precise, for all $a\geq m = \avg(X_1)$, as $n\rightarrow \infty$,
\begin{align*}
\frac{1}{n} \log \prob (S_n \geq na) \rightarrow -\psi^*(a)
\end{align*}
where $\psi^*$ is the \emph{Legendre transform} of the \emph{cumulant generating function} $\psi (\lambda) = \log (\avg(e^{\lambda X_1}))$, where Legendre transform is given by
\begin{align*}
\psi^* (x) = \sup_{\lambda \in \reals} \{ \lambda x - \psi(\lambda) \}
\end{align*}
\quad In particular, for $n$ sufficiently large and in case $\psi^{*}(a) < \infty$, we get
\begin{align*}
-\psi^{*}(a) - \epsilon \leq \frac{1}{n} \log (\prob(S_n \geq a)) \leq -\psi^{*}(a) + \epsilon 
\end{align*}
and therefore
\begin{align*}
e^{-n(\psi^*(a)+\epsilon)} \leq \prob (S_n \geq na) \leq e^{-n(\psi^*(a)-\epsilon)} .
\end{align*}
\s

\textbf{Note :} $\psi$ is always a convex function, so $\psi^*$ is also a convex function.
\s

\textbf{Examples :}
\begin{itemize}
\item[(i)] $X_1 \sim N(0,1)$, then $\avg(e^{\lambda X_1}) = e^{\lambda^2 /2}$,  $\psi(\lambda) = \lambda^2 /2$ and $\psi^*(x) = x^2/2$. Hence
\begin{align*}
\frac{1}{n} \log (\prob(S_n \geq a)) \rightarrow -\frac{a^2}{2} \quad \forall a\geq 0
\end{align*}
Can check this directly, using the fact that $S_n \sim N(0,n)$ in this case.
\item[(ii)] $X_1 \sim \text{Exp}(1)$, then
\begin{align*}
\avg(e^{\lambda X_1}) = \int_0^{\infty} e^{\lambda x} e^{-x} dx = \begin{cases}
\begin{array}{ll}
\infty & \text{if } \lambda \geq 1 \\
\frac{1}{1-\lambda} & \text{if } \lambda <1
\end{array}
\end{cases}
\end{align*} 
so $\psi(\lambda) = -\log (1-\lambda)$ if $\lambda <1$ and $\infty$ otherwise, and $\psi^*(x) = x-1-\log(x)$ for $x>0$. Cram\'{e}r's theorem implies that
\begin{align*}
\frac{1}{n} \log \prob(S_n \geq na) \rightarrow -(a-1-\log (a)) \quad \forall a\geq 1
\end{align*}
On the other hand, $\text{Var}(X_1) =1 <\infty$, so $\frac{S_n -n}{\sqrt{n}} \rightarrow N(0,1)$ by CLT. So
\begin{align*}
\prob(S_n \geq n + a\sqrt{n}) \rightarrow \int_a^{\infty} \frac{1}{\sqrt{2\pi}} e^{-x^2/2} dx
\end{align*}
so Cram\'{e}r's theorem gives a result of a different flavour from CLT for distributions with bounded variation : while CLT provides a description for distribution near the average, Cram\'{e}r gives an explanation of tail distribution of $S_n$.
\end{itemize}
\s

\begin{proof}
\textbf{preparation for proof of Cram\'{e}r's theorem) } Let $\mu(B) = \prob(X_1 \in B)$. Exclude the easy case where $\mu = \delta_m$. Define for $\lambda \geq 0$ with $\psi(\lambda) < \infty$, the \textbf{tilted distribution} $\mu_{\lambda}$ by
\begin{align*}
\mu_{\lambda} (dx) \propto e^{\lambda x} \mu(dx)
\end{align*}
For $K \geq m = \avg(X_1)$, define the conditional distribution by
\begin{align*}
\mu_K(dx | x\leq K) \propto 1_{\{x\leq K\}} \mu(dx)
\end{align*}
The CGF(cumulant generating function) of $\mu_K$ is then given by
\begin{align*}
\psi_{K}(\lambda) = \log(\avg(e^{\lambda X_1} | X_1 \leq K))
\end{align*}
\end{proof}

\s

\newday

(7th November, Wednesday)
\s

We now start proving the following theorem.
\s

\thmnum{6.1.1} Let $(X_n : n\in \mathbb{N})$ be a sequence of integrable \emph{i.i.d.} random variables in $\reals$. Set $m = \avg(X_1)$, $S_n = X_1 + \cdots + X_n$. Then for all $a\geq m = \avg(X_1)$, as $n\rightarrow \infty$,
\begin{align*}
\frac{1}{n} \log \prob (S_n \geq na) \rightarrow -\psi^*(a)
\end{align*}
where  $\psi (\lambda) = \log (\avg(e^{\lambda X_1}))$, and $\psi^* (x) = \sup_{\lambda \in \reals} \{ \lambda x - \psi(\lambda) \}$.
\begin{proof}
\pf \emph{(Upper bound)} For all $\lambda \geq 0$ and $n\geq 1$
\begin{align*}
\prob(S_n \geq na) = \prob(e^{\lambda S_n} \geq e^{\lambda na} ) \leq e^{-\lambda na} \avg(e^{\lambda S_n})= e^{-(\lambda a -\psi(\lambda))n}
\end{align*}
so $\frac{1}{n} \log \prob(S_n \geq na) \leq -(\lambda a -\psi(\lambda))$ and 
\begin{align*}
\frac{1}{n} \log \prob (S_n \geq na) \leq -\psi^* (a)
\end{align*}

\emph{(Lower bound)} It remains to show the lower bound. That is, we aim to prove
\begin{align*}
\liminf_{n\rightarrow \infty} \frac{1}{n} \log \prob (S_n \geq na) \geq - \psi^* (a)
\end{align*}
Consider first the case where $\prob(X_1 \leq a) = 1$. Then
\begin{align*}
\avg(e^{\lambda(X_1-a)}) \xrightarrow{\lambda \rightarrow \infty} \prob(X_1 =a)
\end{align*}
Call $p = \prob(X_1 =a)$, so $\lambda a - \psi(\lambda) \rightarrow -\log (p)$. So in particular,
\begin{align*}
\psi^* (a) \geq -\log (p)
\end{align*}
Now $\prob (S_n \geq na) = p^n$ so
\begin{align*}
\frac{1}{n} \log \prob(S_n \geq na) = \log (p) \geq -\psi^*(a)
\end{align*}
hence we can eliminate the case $\prob(X_1 \leq a) = 1$.
\s

\quad Next consider the case $\psi(\lambda) < \infty$ for all $\lambda \geq 0$ and $\prob(X_1 > a) >0$. Fix $\epsilon >0$ and set $b = a+ \epsilon$, $c= a+ 2\epsilon$, choosing $\epsilon$ small enough so $\prob(X_1 > b)>0$. Then there exists $\lambda$ such that $\psi'(\lambda) = b$ - where the differentiability and the existence is justified in the following proposition :

\begin{subproof}
\propnum{6.1.2} Suppose $X$ is integrable and not a.s. constant. Then 
\begin{align*}
& \psi_K(\lambda) = \log \avg(e^{\lambda X_1}|X_1 \leq K) < \infty \quad \forall K < \infty \\
\text{\emph{and}} \quad & \psi_K(\lambda) \nearrow \psi(\lambda) \quad \text{as } K\rightarrow \infty
\end{align*}
Moreover in the case $\psi(\lambda) < \infty$ for all $\lambda \geq 0$, $\psi$ has a continuous derivative on $[0,\infty)$ and is $C^2$ on $(0,\infty)$ with
\begin{align*}
& \psi'(\lambda) = \int_{\reals} x\mu_{\lambda}(dx) \\
& \psi''(\lambda) = \text{Var}(\mu_{\lambda}) > 0
\end{align*}
and $\psi'$ is a homeomorphism from $[0,\infty)$ to $[m, \text{sup}(\text{supp}(\mu))$.

\pf (Exercise)
\end{subproof}

\quad Now we use the idea of tilting the probability measure. Define a new probability measure $\prob_{\lambda}$ by $d\prob_{\lambda} = e^{\lambda S_n -n \psi(\lambda)} d\prob$. Then observe that under $\prob_{\lambda}$ the random variables $X_1, \cdots, X_n$ are independent with distributions $\mu_{\lambda}$ and that $\avg_{\lambda}(X_1) = b$. Consider the event
\begin{align*}
A_n = \{\Big| \frac{S_n}{n} -b \Big| \leq \epsilon \} = \{ (b- \epsilon)n = an \leq S_n \leq (b + \epsilon)n =cn \}
\end{align*}
By the weak law of large numbers, $\prob_{\lambda}(A_n ) \rightarrow 1$. So
\begin{align*}
\prob(S_n \geq na) \geq \prob(A_n) &= \avg_{\lambda} \Big( 1_{A_n} e^{-\lambda S_n + \psi(\lambda)n} \Big) \\
&\geq e^{-\lambda cn + \psi(\lambda) n} \prob_{\lambda}(A_n)
\end{align*}
So 
\begin{align*}
\frac{1}{n} \log \prob (S_n \geq na) \geq -\lambda c + \psi(\lambda) + \frac{\log (\prob_{\lambda} (A_n))}{n}
\end{align*}
and
\begin{align*}
\liminf_{n\rightarrow \infty} \frac{1}{n} \log \prob(S_n \geq na) \geq -(\lambda c - \psi(\lambda)) \geq -\psi^* (c)
\end{align*}
Now $\psi^*$ is continuous at $a$ (recall, $\psi^*$ is a Legendre transform of a convex function so is convex, and therefore continuous. Or, see \textbf{Lemma 6.1.3}) and $\epsilon >0$ is arbitrary so the desired lower bound follows on letting  $\epsilon \rightarrow 0$.
\s

\quad Finally, consider the general case $\prob(X_1 >a) >0$ but allowing $\psi(\lambda) = \infty$ for some $\lambda \geq 0$. For $K>a$, we have $\prob(X_1 >a | X_1 \leq K) >0$ and $\psi_K(\lambda) < \infty$ for all $\lambda \geq 0$. So preceding argument shows
\begin{align*}
\liminf_{n\rightarrow \infty} \frac{1}{n} \log \prob_K (S_n > na) \geq -\psi_K^* (a)
\end{align*}
where $\prob_K$ is the probability measure given by
\begin{align*}
d\prob^{(n)}_K \propto 1_{ \{ X_1 \leq K, \cdots, X_n \leq K \}} d\prob
\end{align*}
(To see this, note, under $\prob_K$, random variables $X_1, \cdots X_n$ are independent with distribution $\mu(\cdot | x\leq K)$). But
\begin{align*}
\prob(S_n \geq na) \geq \prob(S_n \geq na | X_1\leq K, \cdots, X_n \leq K)  = \prob_K(S_n \geq na)
\end{align*}
and $\psi^*_K(a) \searrow \psi^*(a)$ as $K\rightarrow \infty$ (by \textbf{Lemma 6.1.3}) so we see
\begin{align*}
\liminf_{n\rightarrow \infty} \frac{1}{n} \log \prob(S_n \geq na) \geq -\psi^*_K(a) \nearrow -\psi^*(a)
\end{align*}

\eop
\end{proof}
\s

One different way to see that $\psi^*$ is continuous at $a$ is presented in the following lemma.
\s

\lemnum{6.1.3} For all $a\geq m$, with $\prob(X_1 > 0) > 0$ we have $\psi^*_K(a) \searrow \psi^*(a)$ as $K\rightarrow \infty$. Moreover in the case $\psi(\lambda) < \infty$ for all $\lambda \geq 0$, $\psi^*$ is continuous at $a$ and we have $\psi^*(a) =\lambda^* a - \psi(\lambda^*)$ where $\lambda^*$ is uniquely determined by $\psi'(\lambda^*) = a$.
\begin{proof}
\pf Consider first the later case where $\psi(\lambda) < \infty$ for $\lambda \geq 0$. Then by \textbf{Proposition 6.1.2} wee see that
\begin{align*}
\psi^*(a) = \lambda^* a -\psi(\lambda^*)
\end{align*}
where $a= \psi' (\lambda^*)$ and $\psi^*$ is continuous at $a$ with $\lambda^* = (\psi')^{-1}(a)$.
\s

\quad For the first part, note that $\psi_K^*$ is non-increasing in $K$. For $K$ sufficiently large, we have
\begin{align*}
\prob(X_1>a | X_1 \leq K) >0
\end{align*}
and $a\geq m \geq m_K$ (where $m_K = \avg(X_1 |\leq X_1 \leq K)$) and $\psi_K(\lambda) < \infty$ for all $\lambda \geq 0$, so we may apply the preceding argument to $\mu_K$ to see that
\begin{align*}
\psi^*_K(a) = \lambda_K^* a - \psi_K(\lambda_K^*)
\end{align*}
where $\lambda_K^* \geq 0$ is determined by $\psi_K'(\lambda_K^*) = a$. Now $\psi_K'(\lambda)$ is non-decreasing in $K$ and $\lambda$, so $\lambda_K^* \searrow \lambda^*$ for some $\lambda^* \geq 0$. Also $\psi_K'(\lambda) \geq m_K$ for all $\lambda \geq 0$ so
\begin{align*}
\psi_K(\lambda_K^*) \geq \psi_K(\lambda^*) + m_K(\lambda_K^* - \lambda^*)
\end{align*}
Then
\begin{align*}
\psi^*_K(a) = \lambda_K^* a - \psi_K(\lambda^*_K) \leq \lambda^*_K a - \psi_K(\lambda^*) - m_K(\lambda^*_K - \lambda^*) \rightarrow \lambda^* a - \psi(\lambda^*) \leq \psi^*(a)
\end{align*}
So $\psi_K^*(a) \searrow \psi^*(a)$ as $K\rightarrow \infty$ as claimed.

\eop
\end{proof}
\s

\newday

(9th November, Friday)
\s

\section*{7. Borwnian Motion}

\subsection*{7.1. Definition}

\begin{tabular}{|p{0.9\textwidth}|}
\hline\\
Let $(X_t)_{t\geq 0}$ is a random process in $\reals^d$. We say $(X_t)_{t\geq 0}$ is a \textbf{Brownian motion} if :
\begin{itemize}
\item[(i)] For all $s,t\geq 0$, the random variable $X_{s+t}-X_s$ is Gaussian, of mean 0 and variance $tI$ and is independent of $\F_s^X = \sigma(X_r : r\leq s)$
\item[(ii)] for all $\omega \in \Omega$ the map $t\mapsto X_{t} (\omega) : [0,\infty ) \rightarrow \reals^d$ is \emph{continuous}.
\end{itemize}
\\\\\hline
\end{tabular}
\s

Condition (i) means that, for all $s\geq 0$, $t>0$, all Borel sets $B\subset \reals^d$ and all $A \in \F_s^X$,
\begin{align*}
\prob( \{ X_{s+t} - X_s \in B \} \cap A ) = \prob(A) \int_B (2\pi t)^{-\frac{d}{2}} e^{-|y|^2/2t} dy
\end{align*}
\emph{Or}, in terms of conditional expectation, (i) is equivalent to : for all $s,t \geq 0$ and all $f\in C_b (\reals^d)$,
\begin{align*}
\avg (f(X)_{s+t} | \F^X_s) = P_t f(X_x) \quad \text{a.s.} 
\end{align*}
where $P_t$ is the \textbf{heat semigroup}, i.e. 
\begin{align*}
P_0 f(x) = \int_{\reals^d} p(t,x,y)f(y)dy, \quad p(t,x,y) = (2\pi t)^{-\frac{d}{2}}  e^{-|y-x|^2/2t}
\end{align*}
\s

If $X_0 =x$ then we call $(X_t)_{t\geq 0}$ a \textbf{Brownian motion starting from} $x$. In this case, condition (i) is equivalent following property : for all $t_1, \cdots, t_n \geq 0$ with $t_1 < \cdots < t_n$ and all $B \in \borel(\reals^{dn})$
\begin{align*}
\prob( (X_{t_1},\cdots, X_{t_n}) \in \borel  ) = \int_B \prod_{i=1}^n p(s_i, x_{i-1},x_i) dx_i
\end{align*}
where $t_0 =0$, $x_0=x$, $s_i = t_i-t_{i-1}$.
\s

Given independent Brownian motions $(X^1_t)_{t\geq 0},\cdots, (X_t^d)_{t\geq 0}$ in $\reals$ starting from $0$ and given $x=(x^1, \cdots, x^d) \in \reals^d$, the process $(x + (X_t^1, \cdots, X_t^d))_{t\geq 0}$ is a Brownian motion in $\reals^d$ starting from $x$ and we obtain all Brownian motion starting from $x$ in $\reals^d$ in this way.
\s

\subsection*{7.2. Wiener's theorem}

Brownian motion was established as a mathematical object only after 1920's.
\s

Let $W_d = C([0,\infty), \reals^d)$, and $x_t : W_d \rightarrow \reals^d$, $x_t(w) = w(t)$ be the coordinate functions. We may endow $W_d$ with $\sigma$-algebra $\W_d = \sigma(x_t:t\geq 0)$. 
\s

Given a continuous process $(X_t)_{t\geq 0}$ in $\reals^d$ on $\Omega$, we can define 
\begin{align*}
X : \Omega \rightarrow W_d, \quad X(\omega)(t) = X_t(\omega)
\end{align*}
then $X$ is $\W_d$-measurable so $X$ has a law on $(W_d, \W_d)$.
\s

\thmnum{7.2.1.}\emph{(Wiener)} For all $d\geq 1$ and $x\in \reals^d$, there exist a unique probability measure $\mu_x$ on $(W_d, \W_d)$ such that $(x_t)_{t\geq 0}$ is a Brownian motion in $\reals^d$ staring from $x$. In particular, Brownian motion exists.
\begin{proof}
\pf Conditions (i) and (ii) determine the finite dimensional distributions of a Brownian motion and hence determine the law of any BM on $(W_d, \W_d)$ (with given starting point - hence such probability measure is unique.

\quad Suppose we have a measure $\mu_0$ on $(W_1, \W_1)$ such that $(x_t)_{t\geq 0} \sim \text{BM}_0$ in $\reals$. For $x\in \reals$, $(x+x_t)_{t\geq 0} \sim \text{BM}_x$ so could take $\mu_x$ as law of this process. Then for $x=(x_1, \cdots, x_d) \in \reals^d$, the measure $\mu_{x_1} \otimes \cdots \otimes \mu_{x_d}$ has required properties. So we only have to work in 1 dimension, starting at 0.

\quad Define $\mathbb{D}_N = \{k2^{-N}: k\in \mathbb{Z}^+ \}$ and $\mathbb{D} = \bigcup_{N\geq 0} \mathbb{D}_N$. There exists some probability space $(\Omega, \F, \prob)$ and a family $(Y_t : t\in \mathbb{D})$ of independent $N(0,1)$ random variables. First define for $t\in \mathbb{D}_0 = \mathbb{Z}^+$,
\begin{align*}
\xi_t = Y_1 + \cdots + Y_t
\end{align*}
Then $(\xi_n)_{n \in \mathbb{D}_0}$ is Gaussian and $(\xi_{t+1} - \xi_t : t\in \mathbb{D}_0)$ are independent and has distribution $\sim N(0,1)$. We define recursively $(\xi_t)_{t\in \mathsf{D}_N}$ as follows for $t\in \mathbb{D}_{N+1} \backslash \mathbb{D}_N$ :
\begin{subproof}
: set $r = t-2^{-N-1}$, $s=t+2^{-N-1} \in \mathbb{D_N}$, set $Z_t = 2^{- \frac{N+2}{2}} Y_t$ and define $\xi_t = \frac{\xi_r + \xi_s}{2} + Z_t$.
\end{subproof}
We will show by induction that for all $N\geq 0$, $(\xi_{t+2^{-N}} - \xi_t: t\in \mathbb{D}_N)$ are independent, $\sim N(0,2^{-N})$ random variables
\begin{subproof}
: Suppose true for $N$. Take $t\in \mathbb{D}_{N+1} - \mathbb{D}_N$ and $r,s$ as above. Then
\begin{align*}
& \xi_t - \xi_r = \frac{\xi_s - \xi_r}{2} + Z_t, \quad \xi_s - \xi_t = \frac{\xi_s - \xi_r}{2} - Z_t \\
& \text{Var}\Big( \frac{\xi_s-\xi_r}{2}\Big) = \frac{1}{4} 2^{-N}, \quad \text{Var}(Z_t) = 2^{-N-2}
\end{align*}
so
\begin{align*}
& \text{Var}(\xi_t - \xi_r) = \frac{1}{4} 2^{-N} + 2^{-N-2} = 2^{-N-1} = \text{Var} (\xi_s -\xi_r) \\
& \text{cov}(\xi_t -\xi_r, \xi_s - \xi_t) =0
\end{align*}
Also for any interval $(u,v]$ disjoint from $(r,s]$ with $u,v\in \mathbb{D}_{N+1}$,
\begin{align*}
\text{cov}(\xi_s - \xi_r, \xi_v - \xi_u) = \text{cov}(\xi_s - \xi_t, \xi_v- \xi_u) =0
\end{align*}
So the induction proceeds.
\end{subproof}
\end{proof}
\s

\newday

(12th November, Monday)
\s

\begin{proof}
\textbf{proof of Wiener's theorem continues)} We constructed $(\xi_t : t\in \mathbb{D})$ such that for all $s,t\geq 0$, $\xi_{s+t} -\xi_s \sim N(0,t)$ and is independent of $\sigma(\xi_r : r\leq s , r\in \mathbb{D})$. Choose $p>2$ and set $C_p = \avg(|\xi_1|^p)$. Then $C_p <\infty$ and $\avg(|\xi_t - \xi_s|^p) = C_p|t-s|^{p/2}$ so by Kolmogorov's lemma(\textbf{Theorem 4.2.1}), there exists a continuous process $(X_t)_{t\geq 0}$ such that $X_t = \xi_t$ a.s. for all $t\in \mathbb{D}$.
\s

For $s\geq 0$, $t>0$ and for any $A\in \F_s^X$ there exist sequence $(s_n)$ in $[0,\infty)$, $(t_n)$ in $(0,\infty)$ and $A_0 \in \sigma(\xi_r : r\leq s, r \in \mathbb{D} )$. s.t. $s_n \rightarrow s$, $t_n \rightarrow t$ as $n\rightarrow \infty$ and $1_A =1_{A_0}$ a.s. (to check this, consider $\{ A \in \F_s : $ this holds $\}$). Then for any $f\in C_b(\reals)$,
\begin{align*}
\avg(f (X_{s_n + t_n} - X_{s_n})1_A) &= \avg( f(\xi_{s_n + t_n} - \xi_{s_n})1_{A_0} ) \\
&= \prob(A_0) \int_{\reals} p(t_0, 0,y) f(y)dy
\end{align*}
so letting $n\rightarrow \infty$ using bounded convergence theorem gives
\begin{align*}
\avg(f(X_{t+s}-X_s)1_{A}) = \prob(A) \int_{\reals} p(t,0,y) f(y)dy
\end{align*}
so $X$ is a Brownian motion as required.

\eop
\end{proof}

\subsection*{7.3. Symmetries of Brownian Motion}

\propnum{7.3.1} Let $(X_t)_{t\geq 0}$ be a $\text{BM}_0(\reals^d)$ and let $\sigma \in (0,\infty)$ and $U\in O(d)$. Then the following processes are also $\text{BM}_0(\reals^d)$.
\begin{itemize}
\item[(i)] \textbf{(Scaling property)} $(\sigma X_{\sigma^{-2}t})_{t\geq 0}$,
\item[(ii)] \textbf{(Rotation invariance)} $(UX_t)_{t\geq 0}$.
\end{itemize}
In fact $\text{BM}_0(\reals^d)$ is characterized among continuous Gaussian processes by its means and covariances,
\begin{align*}
\avg(X_t) =0, \quad \text{cov}(X_s^i, X_t^j) = \avg(X^i_s X^j_t) = \delta_{ij} (s\wedge t)
\end{align*}
\begin{proof}
\pf Exercise.
\end{proof}

\subsection*{7.4. Brownian Motion in a Given Filtration}

Suppose given a filtration $(\F_t)_{t\geq 0}$ on $(\Omega, \F, \prob)$.
\begin{itemize}
\item We say that $(X_t)_{t\geq 0}$ is a \textbf{$(\F_t)_{t\geq 0}$-BM} if
\begin{itemize}
\item[(a)] $X_t$ is $\F_t$-measurable,
\item[(b)] for all $s,t \geq 0$, the random variable $X_{s+t} - X_s \sim N(0, tI)$ and is independent of $\F_s$,
\item[(c)] for all $\omega \in \Omega$, the map $t\mapsto \Big( X_t(\omega) : [0,\infty ) \rightarrow \reals^d \Big)$ is continuous.
\end{itemize}
This implies that $X$ is a BM in the old sense and any process $X$ which is a BM in the old sense is an $(\F_t^X)_{t\geq 0}$-BM.
\end{itemize}

\propnum{7.4.1} Let $X = (X_t)_{t\geq 0}$ be a $\text{BM}(\reals^d)$ and let $F$ be a bounded measurable function on $W_d$. Define
\begin{align*}
f(x) = \int F(\omega) \mu_x (d\omega), \quad x\in \reals^d
\end{align*}
Then $f$ is measurable on $\reals^d$ and $\avg(F(X) |\F_0)  = f(X_0)$ a.s. (recall, $\mu_x$ is the law of $\text{BM}_x$)
\begin{proof}
\pf (Was an exercise.) First consider $F$ of form $1_{A^{(a,\eta)}}$ where $A^{(a,\underline{\eta)} } = \{ g \in W_d : g(a) \geq (\eta_1, \cdots, \eta_d ) \} \subset W_d$. Then
\begin{align*}
\int A^{(a,\eta)}(\omega) \mu_x( d\omega) = \prob ( B^{(x)} (a) \geq \eta) = \int_{\eta_1}^{\infty} \cdots \int_{\eta_d}^{\infty} \frac{1}{(2\pi)^{d/2}}e^{-y^2 /2} d^d y
\end{align*}
where $B^{(x)}$ is a Brownian motion starting at $x$, so this is measurable. Also, we easily see that any finite intersection of sets $A^{(a,\eta)}$ if of form
\begin{align*}
B^{(a_1, \cdots, a_m)}_{(\eta_1, \cdots, \eta_m)} = \{ g \in W_d : g(a_1) \geq \eta_1, \cdots, g(a_1 + \cdots + a_m) \geq \eta_m \}
\end{align*}
with $a_1, \cdots a_m \geq 0$ so
\begin{align*}
\int B^{(a_1, \cdots, a_m)}_{(\eta_1, \cdots, \eta_m)} \mu_x( d\omega) =  \int A^{(a_1,\eta_1)}(\omega) \mu_x( d\omega) \prod_{k=2}^m \int A^{(a_k,\eta_{k} - \eta_{k-1})}(\omega) \mu_0( d\omega)
\end{align*}
is measurable. We also see that $\avg(F(X) | \F_0 ) = f(X_0)$ a.s. for each case $F = 1_{A^{(a,\eta)}}$ or $F = 1_{B^{(a_1, \cdots, a_m)_{(\eta_1, \cdots, \eta_m)}}}$.

\quad Now notice that the sets $(B^{(a_1, \cdots, a_m)}_{(\eta_1, \cdots, \eta_m)} : m\geq 1, a_j \geq 0, \eta_j \in \reals)$ forms a $\pi$-system generating the $\sigma$-algebra $\mathscr{W}_d$. Since any limit of measurable functions is measurable, and $\avg(F_n(X) | \F_0 ) = f_n(X_0)$ $\forall n$, $F_n \nearrow F$ implies $\avg(F(X) | \F_0 ) = f(X_0)$, we see from monotone class theorem that the statement also holds for general bounded measurable functions $F$.

\eop
\end{proof}
\s

\subsection*{7.5. Martingales of BM}

\thmnum{7.5.1} Let $(X_t)_{t\geq 0}$ be an $(\F_t)_{t\geq 0}$-BM in $\reals^d$ and let $f\in C_b^2(\reals^d)$ . Define
\begin{align*}
M_t = f(X_t) - f(X_0) - \int_0^t \frac{1}{2} \Delta f(X_s) ds
\end{align*}
Then $(M_t)_{t\geq 0}$ is an $(\F_t)_{t\geq 0}$-martingale.
\s

There are two ways for doing this. One is to use stochastic calculus, and the other is to do Markovian-way(which would be made clear soon). You will see the first method in Stochastic Calculus course next term, and we are going to prove this in the second way.
\begin{proof}
\pf It is clear that $(M_t)_{t\geq 0}$ is adapted and integrable. Consider for now the case $X_0 =x \in \reals^d$. Set
\begin{align*}
m(x) = \avg(M_t) = \int_{W_d} \Big[ f(\omega(t)) - f(\omega(0)) -\int_0^t \frac{1}{2} \Delta f(\omega(s)) ds \Big] \mu_x(d\omega) 
\end{align*}
Fix $s\in (0,t]$. We have
\begin{align*}
M_t - M_s = f(X_t) - f(X_s) - \int_s^t \frac{1}{2} \Delta f(X_r) dr
\end{align*}
so using Fubini,
\begin{align*}
\avg(M_t - M_s) = \int_{\reals^d} p(t,x,y)f(y)dy - \int p(s,x,y) f(y)dy - \int_s^t \int_{\reals^d} p(r,x,y) \times \frac{1}{2}\Delta f(y) dy
\end{align*}
But since $p$ is a heat kernel, has $\frac{d}{dt}p = \frac{1}{2} \Delta p$ so
\begin{align*}
\int_{\reals^d} p(r,x,y) \times \frac{1}{2} \Delta f(y) dy &= \int_{\reals} \frac{1}{2} \Delta_y p(r,x,y) f(y) dy \\
&= \int_{\reals} \frac{d}{dt} p(r,x,y) f(y) dy
\end{align*}
and
\begin{align*}
\int_s^t \int_{\reals^d} p(r,x,y) \times \frac{1}{2} \Delta f(y) dy dr &= \int_s^t \int_{\reals} \frac{d}{dt} p(r,x,y) f(y) dy dr \\
&= \int_{\reals^d} (p(t,x,y) - p(s,x,y) ) dy
\end{align*}
so $\avg(M_t -M_s) =0$. But $M_s \rightarrow 0$ a.s. as $s\rightarrow 0$ so by bounded convergence, $\avg(M_s) \rightarrow 0$. Hence $m(x) = \avg(M_t) =0$ for all $x$.

\quad Return to the case of general $X_0$. By \textbf{Prop 7.4.1},
\begin{align*}
\avg(M_t | \F_0) = m(X_0) =0 \quad \text{a.s.}
\end{align*}
\s

Now for any $s,t\geq 0$,
\begin{align*}
M_{s+t} - M_s = f(X_{s+t}) - f(X_s) - \frac{1}{2}\int_0^s \Delta f(X_{s+r}) dr
\end{align*}
and $(X_{s+t})_{t\geq 0}$ is an $(\F_{s+t})_{t\geq 0}$-BM. So we see that 
\begin{align*}
\avg(M_{s+t} - M_s | \F_s)  =0\quad \text{a.s.}
\end{align*}
as required.

\eop
\end{proof}

We are in fact apply the theorem in the case where the assumption $f \in C_b^2(\reals^d)$ does not hold. We can actually relax this condition using almost the same proof, e.g. any $f$ of polynomial growth rate would work.
\s

\textbf{Exercise :} see how much we can relax $C_b^2(\reals^d)$.
\begin{proof}
-Every step other than the integration by parts $\int_{\reals^d} p(r,x,y) \times \frac{1}{2} \Delta f(y) dy = \int_{\reals} \frac{1}{2} \Delta_y p(r,x,y) f(y) dy$ works without assumption that $f\in C_b^2(\reals^d)$. Hence, it is enough to have $f \in W^{2,1}(\reals^d)$(the Sobolev space) with sub-exponential growth rate.
\end{proof}
\s

\newday

(14th November, Wednesday)
\s

\textbf{Examples : } Let $(X_t^i)_{t\geq 0}$, $i=1, \cdots, n$ be a BM.
\begin{itemize}
\item $f(x) = x^i$ then $\Delta f =0$, so is a martingale.
\item $f(x) = x^i x^j$ then $\Delta f=0$, so is a martingale.
\item $(X_t^i X_t^j - \delta_j t)_{t\geq 0}$ is a martingale.
\item $\exp(\lambda X_t - \frac{1}{2} \lambda^2 t)_{t\geq 0}$ is a martingale.
\item For all harmonic functions $f$ bounded, $(f(X_t))_{t\geq 0}$ is a martingale.
\end{itemize}

\subsection*{7.6. Strong Markov Property}

Strong Markov Property of Brownian Motion would be particularly useful.
\s

\thmnum{7.6.1} Let $(X_t)_{t\geq 0}$ be an $(\F_t)_{t\geq 0}$-BM and let $T$ be a stopping time. Then conditioned on $\{T<\infty \}$, $(X_{T+t})_{t\geq 0}$ is an $(\F_{T+t})_{t\geq 0}$-BM.
\begin{proof}
\pf Clearly on $\{T<\infty \}$, $(X_{T<t})_{t\geq 0}$ is continuous on $[0,\infty)$. Also on $\{T<\infty \}$, $X_{T+t}$ is $\F_{T+t}$-measurable. Fix $s\geq 0$, $t>0$, $f\in C_b(\reals^d)$, $m,n\in \mathbb{N}$, $A\in \F_{T+s}$ with $A\subset \{T\leq m\}$. For $k=0,1,\cdots 2^{n}m$, set $t_k = k2^{-n}$ and $A_k = A \cap \{t_k -2^{-n}< T \leq t_k \}$. Note $A_k \in \F_{t_k +s}$ and $A_k = A \cap \{T_n = t_k \}$ where $T_n = 2^{-n} \lceil 2^n T \rceil$, so
\begin{align*}
& \avg(f(X_{T_n + s+t}) 1_{A_k}) = \avg(f(X_{t_k +s +t}) 1_{A_k}) \\
=& \avg(P_t f(X_{t_k+s}) 1_{A_k}) = \avg(P_t f(X_{T_n +s}) 1_{A_k})
\end{align*}
Now sum over $k$ and let $n\rightarrow \infty $ using bounded convergence to obtain
\begin{align*}
\avg(f(X_{T+s+t})1_A) = \avg( P_t f(X_{T+s}) 1_A)
\end{align*}
But $m$ and $A$ were arbitrary so this implies, on $\{ T< \infty \}$,
\begin{align*}
\avg (f(X_{T+t+s}) |\F_{T+s}) = P_t f(X_{T+s}) \quad \text{a.s.}
\end{align*}
as required.

\eop
\end{proof}

\subsection*{7.7. Properties of 1-d BM}

\propnum{7.7.1} Let $(X_t)_{t\geq 0}$ be a $\text{BM}_0(\reals)$. Set $T_a = \inf \{t\geq 0 : X_t =a \}$. Then
\begin{align*}
& \prob(T_a < \infty) =1 \quad \text{for all } a\in \reals \quad \text{\emph{and}} \\
& \prob(T_{-a} \leq T_b) = \frac{b}{a+b} \quad \forall a,b\geq 0 \quad \text{\emph{and}} \\
& \avg(T_a \wedge T_b) = ab
\end{align*}
Moreover $T_a$ has a density $f_a$ on $[0,\infty)$ given by
\begin{align*}
f_a(t) = \frac{1}{\sqrt{2\pi t^3}} e^{-a^2/2t} \quad t\geq 0
\end{align*}
Moreover the following holds almost surely.
\begin{itemize}
\item[(a)] $X_{t} /t \rightarrow 0$ as $t\rightarrow \infty$.
\item[(b)] $\inf_{t\geq 0} X_t = -\infty$, $\sup_{t\geq 0} X_t =\infty$. 
\item[(c)] for all $s\geq 0$, there exist $t,n\geq s$ such that $X_t < 0 < X_n$.
\item[(d)] for all $s>0$ there exist $t,n \in [0,s)$ such that $X_t < 0<X_n$.
\end{itemize}
\begin{proof}
\pf Exercise.
\end{proof}
\s

\thmnum{7.7.2} Let $X \sim \text{BM}_0(\reals)$. Then the following properties hold almost surely : 
\begin{itemize}
\item[(a)] for all $\alpha < 1/2$, $(X_t)_{t\geq 0}$ is locally H\"{o}lder continuous of exponent $\alpha$.
\item[(b)] for all $\alpha >1/2$ there is no non-trivial interval on which $X$ is H\"{o}lder continuous of exponent $\alpha$.
\end{itemize}
\begin{proof}
\pf \begin{itemize}
\item[(a)] For $s,t\geq 0$ with $s<t$, $X_t - X_s \sim (t-s)^{1/2} X_1$ so for all $p< \infty$,
\begin{align*}
\avg( |X_t-X_s|^p ) = (t-s)^{p/2} C_p
\end{align*}
where $C_p = \avg( |X_1|^p ) < \infty$. Given $\alpha < 1/2$ we can find $p<\infty$ so that $\alpha + \frac{1}{p} < \frac{1}{2}$. Then by Kolmogorov's lemma there exist $K\in L^p(\prob)$ such that
\begin{align*}
|X_t - X_s| \leq K|t-s|^{\alpha} \quad \text{for all } s,t\in [0,1]
\end{align*}
So by scaling of BM, $(X_t)_{t\geq 0}$ is locally H\"{o}lder continuous of exponent $\alpha$ a.s.

\quad Finally consider $\alpha_n \nearrow 1/2$, $\alpha_n < 1/2$ to see $(X_t)_{t\geq 0}$ is H\"{o}lder continuous of exponent $\alpha$ a.s. for all $\alpha <1/2$.
\item[(b)] Let $m,n\in \mathbb{N}$ with $m\geq n$. For $s,t\in \mathbb{D}_n$, define
\begin{align*}
[X]^m_{s,t} = \sum_{\tau} (X_{\tau + 2^{-m}} -X_{\tau})^2
\end{align*}
where we sum over all $\tau \in \mathbb{D}_n$ with $\tau \in (s,t]$. The random variables $(X_{\tau + 2^{-m}} - X_{\tau})^2$ are independent of mean $2^{-m}$ and variance $2^{-2m +1}$ (Here we need $\avg(X_1^4)=3$, $\text{Var}(X_1^2)=2$). So $\avg([X]_{s,t}^m) = 2^m (t-s) 2^{-m} =t-s$ and $\text{Var}([X]_{s,t}) = 2^m (t-s)2^{-2m+1}$. So $[X]_{s,t}^m \rightarrow t-s$ a.s.

\quad Suppose $X$ is H\"{o}lder $\alpha$ on $[s,t]$ then for some constant $K$, 
\begin{align*}
|X_{\tau + 2^{-m}} - X_{\tau}|^2 \leq K^2 (2^{-m})^{2\alpha}
\end{align*}
so $[X]^{m}_{s,t} \leq 2^m (t-s) K^2 2^{-2m\alpha} \rightarrow 0$ as $m\rightarrow \infty$. So $(X_t)_{t\geq 0}$ is not H\"{o}lder $\alpha$ on $[s,t]$, which contradicts with our earlier result.
\end{itemize}

\eop
\end{proof}
\s


\end{document}