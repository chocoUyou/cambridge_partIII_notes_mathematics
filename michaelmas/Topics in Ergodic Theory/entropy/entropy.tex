\documentclass[12pt,a4paper]{report}


\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{calrsfs}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[mathscr]{euscript}

%%%for drawing commutative diagrams.%%%%%%
\usepackage{tikz-cd}  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%for writing large parallel%%%%%%
\usepackage{mathtools}
\DeclarePairedDelimiter\bignorm{\lVert}{\rVert}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%for changing margin
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 

\newenvironment{proof}
{\begin{changemargin}{1cm}{0.5cm} 
	}%your text here
	{\end{changemargin}
}

\newenvironment{subproof}
{\begin{changemargin}{0.5cm}{0.5cm}
	}%your text here
	{\end{changemargin}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\newcommand{\thm}{\textbf{Theorem) }}
\newcommand{\thmnum}[1]{\textbf{Theorem #1) }}
\newcommand{\defi}{\textbf{Definition) }}
\newcommand{\lem}{\textbf{Lemma) }}
\newcommand{\lemnum}[1]{\textbf{Lemma #1) }}
\newcommand{\prop}{\textbf{Proposition) }}
\newcommand{\pf}{\textbf{proof) }}
\newcommand{\cor}{\textbf{Corollary) }}
\newcommand{\cornum}[1]{\textbf{Corollary #1) }}

\newcommand{\lap}{\triangle} %%Laplacian
\newcommand{\s}{\vspace{10pt}}
\newcommand{\bull}{$\bullet$}
\newcommand{\sta}{$\star$}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\eop}{\hfill  \textsl{(End of proof)} $\square$} %end of proof

\newcommand{\intN}{\mathbb{Z}_N}
\newcommand{\norms}[2]{\bignorm[\big]{#1}_{#2}}
\newcommand{\abs}[1]{\big| #1 \big|}
\newcommand{\avg}{\mathbb{E}}
\newcommand{\borel}{\mathscr{B}}
\newcommand{\setlimsup}[2]{\bigcap_{#1=1}^{\infty}\bigcup_{#2=#1}^{\infty}}
\newcommand{\dlim}{D\text{-}\lim}
\newcommand{\clim}{C\text{-}\lim}

\newcommand{\ndiv}{\hspace{-5pt}\not|\hspace{3pt}}

\newcommand{\newday}{======================================================================}
\newcommand{\digression}{**********************************************************************************************}

\setlength\parindent{0pt}
\noindent

\chapter*{Topics in Ergodic Theory}
\s

\section*{10. Entropy}

\textbf{Bernoulli Shift : } Let $(P_1, P_2,\cdots, P_d)$ be a probability vector. The \textbf{$(P_1, \cdots, P_d)$-Bernoulli shift} is the MPS
\begin{align*}
\big( \{1,\cdots, d\}^{\mathbb{Z}}, \borel, \mu_{P_1,\cdots, P_d},\sigma \big)
\end{align*}
where $\mu_{P_1,\cdots, P_d}$ is the product measure with $(P_1,\cdots, P_d)$ on the coordinates and $\sigma$ is the shift map.
\s

\defi The MPS $(X_1, \borel_1, \mu_1, T_1)$ and $(X_2,\borel_2, \mu_2, T_2)$ are called \textbf{isomorphic}, if there are maps
\begin{align*}
S_1 : X_1\rightarrow X_2,\quad S_2 : X_2 \rightarrow X_1
\end{align*}
such that $S_1 \circ S_2 = id_{X_2}$ a.e., $S_2 \circ S_1 = id_{X_1}$ a.e., $(S_1)_* \mu_1 =\mu_2$, $(S_2)_* \mu_2 =\mu_1$ and $T_1 \circ S_2 = T_1 \circ T_2$ a.e.
\s

\textbf{The Question :} Are the $(\frac{1}{2}, \frac{1}{2})$ and $(\frac{1}{3},\frac{1}{3},\frac{1}{3})$ Bernoulli-shift isomorphic?
\s

This question does not seem very difficult, but this had been unsolved for a long time. These two shifts have "the same" Koopman operators, and moreover Meshalkin proved that $(\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4})$ and $(\frac{1}{2},\frac{1}{8},\frac{1}{8},\frac{1}{8},\frac{1}{8})$-Bernoulli shifts are isomorphic. This problem was finally solved by Kolmogorov. He proved that these two systems are not isomorphic by attaching a quantity called \emph{entropy}, which should be preserved by isomorphism, on each system and by showing they are not equal. Later, Ornstein showed that two Bernoulli shifts are isomorphic if and only if they have the same entropy. Actually, the introduction of notion of entropy in measure preserving systems was the starting point of ergodic theory being identified as an independent subject, so the importance of entropy in the field of ergodic theory cannot be overemphasize.
\s

Let us do some actual mathematics now. We define the entropy as the measure of the amount how difficult it is to predict the system.
\s

\defi Let $(X,\borel, \mu)$ be a probability space. A \textbf{countable measurable partition} is a collection of measurable sets $A_1, A_2, \cdots$ such that $A_i \cap A_j =\phi$ for all $i\neq j$ and $\bigcup A_i = X$. The sets $A_i$ are called the atoms of partition. 

\quad The \textbf{join} or \textbf{coarsest common refinement} of two countable measurable partiotion $\xi, \eta$ is
\begin{align*}
\xi \vee \eta = \{A \cap B : A\in \xi, B \in \eta \}
\end{align*}

\quad Define a function
\begin{align*}
H(p_1, \cdots, p_d) = - \sum_{j=1}^d p_j \log (p_j)
\end{align*}
for all probability vector $(p_1, \cdots, p_d)$ with the convention $0 \cdot \log 0 =0$. The \textbf{entropy} of a countable measurable partition $\xi$ is
\begin{align*}
H_{\mu}(\xi) = H(\mu(A_1), \mu(A_2), \cdots)
\end{align*}
where $\xi = \{A_1, A_2, \cdots \}$.

\quad The \textbf{conditional entropy} of $\xi = \{A_1, A_2, \cdots \}$ relative to $\eta = \{B_1, B_2, \cdots \}$ is 
\begin{align*}
H_{\mu} (\xi | \eta) = \sum_{n=1}^{\infty} \mu(B_n) \cdot H\Big( \frac{\mu(A_1 \cap B_n)}{\mu(B_n)},\frac{\mu(A_2 \cap B_n)}{\mu(B_n)},\cdots \Big)
\end{align*}
This is the average average of entropy conditioned on each partition of 
\s

$\star$ Very Useful Interpretations : 

\quad Entropy of $\xi$ provides the amount of information we can obtain from an experiment $\xi$. Conditional entropy of $\xi$ relative to $\eta$ provides the amount of information we can get from $\xi$ given the information about experiment $\eta$. Entropy of join $\xi \vee \eta$ gives the amount of information we can get if we perform both experiments $\xi$ and $\eta$.
\s

\lem
\begin{itemize}
\item[(1)] $H_{\mu} (\xi) \geq 0$.
\item[(2)] The value of $H_{\mu}(\xi)$ is maximal among partition $\xi$ with $k$ atoms if all atoms have the same measure $\frac{1}{k}$.
\item[(3)] $H_{\mu}(\{A_1, \cdots, A_k \}) = H_{\mu}(A_{\rho (1)}, \cdots, A_{\rho (k)})$ for all permutations $\rho \in \text{Sym}(\{1,\cdots,k\})$.
\item[(4)] $H_{\mu} (\xi \vee \eta) = H_{\mu}(\xi) + H_{\mu}(\eta |\xi)$. This is called \emph{chain rule.}
\end{itemize}
\begin{proof}
\pf \begin{itemize}
\item (1) is trivial and (2) is going to be proved shortly using Jensen's inequality.
\item (3) and (4) are going to be proved later, in more general setting.
\end{itemize}
\end{proof}
\s

\textbf{Khinchin)} Let $H_{\cdot}(\cdot) : P(X) \times \borel$ be a function satisfying the conditions (1)-(4) of the lemma, where $P(X)$ is the set of Borel probability measures on $(X,\borel)$. Then $H_{\cdot}(\cdot)$ is uniquely determined by these properties, up to a multiplication of a scalar factor.
\s

\newday
\s

(6th November, Tuesday)

\defi A function $[a,b] \rightarrow \reals \cup \{ \infty \}$ is \textbf{convex}, if $\forall x \in (a,b)$, $\exists \alpha_x \in \reals$ such that
\begin{align*}
f(y) \geq f(x) + \alpha_x (y-x) \quad \forall y \in [a,b]
\end{align*}
$f$ is \textbf{strictly convex} if the equality occurs only for $x=y$. 
\s

\textbf{Remark :} If $f$ is $C^2([a,b])$ and $f''(x) >0$ for all $x\in (a,b)$, then $f$ is strictly convex.
\s

\textbf{Jensen's inequality)} Let $f:[a,b] \rightarrow \reals \cup \{\infty\}$ be a convex function. Let $p_1, p_2, \cdots$ be a probability vector (possibly countably infinite). Let $x_1, x_2, \cdots \in [a,b]$. Then
\begin{align*}
f(p_1 x_1 + p_2 x_2 + \cdots) \leq \sum_i p_i f(x_i)
\end{align*}
If $f$ is strictly convex, then equality occurs \emph{iff} those $x_i$ for which $p_i >0$ coincide.
\s

\textbf{Claim :} Let $(X,\borel, \mu)$ be a probability space. Let $\xi$ be a measurable partition with $k$ atoms. Then
\begin{align*}
H_{\mu}(\xi) \leq \log (k)
\end{align*}
and equality occurs only if each atom of $\xi$ has measures $\frac{1}{k}$.
\begin{proof}
\pf Apply Jensen's inequality to the function $x\mapsto x \log (x)$ with weights $p_i = \frac{1}{k}$ at the point $\mu(A_i)$, where $A_i$ are the atoms of $\xi$. Note,
\begin{align*}
& \sum p_i \mu(A_i) = \frac{1}{k} \sum \mu(A_i) = \frac{1}{k} \\
\Rightarrow \quad & \frac{1}{k} \log (\frac{1}{k}) \leq \sum \frac{1}{k} \mu(A_i) \log (\mu(A_i))
\end{align*}
so
\begin{align*}
\log k \geq \sum (-1) \mu(A_i) \log \mu(A_i) = H_{\mu}(\xi)
\end{align*}

\eop
\end{proof}

\s

\defi Let $(X, \borel, \mu)$ be a probability space and let $\xi$ be a countable measurable partition. The \textbf{information function} of $\xi$ is
\begin{align*}
I_{\mu} (\xi) : & X \rightarrow \reals \cup \{\infty \} \\
& x\mapsto -\log \mu \big( [x]_{\xi} \big)
\end{align*}
where $[x]_{\xi}$ is the atom of $\xi$ where $x$ belongs.

\quad If $\eta$ is another partition, then the \textbf{conditional information function} of $\xi$ relative to $x$ is
\begin{align*}
I_{\mu} (\xi | \eta) (x)  = - \log \frac{\mu \big( [x]_{\xi \vee \eta} \big)}{\mu \big( [x]_{\eta} \big)}
\end{align*}
\s

It is apparent that the information function is related to entropy. This is summarized in the following lemma.
\s

\lem With notation as above,
\begin{align*}
& H_{\mu}(\xi) = \int I_{\mu} (\xi) d \mu \\
& H_{\mu}(\xi | \eta) = \int I_{\mu} (\xi | \eta) d \mu
\end{align*}
\begin{proof}
\pf The first equality is direct from the definition. For the second equality,
\begin{align*}
I_{\mu} (\xi |\eta) d\mu &= \sum_{A\in \xi, B \in \eta} \int_{A \cap B} I_{\mu}(\xi | \eta) d\mu = - \sum_{A \in \xi, B\in \eta} \mu(A\cap B) \log \Big( \frac{\mu(A \cap B)}{\mu(B)}\Big) \\
&= -\sum_{B\in \eta} \mu(B) \cdot \sum_{A\in \xi} \frac{\mu(A\cap B)}{\mu(B)} \log \Big( \frac{\mu(A\cap B)}{\mu(B)} \Big)
\end{align*}

\eop
\end{proof}
\s

One reason we use information function is that it is much easier to prove chain rule with information function.
\s

\lem \emph{(Chain rule)} Let $(X,\borel, \mu)$ be a probability space and let $\xi, \eta, \lambda$ be countable measurable partitions. Then
\begin{align*}
I_{\mu} (\xi \vee \eta | \lambda)(x) &= I_{\mu}(\xi | \lambda) (x) + I_{\mu}(\eta | \xi \vee \lambda)(x) \quad \forall x \in X \\
H_{\mu} (\xi \vee \eta | \lambda) &= H_{\mu}(\xi | \lambda)  + H_{\mu}(\eta | \xi \vee \lambda)
\end{align*}
\begin{proof}
\pf For the first equality,
\begin{align*}
I_{\mu} (\xi \vee \eta |\lambda) (x)  &= \log \frac{\mu ([x]_{\lambda})}{\mu([x]_{\xi \vee \eta \vee \lambda})} \\
I_{\mu} (\xi |\lambda) (x)  &= \log \frac{\mu ([x]_{\lambda})}{\mu([x]_{\xi \vee \lambda})} \\
I_{\mu} (\eta |\lambda \vee \xi) (x)  &= \log \frac{\mu ([x]_{\xi \vee \lambda})}{\mu([x]_{\xi \vee \eta \vee \lambda})} \\
\end{align*}
and this proves the chain rule for information function.

\quad The second equality follows from the first equality by integration the information function (as in the previous lemma).

\eop
\end{proof}
\s

The following inequality is very important in theory of mathematics of information.
\s

\lem Let notation be as above. Then
\begin{align*}
H_{\mu}(\xi | \eta) \geq H_{\mu}(\xi | \eta \vee \lambda)
\end{align*}

"The amount of information obtained from $\xi$ given $\eta$ is larger than information obtained from $\xi$ given $\eta$ and $\lambda$."
\begin{proof}
\pf 
\begin{align*}
H_{\mu}(\xi | \eta \vee \lambda) &= \sum_{A \in \xi, B \in \eta, C \in \lambda} \mu(A\cap B \cap C) \log \Big( \frac{\mu(B\cap C)}{\mu(A\cap B \cap C)}\Big) \\
H_{\mu}(\xi | \eta ) &= \sum_{A \in \xi, B \in \eta} \mu(A\cap B ) \log \Big( \frac{\mu(B)}{\mu(A\cap B )}\Big)
\end{align*}
It is enough to show that for all fixed $A \in \xi$, $B\in \eta$, we have
\begin{align*}
\mu(A\cap B) \log \Big( \frac{\mu(B)}{\mu(A\cap B)}\Big) \geq \sum_{C \in \lambda} \mu(A \cap B \cap C) \log \Big( \frac{\mu(B\cap C)}{\mu(A\cap B \cap C)} \Big)
\end{align*}
To see this, apply Jensen's inequality for $x\mapsto x\log x$ at points $\frac{\mu(A\cap B\cap C)}{\mu(B \cap C)}$ for $C \in \lambda$ with weights $\frac{\mu(B\cap C)}{\mu(B)}$. Write
\begin{align*}
\sum_{C\in \lambda } \frac{\mu(B\cap C)}{\mu(B)} \cdot \frac{\mu(A\cap B \cap C)}{\mu(B\cap C)} = \frac{1}{\mu(B)} \sum_{C\in \lambda} \mu(A\cap B \cap C) = \frac{\mu(A\cap B)}{\mu(B)}
\end{align*}
and application of Jensen gives
\begin{align*}
\frac{\mu(A\cap B)}{\mu(B)}\cdot \log \Big( \frac{\mu(A\cap B)}{\mu(B)} \Big) \leq \sum_{C\in \lambda} \frac{\mu(B\cap C)}{\mu(B)} \cdot \log\Big( \frac{\mu(A\cap B \cap C)}{\mu(B\cap C)} \Big)
\end{align*}
and therefore
\begin{align*}
\mu(A\cap B)\cdot \log \Big( \frac{\mu(A\cap B)}{\mu(B)} \Big) \leq \sum_{C\in \lambda} \mu(B\cap C) \cdot \log \Big( \frac{\mu(A\cap B \cap C)}{\mu(B\cap C)} \Big)
\end{align*}

\eop
\end{proof}
\s

\cor $H_{\mu}(\xi) \leq H_{\mu}(\xi \vee \eta) \leq H_{\mu}(\xi) + H_{\mu}(\eta)$.
\begin{proof}
\pf Using the chain rule, obtain
\begin{align*}
H_{\mu}(\xi \vee \eta) = H_{\mu} (\xi)  + H_{\mu}(\eta | \xi)
\end{align*}
and form the previous lemma, has $H_{\mu}(\eta |\xi) \leq H_{\mu}(\eta)$

\eop
\end{proof}
\s

\newday

(8th November, Thursday)
\s

\lem Let $(X,\borel, \mu, T)$ be an MPS. Let $\xi, \eta$ be countable measurable partitions. Then :
\begin{align*}
& I_{\mu}(T^{-1}\xi | T^{-1} \eta ) = I_{\mu}(\xi | \eta)(Tx) \\
& H_{\mu}(T^{-1} \xi | T^{-1} \eta) = H_{\mu}(\xi | \eta)
\end{align*}
where $T^{-1} \xi$ is the partition whose atoms are $T^{-1}([x]_{\xi})$.

\begin{proof}
\pf Has 
\begin{align*}
I_{\mu}(T^{-1} \xi | T^{-1} \eta)(x)  = -\log \Big( \frac{\mu([x]_{T^{-1}\xi \vee T^{-1}\eta} )}{\mu([x])_{T^{-1}\eta}} \Big)
\end{align*}
Note
\begin{align*}
T^{-1} \xi \vee T^{-1}\eta = T^{-1}(\xi \vee \eta) \quad \text{and} \quad [x]_{T^{-1} \xi \vee T^{-1}\eta} = T^{-1} [Tx]_{\xi \vee \eta}
\end{align*}
hence $\mu([x]_{T^{-1} \xi \vee T^{-1}\eta}) = \mu([Tx]_{\xi \vee \eta})$ by the measure preserving property. Similarly $\mu([x]_{T^{-1}\eta}) = \mu ([Tx]_{\eta})$. Then $I_{\mu}(T^{-1}\xi | T^{-1} \eta ) = -\log \Big( \frac{\mu([x]_{T^{-1}\xi \vee T^{-1}\eta} )}{\mu([x])_{T^{-1}\eta}} \Big) =  I_{\mu}(\xi | \eta)(Tx)$

\quad The statement on $H_{\mu}$ follows by integrating $I_{\mu}$

\eop
\end{proof}
\s

\cor Writing $\xi_m^n = T^{-m} \xi \vee T^{-(m+1)}\xi \vee \cdots \vee T^{-n} \xi$, has
\begin{align*}
H_{\mu}(\xi_0^{n+m-1}) \leq H_{\mu}(\xi_0^{n-1}) + H_{\mu}(\xi_0^{m-1})
\end{align*}
\begin{proof}
\pf Note that $\xi_0^{n+m-1} = \xi_0^{n-1} \vee \xi_n^{n+m-1}$. So we have
\begin{align*}
H_{\mu}(\xi_0^{n+m-1}) &\leq H_{\mu}(\xi_0^{n-1})+ H_{\mu}(\xi_n^{n+m-1}) = H_{\mu}(\xi_0^{n-1}) + H_{\mu}(T^{-n}\xi_0^{m-1}) \\
&= H_{\mu}(\xi_0^{n-1}) + H_{\mu}(\xi_0^{m-1})
\end{align*}
where the last equality follows from the previous lemma.

\eop
\end{proof}
\s

\lem \emph{(Felate's lemma)} Let $(a_n)\subset \reals$ be a subadditive sequence, that is
\begin{align*}
a_{n+m} \leq a_n + a_m \quad \forall n,m
\end{align*}
Then $\lim_{n\rightarrow \infty} a_n/n$ exists and equals $\inf_{n} a_n /n $.
\begin{proof}
\textbf{proof sketch)} Need to show that $\limsup_{n\rightarrow \infty} \frac{a_n}{n} \leq \frac{a_{n_0}}{n_0}$ for all $n_0$. For each fixed $n_0$, we can write $n = j(n)n_0 + i(n)$, where $i(n) \in [0,n_0-1]$. Iterate sub-additivity to get $a_n \leq j(n) a_{n_0} + a_{i(n)}$.

\quad See the online note for the full proof.
\end{proof}
\s

\defi Let $(X,\borel, \mu, T)$ be an MPS. Let $\xi, \eta$ be countable measurable partitions such that $H_{\mu}(\xi) < \infty$. The \textbf{entropy of the MPS w.r.t. $\xi$} is :
\begin{align*}
h_{\mu} (\xi) = \lim_{n\rightarrow \infty} \frac{H_{\mu}(\xi_0^{n-1})}{n} = \inf_n \frac{H_{\mu}(\xi_0^{n-1})}{n}
\end{align*}
whose existence of the limit is guaranteed by \emph{Felate's lemma}.(in fact, $\frac{H_{\mu}(\xi_0^{n-1})}{n}$ is a monotone decreasing sequence - will show in the example sheet)

\quad The \textbf{entropy of the MPS} is $h_{\mu}(T) = \sup_{\xi : H_{\mu}(\xi)<\infty} h_{\mu}(T|\xi)$.
\s

$h_{\mu} (\xi)$ expresses how fast we can learn information from a particular experiment $\xi$, and $h_{\mu}(T)$ is the maximal information we can obtain from the system when an appropriate experiment is chosen.
\s

The problem of this definition is that it is generally difficult to find out the supremum $\sup_{\xi : H_{\mu}(\xi)<\infty} h_{\mu}(T|\xi)$ - since this requires computing entropy w.r.t $\xi$ for each $\xi$. The good news is that (at least for the Bernoulli shifts), if we can find a partition that satisfies a particular property(so called \textbf{2-sided generator}), then in fact the supremum is achieved by the partition.
\s

\defi Let $(X,\borel, \mu, T)$ be an invertible MPS. Let $\xi \subset \borel$ be a countable measurable partitions. We say that $\xi$ is a \textbf{2-sided generator} if $\forall A\in \borel$ and $\forall \epsilon >0$, $\exists k\in \mathbb{Z}_{>0}$ such that $\exists A'\in \sigma( \xi^k_{-k} )$ and $\mu(A \triangle A') <\epsilon$.
\s

\thm \emph{(Kolmogorov-Sinai)} Let $(X,\borel, \mu, T)$ be an \emph{invertible} measure preserving system. Let $\xi$ be a countable measurable partition with $H_{\mu}(\xi) < \infty$, which is a 2-sided generator. Then
\begin{align*}
h_{\mu}(T) = h_{\mu}(T,\xi)
\end{align*}
\s

We delay the proof of this theorem until next lecture. Instead, we start to compute something useful.
\s

\textbf{Example :} Let $(\{1,2,\cdots, k\}^{\mathbb{Z}}, \borel, \mu, \sigma)$ be the $(p_1, \cdots, p_k)$-Bernoulli shift. Let $X =\{1,2,\cdots, k\}^{\mathbb{Z}}$.
\begin{itemize}
\item \textbf{Claim :} The partition $\xi = \{ \{x\in X: x_0 =j \} : j=1, \cdots, k \}$ is a 2-sided generator.
\begin{subproof}
\pf The collection of sets
\begin{align*}
\{ A\in \borel : \forall \epsilon, \,\, \exists k \,\, \exists A' \in \xi^{k}_{-k} \text{ with } \mu(A\triangle A') < \epsilon \} \subset \sigma(\xi) \subset \borel
\end{align*}
is a $\sigma$-algebra, and it contains cylinder sets. Hence it is equal to $\borel$, as $\borel$ is generated by cylinder sets.

\eop
\end{subproof}
\s

\item \textbf{Claim :} With $\xi$ defined as above, we have 
\begin{align*}
H_{\mu}(\xi | \xi_{1}^n) = H(p_1, p_2, \cdots, p_k) = -p_1 \log p_1 -\cdots -p_k \log p_k
\end{align*}
for all $n \in \mathbb{Z}_{\geq 0}$.
\begin{subproof}
\pf Calculate the information function : 
\begin{align*}
I_{\mu}(\xi | \xi_1^n) (x)  = \log \Big( \frac{\mu([x]_{\xi_1^n})}{\mu([x]_{\xi_0^n})} \Big)
\end{align*}
Note $[x]_{\xi_0^n} = \{y\in X : y_0=x_0, \cdots, y_n =x_n \}$, so $\mu([x]_{\xi_0^n}) = p_{x_0}\cdots p_{x_n}$. Similarly, has $\mu([x]_{\xi_1^n})=p_{x_1} \cdots p_{x_n}$, and
\begin{align*}
I_{\mu}(\xi|\xi_1^n) (x) = - \log p_{x_0} 
\end{align*}
therefore $H_{\mu} (\xi |\xi_1^n) = \sum_{j=1}^{k} p_j (-\log (p_j)) = H(p_1, \cdots, p_k)$. 

\eop
\end{subproof}
\s

\item Hence
\begin{align*}
H_{\mu}(\xi_1^{n-1}) &= H_{\mu}(\xi_{n-1}^{n-1}) + H_{\mu}(\xi_{n-2}^{n-2} | \xi_{n-1}^{n-1}) + H_{\mu}(\xi_{n-3}^{n-3} | \xi_{n-2}^{n-1}) + \cdots + H_{\mu}(\xi | \xi^{n-1}_{1}) \quad (\text{Chain rule}) \\
&= H_{\mu}(\xi) + H(\xi |\xi_1^1) + \cdots + H_{\mu}(\xi | \xi_1^{n-1}) \quad (\text{invariance, first lemma of the day}) \\
&= nH(p_1, \cdots, p_k)
\end{align*}
Divide by $n$ and take the limit, 
\begin{align*}
h_{\mu}(T) = h_{\mu}(T,\xi) = H(p_1, \cdots, p_k)
\end{align*}

So the entropy of $(1/2,1/2)$ shift is $\log2$ and $(1/3,1/3,1/3)$ shift is $\log3$ - which shows that two systems cannot be isomorphic.
\end{itemize}
\s

\newday

(10th November, Saturday)
\s

\thm \emph{(Kolmogorov-Sinai)} Let $(X,\borel, \mu, T)$ be an \emph{invertible} measure preserving system. Let $\xi$ be a countable measurable partition with $H_{\mu}(\xi) < \infty$, which is a 2-sided generator. Then
\begin{align*}
h_{\mu}(T) = h_{\mu}(T,\xi)
\end{align*}
\s

We will need three lemmas.
\s

\lemnum{1} Let $(X,\borel, \mu, T)$ be an \emph{invertible} measure preserving system. Let $\xi \subset B$ be a countable partition. Then
\begin{align*}
h_{\mu}(T, \xi^n_{-n}) = h_{\mu}(T,\xi)
\end{align*}
\s

\lemnum{2} Let $(X,\borel, \mu, T)$ be an MPS. Let $\xi, \eta \subset \borel$ be two countable partitions. Then
\begin{align*}
h_{\mu}(T,\eta) \leq h_{\mu}(T, \xi) + H_{\mu}(\eta | \xi)
\end{align*}
\s

\lemnum{3} For any $\epsilon>0$ and $k\in \mathbb{Z}_{>0}$, $\exists \delta >0$ such that the following holds : let $(X, \borel, \mu)$ be a probability space. Let $\xi \subset \borel$ be a countable and $\eta \subset \borel$ a finite partition. Suppose that $\eta$ has $k$ atoms and for each $A\in \eta$, $\exists B\in \sigma(\xi)$ such that $\mu(A \triangle B) <\delta$. Then 
\begin{align*}
H_{\mu}(\eta | \xi) \leq \epsilon
\end{align*}
\s

Let us prove the theorem assuming the lemmas. We will come hack to the proof of the lemmas later.
\s

\begin{proof}
\textbf{proof of Theorem)} We first show $h_{\mu} (T,\xi) = \sup_{\eta \text{ finite}} h_{\mu} (T,\eta)$. We need to show that for all finite partition $\eta \subset \borel$, we have $h_{\mu}(T,\eta) \leq h_{\mu}(T,\xi)$. Fix $\epsilon>0$. By \textbf{Lemma 3} and definition of 2-sided generator, $\exists n$ such that $H_{\mu}(\eta | \xi^n_{-n}) \leq \epsilon$. Then we have
\begin{align*}
h_{\mu}(T,\eta) & \leq h_{\mu}(T,\xi_{-n}^n) + H_{\mu}(\eta | \xi^n_{-n}) \leq h_{\mu}(T,\xi_{-n}^n) + \epsilon \quad (\text{Lemma 2}) \\
& = h_{\mu} (T,\xi) + \epsilon \quad (\text{Lemma 1})
\end{align*}
We are done if we take $\epsilon \searrow 0$.
\s

Now it is left to show that $\forall \epsilon >0$, for every countable partition $\eta \subset \borel$, $\exists \tilde{\eta} \subset \borel$ finite such that $h_{\mu}(T,\eta) \leq h_{\mu}(T, \tilde{\eta}) + \epsilon$. By \textbf{Lemma 2,} it is enough to show $H_{\mu}(\eta |\tilde{\eta}) \leq \epsilon$. When $\eta = \{A_1, A_2, \cdots \}$, let $\tilde{\eta} = (A_1, A_2, \cdots, A_n, \bigcup_{j>n} A_j)$ for sufficiently large $n$ . Then
\begin{align*}
H_{\mu}(\eta) = H_{\mu} (\eta \vee \tilde{\eta}) = H_{\mu}(\tilde{\eta}) + H_{\mu}(\eta |\tilde{\eta})
\end{align*}
Thus
\begin{align*}
H_{\mu}(\eta | \tilde{\eta}) &= H_{\mu}(\eta) - H_{\mu}(\tilde{\eta}) = \sum_{j>n} (-1) \mu(A_j) \log \mu(A_j) + \mu \Big( \cup_{j=n+1}^{\infty} A_j \Big) \log \mu \Big( \bigcup_{j=n+1}^{\infty} A_j \Big) \\
& \leq \sum_{j>n} (-1) \mu(A_j) \log \mu(A_j) \leq \epsilon
\end{align*}
if $n$ is sufficiently large.

\eop
\end{proof}
\s

Now we prove the lemmas.
\s

\begin{proof}
\textbf{proof of Lemma 1)} \begin{align*}
h_{\mu}(T, \xi_{-m}^m) &= \lim_{n\rightarrow \infty} \frac{1}{n} H_{\mu}(\xi^{n+m-1}_{-m}) = \lim_{n\rightarrow \infty} \frac{1}{n} H_{\mu} (\xi_0^{n+2m -1}) \\
&= \lim_{n\rightarrow \infty} \frac{1}{n+2m-1} H_{\mu}(\xi_0^{n+2m-1}) = h_{\mu}(T,\xi)
\end{align*}

\eop
\end{proof}
\s

\begin{proof}
\textbf{proof of Lemma 2)} \begin{align*}
h_{\mu}(T,\eta) &= \lim_{n\rightarrow \infty} \frac{1}{n} H_{\mu}(\eta^{n-1}_0) \\
& \leq \lim_{n\rightarrow \infty} \frac{1}{n} H_{\mu}((\xi \vee \eta)_0^{n-1}) \\
&= \lim_{n\rightarrow \infty} \frac{1}{n}\Big( H_{\mu}(\xi_0^{n-1}) + H_{\mu}(\eta_0^{n-1} | \xi_0^{n-1}) \Big) \\
&= h_{\mu}(T,\xi) + \lim_{n\rightarrow \infty} \frac{1}{n} \Big( \sum_{j=0}^{n-1} H_{\mu}(\eta_j^1 | \xi_0^{n-1} \vee \eta_{j+1}^{n-1})  \Big) \quad \text{(Chain rule)} \\ 
&= h_{\mu}(T,\xi) + \lim_{n\rightarrow \infty} \frac{1}{n} \Big( \sum_{j=0}^{n-1} H_{\mu} (\eta_j^j | \xi_j^j) \Big) \quad \text{(throwing away kwown information increases entropy)}\\
&= h_{\mu}(T, \xi) + H_{\mu}(\eta |\xi)
\end{align*}
where the last equality follows because $H_{\mu} (\eta_j^j | \xi_j^j) = H_{\mu}(\eta |\xi)$.

\eop
\end{proof}
\s

\begin{proof}
\textbf{proof of Lemma 3)} Write $A_1, \cdots, A_k$ for the atoms of $\eta$ and for each $i \in \{1,\cdots, k\}$. Let $B_i \in \sigma(\xi)$ such that $\mu(A_i \triangle B_i) < \delta$. We consider the partition $\lambda$, which has the following $k+1$ atoms
\begin{subproof}
: $C_0 = \bigcup_{i=1}^k (A_i \cap (B_i \backslash \bigcup_{j\neq i} B_j ) )$ and $C_i = A_i \backslash C_0$ for $i=1, \cdots, k$.

They have some useful properties :
\begin{itemize}
\item[-] $C_{0}$ is big, i.e. $\mu(C_0) \geq \sum_{i=1}^k (\mu(A_i) - k\delta) = 1-k^2 \delta$.
\item[-] If $x\in C_0$, then $x\in A_i$ $\Leftrightarrow$ $x\in B_i$.
\end{itemize}
Also note, 
\begin{align*}
H_{\mu} (\lambda) &= -\mu(C_0) \log (\mu(C_0)) - \sum_{i=1}^k \mu(C_i) \log \mu (C_i) \\
&\leq -\mu(C_0) \log (\mu (C_0)) - \sum_{i=1}^n \mu(C_i) \log \Big( \frac{\sum_{i=1}^k \mu(C_i)}{k} \Big) \quad \text{(Jensen to }x\mapsto x\log x) 
\end{align*}
\end{subproof}
If $\delta$ is sufficiently small, then $\mu(C_0)$ can be made as close to $1$ as we want and $\sum_{i=1}^k \mu(C_i)$ is as small as we want. Then
$H_{\mu}(\lambda) < \epsilon$ if $\delta$ is sufficiently small. Now, we may write
\begin{align*}
H_{\mu}(\eta | \xi) &\leq H_{\mu}(\eta \vee \lambda | \xi) \leq H_{\mu}(\lambda | \xi) + H_{\mu}(\eta | \xi \vee \lambda) \\
& \leq H_{\mu} + H_{\mu}(\eta | \xi \vee \lambda) < \epsilon + H_{\mu}(\eta | \xi \vee \lambda)
\end{align*}
so it is enough to show that
\begin{align*}
H_{\mu}(\eta | \xi \vee \lambda) =0
\end{align*}
However, this trivially holds because $\sigma (\eta) \subset \sigma(\xi \vee \lambda)$, i.e. $[x]_{\xi \vee \lambda} \subset [x]_{\eta}$.
\begin{subproof}
: formally, this can be deduced from the fact \begin{align*}
A_i &= C_i \cup (C_0 \cap A_i) = C_i \cup (A_i \cap (B_i \backslash \cup_{j\neq i} B_j)) \\
&= C_i \cup (C_0 \cap (B_i \backslash \cup_{j\neq i} B_j)) \in \sigma(\xi \vee \lambda)
\end{align*}
\end{subproof}

\eop
\end{proof}
\s

\newday

(13th November, Tuesday)
\s

\section*{11. The Shannon McMillan Breiman theorem}

\thm \emph{(Shannon-McMillan-Breiman)} Let $(X, \borel, \mu, T)$ be an ergodic MPS. Let $\xi \subset \borel$ be a countable measurable partition. with $H_{\mu}(\xi) < \infty$. Then
\begin{align*}
\frac{1}{n} I(\xi^{n-1}_0) \xrightarrow{n\rightarrow \infty} h_{\mu}(T,\xi) 
\end{align*}
pointwise $\mu$-a.e. and in $L^1$.
\s

\textbf{Recall :} $I(\xi_0^{n-1})(x) = -\log \mu([x]_{\xi_0^{n-1}})$, so get $\mu([x]_{\xi_0^{n-1}})\approx \exp( -n h_{\mu}(T,\xi))$ approximately.
\s

A lecture and a half would be devoted in proving this theorem.

\begin{proof}
\textbf{Idea :} Using the chain rule and then invariance,
\begin{align*}
\frac{1}{n} I_{\mu}(\xi^{n-1}_0)(x) &= \frac{1}{n} \sum_{j=0}^{n-1} I_{\mu}(\xi_j^j |\xi_{j+1}^{n-1})(x) \\
&= \frac{1}{n} \sum_{j=0}^{n-1} I_{\mu}(\xi | \xi_1^{n-j-1})(T^j x)
\end{align*}
so this looks as if we can apply pointwise ergodic theorem. However, we still the notion of conditional entropy to develop this idea.
\end{proof}

\subsubsection*{Conditional expectation}

Let $(X,\borel, \mu)$ be a probability space, and let $\mathscr{A} \subset \borel$ be a sub-$\sigma$-algebra. The for all $f\in L^1(X,\borel, \mu)$, $\exists f^* \in L^1(X, \borel, \mu)$ such that
\begin{itemize}
\item[(1)] $f^*$ is $\mathscr{A}$-measurable.
\item[(2)] $\int_A fd\mu = \int_A f^* d\mu$ for all $A \in \mathscr{A}$.
\end{itemize}
If $f_1^*$ and $f_2^*$ both satisfy (1) and (2) in the role of $f^*$, then $f_1^* = f_2^*$ $\mu$-a.e. The function $f^*$ is called the \textbf{condition expectation} of $f$ relative to $\mathscr{A}$ and it is denoted by $\avg  [f| \mathscr{A}]$.
\s

\textbf{Remarks :}
\begin{itemize}
\item Writing $V_{\mathscr{A}}$ for the closed subspace of $L^2(X,\borel, \mu)$ consisting of $\mathscr{A}$-measurable functions, $\avg[f|\mathscr{A}]$ is the orthogonal projection of $f$ to $V_{\mathscr{A}}$ provided $f\in L^2$. 
\item If $\mathscr{A}$ is generated by a countable partition $\xi$, then
\begin{align*}
\avg[f | \mathscr{A}](x)  = \mu([x]_{\xi})^{-1} \int_{[x]_{\xi}} fd\mu
\end{align*}
\end{itemize}
\s

\thm Let $(X, \borel, \mu)$ be the probability space, let $f,f_1,f_2\in L^1(X, \borel, \mu)$, $\mathscr{A},\mathscr{A}_1,\mathscr{A}_2 \subset \borel$ be sub-$\sigma$-algebra. Then
\begin{itemize}
\item[(1)] $\avg[f_1 + f_2 | \mathscr{A}] = \avg[f_1 |\mathscr{A}] + \avg{f_2 | \mathscr{A}}$.
\item[(2)] If $f_1$ is $\mathscr{A}$-measurable, then $\avg[f_1 f | \mathscr{A}] = f_1 \avg[f|\mathscr{A}]$.
\item[(3)] If $\mathscr{A}_1 \subset \mathscr{A}_2$ then $\avg[f|\mathscr{A}_1] = \avg[\avg[\mathscr{A}|\mathscr{A}_2]|\mathscr{A}_1]$.
\end{itemize}
\s

\thm \emph{(Martingale theorems)} Let $(X,\borel, \mu)$ be a probability space, let $f\in L^1$ and let $\mathscr{A},\mathscr{A}_1,\mathscr{A}_2, \cdots \subset \borel$ be sub-$\sigma$-algebras. Assume that \emph{either} (1) $\mathscr{A}_1 \subset \mathscr{A}_2 \subset \cdots$ and $\mathscr{A} = \sigma(\mathscr{A}_j : j\geq 1)$ \emph{or} (2) $\mathscr{A}_1 \supset \mathscr{A}_2 \supset \cdots$ and $\mathscr{A} = \cap \mathscr{A}_j$. Then
\begin{align*}
\lim_{n\rightarrow \infty} \avg[f| \mathscr{A}_n] = \avg[f|\mathscr{A}]
\end{align*}
both pointwise $\mu$-a.e. and in $L^1$.
\s

If you have not seen these theorems but too lazy to go over the proofs, you can see these easily for $f\in L^2$ case.

\subsubsection*{Conditional information and entropy and entropy relative to $\sigma$-algebra}

Let $(X,\borel, \mu)$ be a probability space, let $\eta \subset \borel$ be a countable partition let $\mathscr{A} \subset \borel$ be a sub-$\sigma$-algebra. Then the \textbf{conditional information function of $\eta$ relative to $\mathscr{A}$} is
\begin{align*}
I_{\mu}(\eta | \mathscr{A}) (x) = \sum_{A \in \eta} -\chi_A \cdot \log \Big( \avg[\chi_A | \mathscr{A}] \Big)
\end{align*}
\s

\textbf{Example :} Suppose that $\mathscr{A}$ is generated by a countable partition $\xi \subset \borel$. Then
\begin{align*}
I_{\mu}(\eta | \mathscr{A})(x) &= \sum_{A \in \eta} -\chi_A \log \avg [\chi_A | \mathscr{A}](x) \\
&= -\log \avg[ \chi_{[x]_{\eta}} | \mathscr{A}] (x)  \\
&= -\log \frac{1}{\mu([x]_{\xi})} \int_{[x]_{\xi}} \chi_{[x]_{\eta}} d\mu \\
&= -\log \frac{\mu([x]_{\eta} \cap [x]_{\xi} )}{\mu([x]_{\xi})} = -\log \frac{[x]_{\eta \vee \xi}}{\mu([x]_{\xi})}
\end{align*}
\s

\defi The \textbf{conditional entropy of $\eta$ relative to $\mathscr{A}$} is defined as $H_{\mu}(\eta | \mathscr{A}) = \int I_{\mu}(\eta | \mathscr{A}) d\mu$. 
\s

\thm \emph{(Maximal inequality)} Let $(X,\borel, \mu)$ be a probability space, and let $\xi_1, \xi_2, \cdots$ be countable measurable partitions such that $\sigma(\xi_1) \subset \sigma(\xi_2) \subset \cdots$. Let $\mathscr{A} = \sigma(\xi_j:j\geq 1)$. Let $\eta \subset \borel$ be another countable partition. Then 
\begin{align*}
I_{\mu}(\eta | \xi_n) \xrightarrow{n\rightarrow \infty} I_{\mu}(\eta | \mathscr{A}) \quad \text{pointwise a.e. and in } L^1
\end{align*}

\quad Moreover, $I^*$ defined by $I^* = \sup_{n\in \mathbb{Z}_{>0}} I_{\mu}(\eta | \xi_n)$ is in $L^1(X,\borel, \mu)$.
\s

The theorem gives a useful tool to deal with $I_{\mu}(\eta | \mathscr{A})$ when $\mathscr{A}$ can be approximated by countable partitions. 
\s

\newday

(15th November, Thursday)
\s

(??? Why are we using the different version of Maximal inequality?)
\s

\thm \emph{(Maximal inequality)} Let $(X,\borel, \mu)$ be a probability space. Let $\mathscr{A}_1 \subset \mathscr{A}_2 \subset \cdots$ be a sequence of sub-$\sigma$-algebras of $\borel$, and let $\eta \subset \borel$ be a countable partition with $H_{\mu}(\eta) >\infty$. Then :
\begin{align*}
I_{\mu}(\eta | \mathscr{A}_n) \rightarrow I_{\mu}(\eta | \mathscr{A}) \quad \text{as } n\rightarrow \infty
\end{align*}
pointwise $\mu$-a.e. and in $L^1$, where $\mathscr{A} = \sigma(\cup_n \mathscr{A}_n)$.

\quad Moreover, $I^*(x) = \sup_{n\in \mathbb{Z}_{>0}}I_{\mu}(\eta | \mathscr{A}_n) (x) \in L^1$.
\begin{proof}
\textbf{proof for $\eta$ finite)} By definition and martingale convergence,
\begin{align*}
I_{\mu}(\eta | \mathscr{A}_n)(x) = -\log \avg [ \chi_{[x]_{\eta}} | \mathscr{A}_n ](x) \xrightarrow{n\rightarrow \infty} -\log \avg [ \chi_{[x]_{\eta}} | \mathscr{A}](x) = I_{\mu}(\eta | \mathscr{A})(x)
\end{align*}
This proves pointwise convergence. By dominated convergence, $L^1$ convergence follows if assuming $I^* \in L^1$.

\quad Now it is enough to show $I^* \in L^1$
\begin{subproof}
: fix $A \in \eta$, fix $\alpha>0$. For $x\in X_1$, let $n(x)$ be defined by
\begin{align*}
n(x) = \min \{ n : \log (\avg[\chi_A | \mathscr{A}_n](x)) > \alpha \}
\end{align*}
If the above does not hold for any $n$, the we write $n(x) = \infty$. Define $B_n = \{ x\in X : n(x) =n \}$. Note that $B_n$ is $\mathscr{A}_n$-measurable as we may write
\begin{align*}
B_n = \{ x\in X : \avg[\chi_A | \mathscr{A}_n](x) < \exp(-\alpha) \text{ but } \avg[\chi_A |\mathscr{A}(x) \geq \exp(-\alpha) \,\, \forall j<n ] \}
\end{align*}
Then
\begin{align*}
\mu(B_n) \exp(-\alpha) \geq \int_{B_n} \avg[\chi_A | \mathscr{A}_n] d\mu = \int_{B_n} \chi_A d\mu = \mu(B_n \cap A)
\end{align*}
Define $A^* = \{ x\in A : I^*(x) > \alpha \} \subset A \cap (\cup_n B_n)$. Then since $B_n$'s are disjoint,
\begin{align*}
\mu(A^*) \leq \sum_{n=1}^{\infty} \mu(A\cap B_n) \leq \exp(-\alpha) \sum_{n=1}^{\infty} \mu(B_n) \leq \exp(-\alpha)
\end{align*}
and summing these over all elements of $\eta$, we obtain
\begin{align*}
\mu \big( \{ x\in X : I^*(x) > \alpha \} \big) \leq |\eta| \exp(-\alpha)
\end{align*}
Then
\begin{align*}
\int I^*(x) d\mu \leq \sum_{n=1}^{\infty} \mu(x\in X : I^*(x) > n-1) \cdot n \leq \sum_{n=1}^{\infty} |\eta| \exp(-(n-1))\cdot n < \infty
\end{align*}
\end{subproof}

\eop
\end{proof}
\s

\lem Let $(X,\borel, \mu, T)$ be an MPS. Let $\xi \subset \borel$ be a countable partition with $H_{\mu}(\xi)< \infty$. Then
\begin{align*}
h_{\mu}(T,\xi) = \lim_{n\rightarrow \infty} H_{\mu}(\xi | \xi_1^n)
\end{align*}
\begin{proof}
\pf By chain rule and invariance of $T$,
\begin{align*}
\frac{1}{n} H_{\mu}(\xi_0^{n-1}) = \frac{1}{n} \sum_{j=0}^{n-1} H_{\mu}(\xi | \xi_1^j)
\end{align*}
hence
\begin{align*}
h_{\mu}(T,\xi) = \clim_{n\rightarrow \infty} H_{\mu}(\xi | \xi_1^n)
\end{align*}
Observe that $H_{\mu}(\xi | \xi_1^n)$ is a monotone decreasing sequence, hence it converges, so in fact the Ces\`{a}ro limit implies strong limit.

\eop
\end{proof}
\s

We are now ready to prove Shannon-McMillan-Breiman theorem.
\s

\thm Let $(X,\borel, \mu, T)$ be an ergodic MPS. Let $\xi \subset \borel$ be a countable partition with $H_{\mu}(\xi) < \infty$. Then
\begin{align*}
\frac{1}{N} I_{\mu}(\xi_0^{N-1})(x) \rightarrow h_{\mu}(\xi, T)
\end{align*}
pointwise a.e. and in $L^1$.
\begin{proof}
\pf
\begin{align*}
\frac{1}{N} I_{\mu}(\xi_{0}^{N-1})(x) = &\frac{1}{N} \sum_{n=0}^{N-1} I_{\mu} (\xi | \xi_1^{N-n-1}) (T^n x) \\
& + \frac{1}{N}\sum_{n=0}^{N-1} I_{\mu}(\xi | \borel(\xi_1^{\infty}))(T^n x) \\
& + \frac{1}{N} \sum_{n=0}^{N-1} \Big( I_{\mu}(\xi | \xi_1^{N-n-1})(T^n x) - I_{\mu}(\xi | \borel(\xi_1^{\infty})(T^n x) \Big)
\end{align*}
where $\borel(\xi_1^{\infty})$ is the $\sigma$-algebra generated by $\bigcup_{n=1}^{\infty} \xi_1^n$.

\quad By the pointwise ergodic theorem, has
\begin{align*}
\frac{1}{N} \sum_{n=0}^{N-1} I_{\mu}( \xi | \borel(\xi_1^{\infty}) ) (T^n x) \rightarrow & \int I_{\mu}(\xi | \borel(\xi_1^{\infty})) d\mu \\
& = \lim_{n\rightarrow \infty} \int I_{\mu}(\xi | \xi_1^n) d\mu \\
& = \lim_{n\rightarrow \infty} H(\xi | \xi_1^n) = h_{\mu}(T, \xi)
\end{align*}
Define
\begin{align*}
I_K^* (x) = \sup_{k\geq K} \Big| I_{\mu}(\xi | \xi_1^k)(x) - I_{\mu}(\xi | \borel(\xi_1^{\infty}))(x) \Big|
\end{align*}
By the maximal inequality, $I_K^* \in L^1$ for all $x$ and $I_K^*(x) \rightarrow 0$ for $\mu$-a.e. $x$. $I_K^*$ are pointwise monotone decreasing, so we have
\begin{align*}
\int I_K^* d\mu \rightarrow 0 \quad \text{as } K \rightarrow \infty
\end{align*}
Now again by pointwise ergodic theorem,
\begin{align*}
& \Big| \frac{1}{N} \sum_{n=0}^{N-1} \Big( I_{\mu}(\xi | \xi_1^{N-n-1})(T^n x) - I_{\mu}(\xi | \borel(\xi_1^{\infty})(T^n x) \Big) \Big| \\
\leq & \frac{1}{N} \sum_{n=0}^{N-K-1} I_K^* (T^n x) + \frac{1}{N} \sum_{n=N-K}^{N-1} I_0^* (T^n x) \xrightarrow{N\rightarrow \infty} \int I_K^* d\mu
\end{align*}
as 
\begin{align*}
\frac{1}{N} \sum_{n=N-K}^{N-1} I_0^* (T^n x) = \frac{1}{N} \sum_{n=0}^{N-1}I_0^* (T^n x) - \frac{N-K}{N} \frac{1}{N-K} \sum_{n=0}^{N-K-1} I_0^*(T^n x) \rightarrow \int I_0^* d\mu -\int I_0^* d\mu =0
\end{align*}
Hence,
\begin{align*}
\limsup_{N\rightarrow \infty} \Big| \frac{1}{N} \sum_{n=0}^{N-1} \Big( I_{\mu}(\xi | \xi_1^{N-n-1})(T^n x) - I_{\mu}(\xi | \borel(\xi_1^{\infty})(T^n x) \Big) \Big| \leq \int I_K^* d\mu
\end{align*}
Since $K$ was arbitrary, and $\int I_K^* d\mu \xrightarrow{K\rightarrow \infty} =0$, so 
\begin{align*}
\limsup_{N\rightarrow \infty} \Big| \frac{1}{N} \sum_{n=0}^{N-1} \Big( I_{\mu}(\xi | \xi_1^{N-n-1})(T^n x) - I_{\mu}(\xi | \borel(\xi_1^{\infty})(T^n x) \Big) \Big| =0
\end{align*}
so we have pointwise convergence.

\quad Moreover, if we observe carefully, we have $L^1$ convergence at each line of the proof, so we also have the $L^1$ convergence.

\eop
\end{proof}
\s

\newday

(17th November, Saturday)

\section*{12. Mixing and Entropy}

\defi An MPS $(X,\borel, \mu, T)$ is called \textbf{K(olmogorov)-mixing} if $\forall \epsilon >0$, $A\in \borel$ and $\xi\subset \borel$ finite partition, $\exists N \in \mathbb{Z}_{>0}$ such that : $\forall B \in \sigma( \xi^{\infty}_N)$ we have
\begin{align*}
|\mu(A \cap B) - \mu(A) \mu(B)| < \epsilon
\end{align*}
\emph{Invertible} MPS's that are K-mixing are called \textbf{K-automorphism}.(this is more common notion in literatures)
\s

From now on in the course, we are only using finite partitions. Some results also holds for countable partitions, but we do not need such stronger results.
\s

\textbf{Remark :} This implies mixing
\begin{proof}
: Let $A, C \in \borel$ and consider $\xi = \{C, X \backslash C \}$. Then for $N$ sufficiently large, we can take $B = T^{-N}(C)$ in the definition. 
\end{proof}

\defi Let $(X,\borel, \mu, T)$ be an MPS. Let $\xi \subset \borel$ be a finite partition. Then the \textbf{tail $\sigma$-algebra of $\xi$} is
\begin{align*}
\tau (\xi) = \bigcap_{n=1}^{\infty} \sigma(\xi_n^{\infty})
\end{align*}
\s

\thm Let $(X,\borel, \mu, T)$ be an MPS. The following are equivalent.
\begin{itemize}
\item[(1)] $(X,\borel, \mu, T)$ is a K-mixing.
\item[(2)] $\tau(\xi)$ is \emph{trivial} for all finite partition $\xi \subset \borel$. That is, $\forall A \in \tau(\xi)$, we have $\mu(A) \in \{0,1\}$.
\item[(3)] $(X,\borel, \mu, T)$ is of \emph{totally positive entropy}. That is, $h_{\mu}(T,\xi) >0$ for all partition $\xi \subset \borel$ finite such that $H_{\mu}(\xi) >0$.
\end{itemize}
\s

-Note : Bernoulli shift is a K-mixing and so (2) implies the Kolmogorov 0-1 law.
\s

\begin{proof}
\textbf{proof of (1) $\Leftrightarrow$ (2))} \begin{itemize}
\item[(1) $\Rightarrow$ (2)] Suppose (1) holds and let $A\in \tau(\xi)$. Then $A \in \sigma(\xi^{\infty}_n)$ for all $n$, by the definition of tail $\sigma$-algebra. For any $\epsilon >0$, by K-mixing property,
\begin{align*}
| \mu(A \cap A) - \mu(A) \mu(A)| < \epsilon
\end{align*}
so $\mu(A) = \mu(A)^2$. Hence $\mu(A) \in \{0,1\}$.
\item[(2) $\Rightarrow$ (1)] Suppose that (2) holds. Let $A, \xi, \epsilon$ be as in the definition of $K$-mixing. Let $N$ be sufficiently large and $B\in \borel(\xi_N^{\infty})$. Then
\begin{align*}
\mu(A \cap B) = \int_B \chi_A d\mu = \int_B \avg[ \chi_A | \sigma(\xi_N^{\infty})]  d\mu
\end{align*}
By (backward) martingale convergence, if $N$ is sufficiently large,
\begin{align*}
\big| \mu(A \cap B) - \int_B \avg[\chi_A | \tau (\xi)] d\mu \big| < \epsilon
\end{align*}
\textbf{Claim :} $\avg[\chi_A | \tau(\xi)] (x) = \mu(A)$ for $\mu$-a.e. x.
\begin{subproof}
: suppose the contrary. Letting $C_{\epsilon} = \{ x\in X : \avg[\chi_A | \tau()\xi] > \mu(A) + \epsilon  \}$. Then $\exists \epsilon >0$ such that $\mu(C_{\epsilon}) >0$. Note $C \in \tau(\xi)$ so $\mu(C_{\epsilon})=1$. But then
\begin{align*}
\mu(A) =\int_C \avg[ \chi_A | \tau(\xi)] d\mu > \mu(A) + \epsilon
\end{align*}
which is a contradiction.
\end{subproof}
From the claim, we see that $\int_B \avg[\chi_A |\tau(\xi)] d\mu = \mu(A) \mu(B)$, and hence the desired result.
\end{itemize}
\end{proof}
\s

For the rest of the theorem, we need additional results.
\s

\lem Let $\xi, \eta \subset \borel$ be a finite partition, let $\mathscr{A} \subset \borel$ be a sub-$\sigma$-algebra. Suppose that $\exists$ a sequence of finite partitions $\lambda_1, \lambda_2, \cdots$ such that $\mathscr{A}$ is the smallest $\sigma$-algebra that contains the atoms of $\tau_n$ for all $n$. Then
\begin{itemize}
\item[(1)] $I_{\mu}(T^{-1}\xi | T^{-1} \mathscr{A})(x) = I_{\mu}(\xi | \mathscr{A}) (Tx)$.
\item[(2)] $H_{\mu}(T^{-1}\xi | T^{-1} \mathscr{A})= H_{\mu}(\xi | \mathscr{A})$.
\item[(3)] $I_{\mu}(\xi \vee \eta  | \mathscr{A}) = I_{\mu}(\xi | \mathscr{A}) + I_{\mu}(\eta | \xi \vee \mathscr{A})$.
\item[(4)] $H_{\mu}(\xi \vee \eta  | \mathscr{A}) = H_{\mu}(\xi | \mathscr{A}) + H_{\mu}(\eta | \xi \vee \mathscr{A})$.
\item[(5)] $H_{\mu}(\xi | \mathscr[A] \vee \eta) \leq H_{\mu}(\xi | \mathscr{A})$.
\end{itemize}
\begin{proof}
The assumption of existence of such $\lambda_1, \lambda_2 \cdots$ is not essential, but makes the proof much easier. However, the assumption is not very serious restriction. The proof is to be done on the example sheet. - if you like measure theory, try proving the general version.
\end{proof}
\s

\prop Let $(X,\borel, \mu)$ be a probability space. Let $\xi \subset \borel$ be a finite partition. Let $\mathscr{A}_1 \supset \mathscr{A}_2 \supset \cdots$ be sub-$\sigma$-algebras of $\borel$ and let $\mathscr{A} = \cap_{n=1}^{\infty} \mathscr{A}_n$. Then
\begin{align*}
I_{\mu} (\xi | \mathscr{A}_n) \rightarrow I_{\mu}(\xi |\mathscr{A}) \quad \mu \text{-a.e. and in } L^1
\end{align*}
\begin{proof}
\pf We prove the stronger result
\begin{align*}
H_{\mu} (\xi | \mathscr{A}_n) \rightarrow H_{\mu}(\xi |\mathscr{A}) \quad \mu \text{-a.e. and in } L^1
\end{align*}
This is a result of backward-martingale convergence and dominated convergence : we clearly have pointwise convergence by continuity of logarithm function. Also, each $H_{\mu}(\xi | \mathscr{A}_n)$ is dominated by $\log(|xi|)$, so we may apply dominated convergence to obtain $L^1$-convergence.

\eop
\end{proof}
\s

\lem Let $(X, \borel, \mu, T)$ be an MPS and let $\xi \subset \borel$ be a finite partition. Then
\begin{align*}
h_{\mu} (T, \xi ) = H_{\mu}(\xi | \sigma(\xi_1^{\infty}))
\end{align*}
\begin{proof}
\pf We have already seen that
\begin{align*}
h_{\mu}(T, \xi )= \lim_{n\rightarrow \infty} H_{\mu}(\xi | \xi_1^n) 
\end{align*}
and this equals $H_{\mu}(\xi | \sigma(\xi_1^{\infty}))$ by the statement of \emph{maximal inequality}.

\eop
\end{proof}
\s

\lem Let $(X,\borel, \mu, T)$ be an MPS. Let $\xi \subset \borel$ be a finite partition such that $h_{\mu}(T, \xi) =0$. Then $H_{\mu}(\xi | \tau(\xi)) =0$. 
\begin{proof}
\pf From the previous lemma, we already know $H_{\mu}(\xi | \sigma(\xi_1^{\infty})) =0$. We show that $H_{\mu}(\xi | \sigma(\xi_n^{\infty})) =0$ for all $n$ and then we are done by the previous proposition. To this end, we write
\begin{align*}
H_{\mu}(\xi | \sigma(\xi_n^{\infty})) &\leq H_{\mu} (\xi_0^{n-1} | \sigma(\xi_n^{\infty})) \\
&= \sum_{j=0}^{n-1} H_{\mu}(\xi_j^j | \sigma(\xi_{j+1}^{\infty})) \quad \text{(chain rule)} \\
&= nH_{\mu}(\xi | \sigma(\xi_1^{\infty})) =0 \quad \text{(invariance)}
\end{align*}

\eop
\end{proof}
\s

\begin{proof}
\textbf{proof of (2) $\Rightarrow$ (3) in Theorem)} We assume $\tau (\xi)$ is trivial, $h_{\mu}(T, \xi) =0$ and we show $H_{\mu}(\xi) =0$. By the lemma, we have $H_{\mu}(\xi | \tau(\xi)) =0$. We have already seen that condition (2) implies (as in proof of (2) $\Rightarrow$ (1) of the theorem)
\begin{align*}
\avg [\chi_A | \tau(\xi)](x) = \mu(A) \quad \mu \text{-a.e.} \quad \forall A \in \borel
\end{align*}
so
\begin{align*}
0=H_{\mu}(\xi | \tau(\xi)) &= \int I_{\mu}(\xi | \tau(\xi))d\mu = -\int \log \avg \big[ \chi_{[x]_{\xi}} | \tau(\xi) \big](x) d\mu(x) \\
&= - \int \log \mu([x]_{\xi}) d\mu(x) \\
&= - \sum_{A \in \xi} \log( \mu(A)) \mu(A) = H_{\mu} (\xi)
\end{align*}
\end{proof}
\s

\newday

(20th November, Tuesday)
\s

Left to prove : If $h_{\mu}(T, \xi)>0$ for all $\xi \subset \borel$ finite with $H_{\mu}(\xi) >0$ then $\tau(\xi)$ is trivial $\forall \xi \subset \borel$ finite.
\s

\prop Let $(X, \borel, \mu, T)$ be an MPS. Let $\xi, \eta \subset \borel$ be two finite partitions. Then
\begin{align*}
h_{\mu}(T, \xi) = H_{\mu}(\xi | \sigma(\xi_1^{\infty}) \vee \tau (\eta))
\end{align*}
\s

\textbf{Note :} This tells us that if the entropy is positive, then the tail-$\sigma$-algebra cannot be too big.
\s

\lem Let $(X, \borel, \mu, T)$ be an MPS. Let $\xi \subset \borel$ be a finite partition. Then
\begin{align*}
h_{\mu}(T, \xi) = \frac{1}{n} H_{\mu} (\xi_0^{n-1} | \sigma(\xi_n^{\infty})) \quad \forall n \in \mathbb{Z}_{>0}
\end{align*}
\begin{proof}
\pf By chain rule and invariance :
\begin{align*}
H_{\mu}(\xi_0^{n-1} | \sigma(\xi_n^{\infty})) &= \sum_{j=0}^{n-1} H_{\mu}(\xi_j^j | \sigma(\xi_j^{\infty})) \\
&= n H_{\mu}(\xi | \sigma(\xi_1^{\infty})) = nh_{\mu}(T, \xi)
\end{align*}
as claimed.

\eop
\end{proof}
\s

The next lemma is bit harder, and where all the magic happens.
\s

\lem Let $(X, \borel, \mu, T)$ be an MPS. Let $\xi, \eta \subset \borel$ be finite measurable partitions. Then
\begin{align*}
h_{\mu}(T, \xi) = \lim_{n\rightarrow \infty} \frac{1}{n} H_{\mu} (\xi_0^{n-1} | \sigma(\xi_n^{\infty}) \vee \sigma(\eta_n^{\infty})  )
\end{align*}
That is, ``$\sigma(\eta_n^{\infty})$ does not add to much information if $n$ is sufficiently large". 
\begin{proof}
\pf Observe that $\forall n \in \mathbb{Z}_{>0}$, has
\begin{align*}
H_{\mu}(\xi_0^{n-1} | \sigma(\xi_n^{\infty}) \vee \sigma(\eta_n^{\infty})) &\leq H_{\mu} (\xi_0^{n-1} | \sigma(\xi_n^{\infty})) \\
&= nh_{\mu}(T, \xi)
\end{align*}
by the last lemma so we already have the inequality in one direction.

\quad Now we do the following : we choose two sequence $a_n$ and $b_n$ such that $a_n \leq b_n$ for all $n$ and
\begin{align*}
\lim_{n} \frac{1}{n} \Big[ H_{\mu}(\xi_0^{n-1} | \sigma(\xi_n^{\infty}) \vee \sigma(\eta_n^{\infty})) + a_n \Big] = \lim_{n} \frac{1}{n} \Big[ H_{\mu}(\xi_0^{n-1} | \sigma(\xi_n^{\infty})) + b_n \Big] \quad \cdots\cdots\cdots (\diamond)
\end{align*}

Suppose to the contrary that
\begin{align*}
\liminf_{n\rightarrow \infty} \frac{1}{n} H_{\mu}(\xi_0^{n-1} | \sigma(\xi_n^{\infty}) \vee \sigma(\eta_n^{\infty})) < h_{\mu}(T, \xi) = \frac{1}{n} H_{\mu}(\xi_0^{n-1} | \sigma(\xi_n^{\infty}))
\end{align*}
Then $a_n \leq b_n$ and this contradicts our assumption ($\diamond$) on $a_n, b_n$. This means that :
\begin{align*}
h_{\mu}(T, \xi) &\leq \liminf_{n\rightarrow \infty} \frac{1}{n} H_{\mu} (\xi_0^{n-1} | \sigma(\xi_n^{\infty}) \vee \sigma(\eta_n^{\infty})) \\
&\leq \limsup_{n\rightarrow \infty} \frac{1}{n} H_{\mu} (\xi_0^{n-1} | \sigma(\xi_n^{\infty}) \vee \sigma(\eta_n^{\infty})) \\
&\leq h_{\mu}(T,\xi)
\end{align*}
so this prove the lemma, given the existence of $a_n, b_n$ with the required property ($\diamond$).
\s

For the existence of such $(a_n),(b_n)$, we take
\begin{align*}
a_n &= H_{\mu} (\eta_0^{n-1} | \sigma(\xi_0^{\infty}) \vee \sigma(\eta_n^{\infty})) \\
b_n &= H_{\mu} (\eta_0^{n-1} | \sigma(\xi_0^{\infty}) )
\end{align*}
Clearly, $a_n \leq b_n$ and indeed
\begin{align*}
& \textbf{(A)} = H_{\mu}(\xi_0^{n-1} | \sigma(\xi_n^{\infty}) \vee \sigma(\eta_n^{\infty})) + a_n = H_{\mu}(\xi_0^{n-1} \vee \eta_0^{n-1} | \sigma (\xi_n^{\infty}) \vee \sigma(\eta_n^{\infty})) \\
& \textbf{(B)} = H_{\mu}(\xi_0^{n-1} | \sigma(\xi_n^{\infty})) + b_n = H_{\mu}(\xi_0^{n-1} \vee \eta_0^{n-1} | \sigma (\xi_n^{\infty}))
\end{align*}
By the last lemma,
\begin{align*}
\frac{\textbf{(A)}}{n} = h_{\mu}(T, \xi \vee \eta) \quad \forall n
\end{align*}
and therefore
\begin{align*}
h_{\mu}(T, \xi\vee \eta) = \frac{\textbf{(A)}}{n} \leq \frac{\textbf{(B)}}{n} \leq \frac{H_{\mu}(\xi_0^{n-1} \vee \eta_{0}^{n-1})}{n} \xrightarrow{n\rightarrow \infty} h_{\mu}(T, \xi\vee \eta)
\end{align*}

\eop
\end{proof}
\s

\begin{proof}
\textbf{proof of the proposition)} Using the chain rule and invariance, we have
\begin{align*}
H_{\mu}(\xi_0^{n-1} | \sigma(\xi_n^{\infty} \vee \sigma(\eta_n^{\infty}))) &= \sum_{j=0}^{n-1} H_{\mu}(\xi_j^j | \sigma(\xi_{j+1}^{\infty}) \vee \sigma(\eta_{n}^{\infty})) \\
&= \sum_{j=0}^{n-1} H_{\mu}(\xi | \sigma(\xi_1^{\infty}) \vee \sigma(\eta_{n-j}^{\infty}))
\end{align*}
Therefore, it follows from the previous lemma that
\begin{align*}
h_{\mu}(\xi, T) = \clim_{n\rightarrow \infty} H_{\mu}(\xi | \sigma(\xi_1^{\infty}) \vee \sigma(\eta_n^{\infty}))
\end{align*}
But by the the previous lemma, we already know
\begin{align*}
\lim_{n\rightarrow \infty} H_{\mu}(\xi | \sigma(\xi_1^{\infty})\vee \sigma(\eta_n^{\infty})) = H_{\mu}(\xi | \sigma(\xi_1^{\infty})\vee \tau(\eta))
\end{align*}
and therefore the claim follows.

\eop
\end{proof}

We finally conclude the main theorem of the chapter.
\s

\begin{proof}
\textbf{proof of Theorem (3) $\Rightarrow$ (2))} Suppose that $h_{\mu}(T, \xi ) >0$ for all $\xi \subset \borel$ finite with $H_{\mu}(\xi) >0$. 

\quad Suppose to the contrary that $\tau(\xi)$ is non-trivial for some $\xi \subset \borel$ finite. Then $\exists A \subset \tau(\xi)$ s.t. $0< \mu(A) <1$. Define $\eta = \{A, X\backslash A \}$. Apply the proposition with the roles of $\xi$ and $\eta$ interchanged,
\begin{align*}
h_{\mu}(T, \eta) = H_{\mu} (\eta | \sigma(\eta_1^{\infty}) \vee \tau(\xi) ) 
\end{align*}
If we prove $h_{\mu}(T, \eta) =0$, this gives a contradiction, as $H_{\mu}(\eta) >0$.
\begin{subproof}
: In fact, we will show $I_{\mu}(\eta | \sigma(\eta_1^{\infty}) \vee \tau(\xi)) (x) =0$ for a.e. $x$. Indeed, since $\eta \in \tau(\xi)$, has
\begin{align*}
I_{\mu}(\eta | \sigma(\eta_1^{\infty}) \vee \tau(\xi)) &= -\log \avg [\chi_{[x]_{\eta}} | \sigma(\eta_1^{\infty} \vee \tau(\xi))] \\
&= -\log \chi_{[x]_{\eta}} =0
\end{align*}
\end{subproof}
\eop
\end{proof}

\section*{13. Rudolph's Theorem}

\thm \emph{(Rudolph)} Let $\mu$ be a probability measure on $\reals/ \mathbb{Z}$ that is invariant and \emph{ergodic} with respect to the joint action of $T_2 : x \mapsto 2x$ and $T_3 : x\mapsto 3x$. That is to say, if $A \subset \borel$ is such that $T_2^{-1} A =A$ and $T_3^{-1} A =A$ then $\mu(A) \in \{0,1\}$. 

\quad Suppose that $h_{\mu}(T_2)>0$ or $h_{\mu}(T_3) >0$. Then $\mu$ is the Lebesgue measure on $\reals/\mathbb{Z}$.
\s

If we drop the assumption of positive entropy, then we might expect the measure to be Lebesgue measure or a measure supported on some set of rational numbers (this is a conjecture).
\s

\thm \emph{(Host)} Let $\mu$ be an \emph{ergodic} $T$-invariant measure on $\reals/\mathbb{Z}$ with $h_{\mu}(T_3)>0$. Then $\mu$-a.e. $x\in \reals/\mathbb{Z}$ is \textbf{normal} in base 2, i.e. the sequence $T_2^n x$ is equi-distributed in $\reals/ \mathbb{Z}$.

\quad Moreover, if $\mu$ is not necessarily ergodic, but the other assumptions holds, then
\begin{align*}
\mu (x\in \reals/\mathbb{Z} : x \text{ is normal in base} 2) \geq \frac{h_{\mu}(T_3)}{\log 3}
\end{align*}

Notice this result does not depend on the measure.
\s

\newday

(22nd November, Thursday)
\s

\thm \emph{(Host)} Let $\mu$ be an \emph{ergodic} $T$-invariant measure on $\reals/\mathbb{Z}$ with $h_{\mu}(T_3)>0$. Then $\mu$-a.e. $x\in \reals/\mathbb{Z}$ is \textbf{normal} in base 2, i.e. the sequence $T_2^n x$ is equi-distributed in $\reals/ \mathbb{Z}$.

\quad Moreover, if $\mu$ is not necessarily ergodic, but the other assumptions holds, then
\begin{align*}
\mu (x\in \reals/\mathbb{Z} : x \text{ is normal in base} 2) \geq \frac{h_{\mu}(T_3)}{\log 3}
\end{align*}
\s

\lem We have $\text{ord}_{3^k}{2} = 2 \cdot 3^{k-1}$, where $\text{ord}_{3^k}(2)$ is the order of $2$ modulo $3^k$. That is, the smallest integer $n$ such that $3^k | 2^n -1$.
\begin{proof}
\pf By induction on $k$, we prove that $\text{ord}_{3^k} = 2\cdot 3^{k-1}$ and $3^{k+1} \ndiv 2^{2\cdot 3^{k-1}} -1$
\begin{subproof}
: Easy to check for $k=0,1$. Assume that $k\geq 2$ and the claim holds for $k-1$. Then $3^k | 2^{\text{ord}_{3^k}(2)} -1$, so $3^{k-1} | 2^{\text{ord}_{3^k}(2)}-1$ hence $\text{ord}_{3^k}(2)=a\cdot 2\cdot 3^{k-2}$ for some $a\in \mathbb{Z}_{>0}$. We also know $2^{2\cdot 3^{k-2}} = b\cdot 3^{k-1} +1$ for some $b\in \mathbb{Z}_{>0}$ and $3\ndiv b$. Then by the binomial theorem we have
\begin{align*}
& 2^{2\cdot 2\cdot 3^{k-2}} = b^2 \cdot 3^{2(k-1)} + 2b3^{k-1} +1 \\
& 2^{3\cdot 2\cdot 3^{k-2}} = b^3 \cdot 3^{3(k-1)} + 3b^2\cdot 3^{2(k-1)} + 3\cdot b\cdot 3^{k-1}
\end{align*}
From this we see that $3^k \ndiv 2^{2\cdot 2 \cdot 3^{k-2}} -1$ but $3^k | 2^{3\cdot 2\cdot 3^{k-2}} -1$ and $3^{k+1} \ndiv 2^{3\cdot 2\cdot 3^{k-2}} -1$.
\end{subproof}

\eop
\end{proof}
\s

\textit{Observations} : look at the $T_2$ orbit of $a/3^k$ where $a\in  \mathbb{Z}$ and $3 \ndiv a$. This orbit is
\begin{align*}
\{ \frac{b}{3^k} : b\in [0, \cdots, 3^{k} -1], 3\ndiv b \}
\end{align*}
Also, $T_2^n (x+ \frac{a}{3^k}) = T_2^n (x) + T_2^n (\frac{a}{3^k})$. 
\s

\textit{Another idea} : If $h_{\mu}(T_3) >0$, then $\mu$ will not ``concentrate" on a few element of the set
\begin{align*}
\{ x+ \frac{a}{3^k} : a\in [0, \cdots, 3^k -1], 3\ndiv a \}
\end{align*}
\s

We need to show that for $\mu$-a.e. $x$ and for all $f\in C(\reals/\mathbb{Z})$ we have 
\begin{align*}
\frac{1}{N} \sum_{n=0}^{N-1} f(T_2^n x) \rightarrow \int f(x) dx
\end{align*}
In fact, it is enough to prove this for functions of the form
\begin{align*}
f(x) = \exp(2\pi imx), \quad m\in \mathbb{Z}
\end{align*}
because linear combinations of trigonometric functions is dense in the set of continuous functions.
\s

\textbf{Notation :} for $N\in \mathbb{Z}_{>0}$, let
\begin{align*}
P_{N,m}(x) = \frac{1}{N} \sum_{n=0}^{N-1} \exp(2\pi im 2^n x)
\end{align*}
Now it is enough to show : for all fixed $m \neq 0$ and for $\mu$-a.e. $x$, we have $P_{N,m}(x) \rightarrow 0$.
\s

\lem Fix $m\neq 0$ and $x\in \reals/\mathbb{Z}$. Let $\alpha$ be the largest exponent such that $3^{\alpha} | m$. Let $N < 2\cdot 3^{k-\alpha -1}$ for some $k\in \mathbb{Z}_{>0}$. Then
\begin{align*}
\sum_{a=0}^{3^{k}-1} \big| P_{N,m}(x + \frac{a}{3^k}) \big|^2 = \frac{3^k}{N}
\end{align*}
\begin{proof}
\pf \begin{align*}
\big| P_{N,m}(x) \big|^2 &= \big| \frac{1}{N} \sum_{n=0}^{N-1} \exp(2\pi i m 2^n x) \big|^2 \\
&= \frac{1}{N^2} \sum_{n_1=0}^{N-1} \sum_{n_2=0}^{N-1} \exp(2\pi im2^{n_1} x) \overline{\exp(2\pi im2^{n_2} x)} \\
&= \frac{1}{N^2} \sum_{n_1=0}^{N-1} \exp(2\pi i m(2^{n_1} - 2^{n_2}) x)
\end{align*}
Note that for each $b\in \mathbb{Z}_{>0}$, :
\begin{align*}
\sum_{a=0}^{3^k-1} \exp(2\pi ib(x+ \frac{a}{3^k})) = \exp(2\pi i bx) \sum_{a=0}^{3^k -1} \exp(2\pi iba/3^k) \quad \cdots \cdots\cdots (\star)
\end{align*}
If $3^k \ndiv b$, then $(\star)=0$. If $3^k | b$, then $(\star) = 3^k \exp(2\pi ibx)$.
\s

When does it happen that $3^k | m \cdot (2^{n_1} - 2^{n_2})$?
\begin{subproof}
Suppose $n_1>n_2$. Then $m\cdot (2^{n_1} - 2^{n_2}) = m\cdot 2^{n_1}(2^{n_1 - n_2} -1)$ Since $n_1-n_2 \leq N < 2\cdot 3^{k - \alpha -1}$, we have $3^{k-\alpha} \ndiv 2^{n_1-n_2} - 1$. So, $3^k \ndiv m\cdot (2^{n_1} - 2^{n_2})$ (recall $\alpha$ was chosen to be the largest s.t. $3^{\alpha} | m$). Same applies if $n_2 >n_1$. We get that $3^k | m(2^{n_1}-2^{n_2})$ \emph{iff} $n_1 =n_2$.
\end{subproof}
Therefore
\begin{align*}
|P_{N,m}(x)|^2 = \frac{1}{N^2} \sum_{n=0}^{N-1} 3^k = \frac{3^k}{N} 
\end{align*}

\eop
\end{proof}





\end{document}
